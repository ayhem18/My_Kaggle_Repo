{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(X: np.ndarray, dim: int = None, dL: np.ndarray = None) -> np.ndarray:\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        X (np.ndarray): _description_\n",
    "        dim (int, optional): _description_. Defaults to None.\n",
    "        dL (np.ndarray, optional): _description_. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: _description_\n",
    "    \"\"\"\n",
    "    # is dL is not None, the function is called to perform the backprop pass\n",
    "    if dL is not None:\n",
    "        # compute backpropagated loss\n",
    "        e = np.exp(X - np.max(X))\n",
    "        s = e / np.sum(e)\n",
    "        return np.sum(dL * s) - s * np.sum(dL * s)\n",
    "    else:\n",
    "        return np.exp(X - np.max(X)) / np.sum(np.exp(X - np.max(X)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add(x: np.ndarray, b: np.ndarray, dL: np.ndarray = None) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    This function executes the addition operation \n",
    "    \"\"\"    \n",
    "    if dL is not None:\n",
    "        # this operation has no effect on the gradient\n",
    "        return dL, dL\n",
    "    else:\n",
    "        # compute addition\n",
    "        return x + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot(b: np.ndarray, x: np.ndarray, dL: np.ndarray = None) -> np.ndarray:\n",
    "    \"\"\"Compute dot product or backpropagates the loss.\n",
    "    \n",
    "    \\\\text{dot}(x, b) = x \\cdot b\n",
    "    \n",
    "    Args:\n",
    "        x (np.ndarray): input array\n",
    "        b (np.ndarray): bias array\n",
    "        dL (np.ndarray, optional): backpropagated loss. Defaults to None.\n",
    "        \n",
    "    Returns:\n",
    "        np.ndarray: dot product or backpropagated loss\n",
    "    \"\"\"\n",
    "    if dL is not None:\n",
    "        return np.dot(dL, x.T), np.dot(b.T, dL)\n",
    "    else:\n",
    "        # compute dot product\n",
    "        return np.dot(b, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_dot(x: np.ndarray, b: np.ndarray, dL: np.ndarray = None) -> np.ndarray:\n",
    "    if dL is not None:\n",
    "        # compute backpropagated loss\n",
    "        return dL * b\n",
    "    else:\n",
    "        # compute batch dot product\n",
    "        return np.sum(x * b, axis=1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(X: np.ndarray, dL: np.ndarray = None) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    compute the RELU operation over (X)\n",
    "    \"\"\"\n",
    "    if dL is not None:\n",
    "        # the gradient of relu is simply\n",
    "        return (X > 0) * dL  \n",
    "    else:\n",
    "        # compute ReLU\n",
    "        return np.maximum(X, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matmul(X: np.ndarray, W: np.ndarray, dL: np.ndarray = None) -> np.ndarray:\n",
    "    \"\"\"Compute matrix multiplication or backpropagates the loss.\n",
    "    \n",
    "    \\\\text{matmul}(X, W) = WX\n",
    "    \n",
    "    Args:\n",
    "        X (np.ndarray): input array\n",
    "        W (np.ndarray): weight array\n",
    "        dL (np.ndarray, optional): backpropagated loss. Defaults to None.\n",
    "        \n",
    "    Returns:\n",
    "        np.ndarray: matrix multiplication or backpropagated loss\n",
    "    \"\"\"\n",
    "    if dL is not None:\n",
    "        # compute backpropagated loss\n",
    "        return np.dot(dL, W.T), np.dot(X.T, dL)\n",
    "    else:\n",
    "        # compute matrix multiplication\n",
    "        return np.dot(X, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bce(X: np.ndarray, y: np.ndarray, dL: np.ndarray = None) -> np.ndarray:\n",
    "    \"\"\"Compute binary cross entropy or backpropagates the loss.\n",
    "    \n",
    "    \\\\text{bce}(X, y) = -y \\log(X) - (1 - y) \\log(1 - X)\n",
    "    \n",
    "    Args:\n",
    "        X (np.ndarray): input array\n",
    "        y (np.ndarray): target array\n",
    "        dL (np.ndarray, optional): backpropagated loss. Defaults to None.\n",
    "        \n",
    "    Returns:\n",
    "        np.ndarray: binary cross entropy or backpropagated loss\n",
    "    \"\"\"\n",
    "    if dL is not None:\n",
    "        # compute backpropagated loss\n",
    "        return -y / (X + (np.finfo(float).eps)) + (1 - y) / (1 - X + np.finfo(float).eps)\n",
    "    else:\n",
    "        # compute binary cross entropy\n",
    "        return -np.mean(y * np.log(X) + (1 - y) * np.log(1 - X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv(\n",
    "    X: np.ndarray, \n",
    "    k: np.ndarray, \n",
    "    dL: np.ndarray = None\n",
    ") -> np.ndarray:\n",
    "    if dL is not None:\n",
    "        # Compute backpropagated loss over kernel and input image\n",
    "        dLdX = np.zeros_like(X)\n",
    "        dLdK = np.zeros_like(k)\n",
    "\n",
    "        for i in range(X.shape[0]):\n",
    "            for j in range(k.shape[0]):\n",
    "                for h in range(X.shape[2] - k.shape[2] + 1):\n",
    "                    for w in range(X.shape[3] - k.shape[3] + 1):\n",
    "                        dLdX[i, :, h:h + k.shape[2], w:w + k.shape[3]] += dL[i, j, h, w] * k[j, :, :, :]\n",
    "                        dLdK[j, :, :, :] += dL[i, j, h, w] * X[i, :, h:h + k.shape[2], w:w + k.shape[3]]\n",
    "        \n",
    "        return dLdX, dLdK\n",
    "\n",
    "    else:\n",
    "        output = np.zeros(\n",
    "            (X.shape[0], k.shape[0], X.shape[2] - k.shape[2] + 1, X.shape[3] - k.shape[3] + 1)\n",
    "        )\n",
    "        for i in range(X.shape[0]):\n",
    "            for j in range(k.shape[0]):\n",
    "                for h in range(X.shape[2] - k.shape[2] + 1):\n",
    "                    for w in range(X.shape[3] - k.shape[3] + 1):\n",
    "                        output[i, j, h, w] = np.sum(X[i, :, h:h + k.shape[2], w:w + k.shape[3]] * k[j, :, :, :])        \n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(X: np.ndarray, dL: np.ndarray = None) -> np.ndarray:\n",
    "    batch_size = X.shape[0]\n",
    "    \n",
    "    if dL is not None:\n",
    "        return dL.reshape(X.shape)\n",
    "    else:\n",
    "        return X.reshape(batch_size, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def minibatch_gd_mnist(X, y, \n",
    "                    k1: np.ndarray = None, \n",
    "                    k2: np.ndarray = None,\n",
    "                    k3: np.ndarray = None, \n",
    "                    w: np.ndarray = None,\n",
    "                    epochs=10, \n",
    "                    batch_size=32,\n",
    "                    lr=0.01):\n",
    "    \n",
    "    # set the parameters\n",
    "    k1 = k1 if k1 is not None else np.random.randn(16, 1, 3, 3)\n",
    "    k2 = k2 if k2 is not None else np.random.randn(32, 16, 3, 3)\n",
    "    k3 = k3 if k3 is not None else np.random.randn(10, 32, 3, 3)\n",
    "\n",
    "    w = w if w is not None else np.random.randn(4840, 10)\n",
    "    \n",
    "    for n in range(epochs):\n",
    "        for i in tqdm(range(0, len(X), batch_size)):\n",
    "            y_ = y[i:i + batch_size]\n",
    "            x = X[i:i + batch_size]\n",
    "            \n",
    "            conv1 = conv(x, k1)\n",
    "            relu1 = relu(conv1)\n",
    "            conv2 = conv(relu1, k2)\n",
    "            relu2 = relu(conv2)\n",
    "            conv3 = conv(relu2, k3)\n",
    "            relu3 = relu(conv3)\n",
    "            \n",
    "            # flatten\n",
    "            x_fl = flatten(relu3)\n",
    "            \n",
    "            # dense\n",
    "            x_mm = matmul(x_fl, w)\n",
    "            \n",
    "            # softmax\n",
    "            y_hat = softmax(x_mm, dim=1)\n",
    "            \n",
    "            # loss\n",
    "            loss = bce(y_hat, y_)\n",
    "            \n",
    "            # backpropagation\n",
    "            dL = bce(y_hat, y_, dL=1)\n",
    "            dL, dW = matmul(x_fl, w, dL=dL)\n",
    "            dL = flatten(relu3, dL)\n",
    "            dL = relu(conv3, dL)\n",
    "            dL, dK_3 = conv(relu2, k3, dL=dL)\n",
    "            dL = relu(conv2, dL)\n",
    "            dL, dK_2 = conv(relu1, k2, dL=dL)\n",
    "            dL = relu(conv1, dL)\n",
    "            dL, dK_1 = conv(x, k1, dL=dL)\n",
    "            \n",
    "            # perform the gradient descent rule\n",
    "            k1 -= lr * dK_1\n",
    "            k2 -= lr * dK_2\n",
    "            k3 -= lr * dK_3\n",
    "            w -= lr * dW\n",
    "            \n",
    "        print(f'Epoch {n + 1} | Loss: {loss:.4f}')\n",
    "\n",
    "    return k1, k2, k3, w\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def eval(x, k1, k2, k3, w):\n",
    "    conv1 = conv(x, k1)\n",
    "    relu1 = relu(conv1)\n",
    "    conv2 = conv(relu1, k2)\n",
    "    relu2 = relu(conv2)\n",
    "    conv3 = conv(relu2, k3)\n",
    "    relu3 = relu(conv3)\n",
    "    \n",
    "    # flatten\n",
    "    x = relu3.reshape(relu3.shape[0], -1)\n",
    "    \n",
    "    # dense\n",
    "    x = matmul(x, w)\n",
    "    \n",
    "    # softmax\n",
    "    y_hat = softmax(x, dim=1)\n",
    "    \n",
    "    return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def compare_with_torch():\n",
    "    X = np.random.rand(1, 1, 28, 28)\n",
    "    k = np.random.rand(1, 1, 3, 3)\n",
    "\n",
    "    my_conv = conv(X, k)\n",
    "    torch_conv = torch.nn.Conv2d(1, 1, 3, bias=False)\n",
    "    torch_conv.weight.data = torch.tensor(k)\n",
    "    torch_conv_out = torch_conv(torch.tensor(X))\n",
    "\n",
    "    assert np.allclose(my_conv, torch_conv_out.detach().numpy())\n",
    "\n",
    "    my_conv_dL = conv(X, k, dL=np.ones_like(my_conv))\n",
    "    torch_conv_out.backward(torch.ones_like(torch_conv_out))\n",
    "\n",
    "    assert np.allclose(my_conv_dL[1], torch_conv.weight.grad.detach().numpy())\n",
    "\n",
    "    print(\"forward pass and backward pass validated !!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_with_torch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first load the MNIST dataset.\n",
    "import os\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms as tr\n",
    "\n",
    "mnist_train = MNIST(root=os.getcwd(), train=True, download=True, transform=tr.ToTensor())\n",
    "mnist_test = MNIST(root=os.getcwd(), train=False, download=True, transform=tr.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = np.asarray([mnist_train[i][0].numpy() for i in range(500)]), np.asarray([mnist_train[i][1] for i in range(500)])\n",
    "X_test, y_test = np.asarray([mnist_test[i][0].numpy() for i in range(500)]), np.asarray([mnist_test[i][1] for i in range(500)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.reshape(-1, 1, 28, 28).astype(np.float32)\n",
    "x_test = X_test.reshape(-1, 1, 28, 28).astype(np.float32)\n",
    "y = y.reshape(-1, 1)\n",
    "y_test = y_test.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Loss: 40.6785\n",
      "Epoch 2 | Loss: 20.4560\n"
     ]
    }
   ],
   "source": [
    "params = minibatch_gd_mnist(X, y, batch_size=32, epochs=2, lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.3456\n"
     ]
    }
   ],
   "source": [
    "y_hat = eval(x_test, *params)\n",
    "print(f'Accuracy: {np.mean(np.argmax(y_hat, axis=1) == y_test)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
