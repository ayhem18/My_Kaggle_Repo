{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = \"train.csv\"\n",
    "test_file = \"test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_org = pd.read_csv(train_file)\n",
    "df = df_train_org.copy()# .set_index(\"PassengerId\")\n",
    "print(df.head())\n",
    "df_test_org = pd.read_csv(test_file)\n",
    "df_test = df_test_org.copy()# .set_index(\"PassengerId\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['train'] = pd.Series([1 for _ in range(len(df) + 1)])\n",
    "df_test['train'] = pd.Series([0.0 for _ in range(len(df_test) + 1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.set_index('PassengerId')\n",
    "df_test = df_test.set_index(\"PassengerId\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.concat([df,df_test], ignore_index=True)\n",
    "print(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = all_data.rename(mapper=str.lower, axis=1) \n",
    "all_data = all_data.rename(columns={\"survived\":\"y\", \"embarked\":\"from\", \"pclass\":\"c\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fam_sizes = all_data['fam_name'].value_counts() \n",
    "\n",
    "all_data['fam_size'] = all_data['fam_name'].apply(lambda x: fam_sizes[x])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in the previous three categories, females have an extremely high probability of surviving. Yet, among the male survivors\n",
    "# the determining factor is still unclear. Let's consider the following new column which the family size determined out of the name column\n",
    "\n",
    "\n",
    "# what matters here in the name is the first part representing the family name\n",
    "all_data['fam_name'] = all_data['name'].apply(lambda x: re.sub('.;:?', \",\", x).strip().lower().split(\",\")[0]) \n",
    "# print(all_data['fam_name'])\n",
    "\n",
    "fam_sizes = all_data.pivot_table(columns='y', index='fam_name', values='ticket', aggfunc='count')\n",
    "# print(fam_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = all_data[all_data['train'] == 1].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_num_names = [\"y\", \"c\", \"age\", \"sibsp\", \"parch\", \"fare\"]\n",
    "X_cat_names = [\"name\", \"sex\", \"ticket\", \"cabin\", \"from\"]\n",
    "df_num = df.loc[:, X_num_names]\n",
    "df_cat = df.loc[:, X_cat_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_num.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df_num.columns:\n",
    "    plt.hist(df_num[col])\n",
    "    plt.title(col)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_survive = df.drop('train', axis=1)[df['y']==1]\n",
    "print(df_survive.describe())\n",
    "print(\"#\" * 50)\n",
    "df_dead = df.drop('train', axis=1)[df['y'] == 0]\n",
    "print(df_dead.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_survive.reset_index().plot(kind='scatter', x='index', y='fare', title='fare variation for survivors')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dead.reset_index().plot(kind='scatter', x='index', y='fare', title='fare variation for the dead')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ultra_rich = df.drop(['ticket', 'name', 'train', 'cabin', 'from', 'fam_name'], axis=1)[df['fare'] >= 200]\n",
    "print(df_ultra_rich[df_ultra_rich['y'] != 0])\n",
    "print(\"**\" * 100)\n",
    "print(df_ultra_rich[df_ultra_rich['y'] == 0]) \n",
    "\n",
    "## so if the passengers are ultra rich, then a female has quite high probability of surviving.\n",
    "## the males with the most expensive fares are the ones to survive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's consider the passengers paying fare in the range [100, 200]\n",
    "df_rich = df.drop(['ticket', 'name', 'train', 'cabin', 'from', 'fam_name'], axis=1)[(df['fare'] >= 100) & (df['fare'] <200)]\n",
    "print(df_rich[df_rich['y'] == 1])\n",
    "print(\"*\" * 100)\n",
    "print(df_rich[df_rich['y'] == 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_3 = df.drop(['ticket', 'name', 'train', 'cabin', 'from', 'fam_name'], axis=1)[(df['fare'] >= 50) & (df['fare'] <100)]\n",
    "df_3.reset_index().plot(kind='scatter', x='index', y='fare', title='third category fare')\n",
    "df_3_sur = df_3[df_3['y'] == 1]\n",
    "df_3_dead = df_3[df_3['y'] == 0]\n",
    "df_3_sur.reset_index().plot(kind='scatter', x='index', y='fare', title = 'third category fare survivors')\n",
    "df_3_dead.reset_index().plot(kind='scatter', x='index', y='fare', title = 'third category fare dead')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_3_sur.describe())\n",
    "print(df_3_dead.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(df_3_sur['sex'].value_counts())\n",
    "# print(df_3_dead['sex'].value_counts())\n",
    "print(df_3[df_3['sex'] == 'male'][['y', 'fam_size', 'fare', 'age']].sort_values(['fam_size', 'y'], ascending=[False, False]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_4 = df.drop(['ticket', 'name', 'train', 'cabin', 'from', 'fam_name'], axis=1)[(df['fare'] < 50) & (df['fare'] >= 40)]\n",
    "print(df_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_5 = df.drop(['ticket', 'name', 'train', 'cabin', 'from', 'fam_name'], axis=1)[(df['fare'] < 40)]\n",
    "df_5.reset_index().plot(kind='scatter', x='index', y='fare')\n",
    "print(df_5[df_5['y']==1]['c'].value_counts())\n",
    "print(df_5[df_5['y']==0]['c'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fun_list = [pd.Series.count, np.sum, np.mean, np.std, np.max, np.min]\n",
    "\n",
    "df_survived_class = df_survive.groupby(\"c\").agg({\"fare\":fun_list})\n",
    "df_dead_class = df_dead.groupby(\"c\").agg({\"fare\":fun_list})\n",
    "print(df_survived_class)\n",
    "print(df_dead_class)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_num.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd.pivot_table(df, index='y', values=['c','age', 'fare'], aggfunc=[pd.Series.count, np.mean]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparing survivors' values with respect to categorical variables.\n",
    "print(pd.pivot_table(df, index='y', columns=['c'], values='ticket', aggfunc='count'))\n",
    "print(pd.pivot_table(df, index='y', columns=['sex'], aggfunc='count', values='ticket'))\n",
    "print(pd.pivot_table(df, index='y', columns='from', aggfunc='count', values='ticket'))\n",
    "# at first glance it might seem that passengers embarking from \"C\" are more likely to survive. Yet, it might be useful to consider\n",
    "# the social class of people coming from the different stations.\n",
    "\n",
    "print(pd.pivot_table(df, index='c', columns=['from'], values='name', aggfunc='count'))\n",
    "# the last observation did not provide evidence to completely rool out the possibility of positive correlation between the embarkment \n",
    "# point and survival, more investigation is needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 0: male, 1: female\n",
    "genre_mapper = {\"male\":0, \"female\":1}\n",
    "all_data[\"sex\"] = pd.Series([genre_mapper[x] for x in all_data['sex']])\n",
    "\n",
    "from_mapper = {\"C\":1, \"Q\":2, \"S\":3}\n",
    "all_data[\"from\"] = pd.Series([from_mapper[x] if x in from_mapper else x for x in all_data[\"from\"]])\n",
    "\n",
    "all_data['from'] = all_data['from'].astype(float)\n",
    "print(all_data.loc[:, [\"y\", \"from\"]].corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = all_data[all_data['train'] == 1].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# understand the relation between classes and the embarkment station\n",
    "from_class_ana = df.groupby(\"from\").agg({\"c\":['count', 'mean'], \"fare\":'mean'})\n",
    "print(from_class_ana)\n",
    "# so we can say the embarkment station has little to no correlation with the social class\n",
    "\n",
    "print(pd.pivot_table(df, index='from',columns='c',values='ticket', aggfunc='count'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # understanding the cabin\n",
    "\n",
    "# print(df[\"cabin\"].isna().sum())\n",
    "# print(df[\"cabin\"].copy().dropna().count())\n",
    "# so only 204 passengers bought cabins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # we assume that the cabins are indeed separated by spaces\n",
    "# df['num_cabins'] = df.cabin.apply(lambda x: 0 if pd.isna(x) else len(x.split(\" \")))\n",
    "# print(df.num_cabins.value_counts())\n",
    "# # let's check the relation between number of cabins and social class\n",
    "# print(pd.pivot_table(df, index='num_cabins', columns='c', values='ticket', aggfunc='count'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# understanding the fare column:\n",
    "print (df.loc[:, [\"fare\"]].describe())\n",
    "fare_s = df['fare']\n",
    "fare_df = df.loc[:, [\"fare\"]]\n",
    "# consider the nan values\n",
    "print(fare_s.isna().sum())\n",
    "# there is no nan values: such a delight !!!\n",
    "\n",
    "print(fare_s[lambda x : x == 0].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can see that the survival probability is higher for upper social classes. However, It might be worth noting\n",
    "# that the ticket's price: fare is as well influencial. In other words, a 3rd class passenger who paid more than a 1st passenger\n",
    "# might be more likely to survive, let's consider this subtle detail: it might lead to a helpful feature\n",
    "\n",
    "# df_no_fare = df[df['fare'] == 0]\n",
    "# df_fare = df[df['fare'] != 0]\n",
    "# fare_np = df_fare[\"fare\"].values\n",
    "\n",
    "# quantiles_values = [0, 0.25, 0.5, 0.75, 1]\n",
    "# fare_quantiles = {}\n",
    "# for i in range(1, 4):\n",
    "#     fare_quantiles[\"fare_q_c\" + str(i)] = np.quantile(df_fare[df_fare[\"c\"] == i][\"fare\"].values, quantiles_values)\n",
    "\n",
    "# for key, value in fare_quantiles.items():\n",
    "#     print(str(key) + \": \" + str(value)) \n",
    "\n",
    "# def quartile_number(value, quantiles):\n",
    "#     # value assumed to be at least larger or equal then the lowest value\n",
    "#     assert (value >= quantiles_values[0])\n",
    "#     for i in range(len(quantiles) - 1):\n",
    "#         if value >= quantiles[i] and value < quantiles[i + 1]:\n",
    "#             return i + 1\n",
    "#     return len(quantiles) - 1 \n",
    "\n",
    "# def classify_passenger(row):\n",
    "#     return quartile_number(row['fare'], fare_quantiles[\"fare_q_c\" + str(int(row['c']))])    \n",
    "\n",
    "\n",
    "# df_fare['quartile_class'] = df_fare.loc[:, ['fare', 'c']].apply(lambda row: classify_passenger(row), axis=1)\n",
    "# print(df_fare.loc[:, ['fare', 'c', 'quartile_class']].head(15))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# fare_class_quartile_effect = pd.pivot_table(df_fare, index='y', columns=['c','quartile_class'], values='name', aggfunc='count')\n",
    "# print(fare_class_quartile_effect)\n",
    "# print(\"#\" * 50)\n",
    "# fare_class_quartile_effect.loc[2] = fare_class_quartile_effect.loc[1] / fare_class_quartile_effect.loc[0]\n",
    "# print(fare_class_quartile_effect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the results are promissing and thus it is worthy experimenting with quartile class feature\n",
    "# let's add the values to the all_data dataframe\n",
    "# first impute the zero values with the column mean\n",
    "\n",
    "fare_by_class_mean = all_data[all_data['fare'] > 0].groupby(\"c\").agg({\"fare\": np.mean}).squeeze()\n",
    "print(fare_by_class_mean)\n",
    "\n",
    "\n",
    "def fill_up_fare(row):\n",
    "    if row['fare'] == 0 or np.isnan(row['fare']):\n",
    "        row['fare'] = fare_by_class_mean[row['c']]\n",
    "    return row\n",
    "\n",
    "# we can see that there is positive correlation between the quartile_class feature and survival\n",
    "# it is necessary to impute the row: the mean seems like a reasonable choice\n",
    "\n",
    "all_data = all_data.apply(lambda row: fill_up_fare(row) , axis=1)\n",
    "\n",
    "print(all_data['fare'].isna().sum()) # there is no Nan values anymore\n",
    "print(all_data[all_data['fare'] <= 0]['fare'].sum()) # there is no 0 fare values anymore\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now the quartile class should be added to the add_data DF\n",
    "# quantiles_values = [0, 0.25, 0.5, 0.75, 1]\n",
    "# fare_quantiles = {}\n",
    "# for i in range(1, 4):\n",
    "#     fare_quantiles[\"fare_q_c\" + str(i)] = np.quantile(all_data[all_data[\"c\"] == i][\"fare\"].values, quantiles_values)\n",
    "\n",
    "# all_data['quartile_class'] = all_data.loc[:, ['fare', 'c']].apply(lambda row: classify_passenger(row), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(df.loc[:, [\"y\", \"num_cabins\"]].corr())\n",
    "\n",
    "# There are 3 features that might reflect in a passenger's social image:\n",
    "# * class * fare * num_cabins\n",
    "# let's consider each individually\n",
    "\n",
    "# print((pd.pivot_table(df, index='y', columns=[ 'num_cabins', 'c'], values='ticket', aggfunc=['count'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## the number of cabins is quite a helpful feature as well so it seems reasonable to add it to the all_data df\n",
    "# all_data['num_cabins'] = all_data.cabin.apply(lambda x: 0 if pd.isna(x) else len(x.split(\" \")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's first consider the title associated with a passenger's name\n",
    "\n",
    "df['title'] = df['name'].apply(lambda x: re.sub('[:?;.]', \",\", x).split(\",\")[1].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.title.value_counts().index.sort_values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's consider the non-uniform titles and their relevance to the survival\n",
    "non_comm_title = df[df['title'].isin(['Capt', 'Col', 'Don', 'Dr', 'Jonkheer', 'Lady', 'Major','Rev', 'Sir', 'the Countess'])]\n",
    "print(pd.pivot_table(non_comm_title, index='y', columns='title', values='ticket', aggfunc='count'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comm_title = df[df['title'].isin(['Master','Miss', 'Mlle', 'Mme', 'Mr', 'Mrs', 'Ms'])]\n",
    "# print(pd.pivot_table(comm_title, index='y', columns='title', values='ticket', aggfunc='count'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's impute the age\n",
    "print(df['age'].isna().sum())\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_3_age = df[(~np.isnan(df['age'])) &(df['c'] == 3) ]['age'].values\n",
    "print(np.nanstd(class_3_age))\n",
    "print(np.amax(class_3_age), np.amin(class_3_age))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = all_data[all_data['train'] != 1]\n",
    "print(df_test['age'].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_class = all_data.groupby('c').agg({\"age\":[np.mean, np.median]}).iloc[:, 0]\n",
    "print(age_class)\n",
    "# count the number of missing age values in each class \n",
    "print(all_data[np.isnan(all_data['age'])].groupby('c').agg({\"name\":'count'})) \n",
    "# as we can see there are only few values missing values for first and second class, thus it might not be harmful\n",
    "# to fill the missing values with the class's mean age\n",
    "# however, since the 3rd class has a large number of missing values, a more careful imputation might be needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as we can see there 4 main titles associated with passenger from the 3rd class.\n",
    "According to the following [link](https://prowritingaid.com/art/968/mr%2c-mrs%2c-ms-and-miss%3a-everything-you-need-to-know-about-titles.aspx), the title is generally associated with \n",
    "an age category or a matrial status (that indeed correlates with age...). It might be a good idea to associate these two features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's first consider the title associated with a passenger's name\n",
    "\n",
    "all_data['title'] = all_data['name'].apply(lambda x: re.sub('[:?;.]', \",\", x).split(\",\")[1].strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's fill the third class missing ages with the mean of the associated title\n",
    "print(all_data[all_data['c'] == 3]['title'].value_counts())\n",
    "\n",
    "title_age_class_3 = all_data[all_data['c'] == 3].groupby('title').agg({\"age\":np.nanmean}).iloc[:, 0]\n",
    "print(title_age_class_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_up_age_class_3(row):\n",
    "    value1 = np.round(title_age_class_3[row['title']])\n",
    "    value2 = np.round(age_class[row['c']]) # used only for one passenger with title Ms.\n",
    "    if np.isnan(row['age']):\n",
    "        row['age'] =  value2 if np.isnan(value1) else value1\n",
    "    return row\n",
    "\n",
    "def fill_up_age_class_1_2(row):\n",
    "    if np.isnan(row['age']):\n",
    "        row['age'] = np.round(age_class[row['c']])\n",
    "    return row\n",
    "\n",
    "def fill_up_age(row):\n",
    "    if row['c'] == 3:\n",
    "        return fill_up_age_class_3(row)\n",
    "    return fill_up_age_class_1_2(row)\n",
    "\n",
    "all_data = all_data.apply(fill_up_age, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it is time to drop the unncessary columns\n",
    "print(all_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = all_data.drop(['name', 'ticket', 'cabin', 'title','sibsp', 'parch', 'fam_name'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = all_data[all_data['train'] == 1].copy()\n",
    "df_test = all_data[all_data['train'] != 1].copy()\n",
    "# df_train.dropna(subset=['from','age'], inplace=True) # drop nan values \n",
    "\n",
    "print(df_train.columns, df_test.columns)\n",
    "df_train.dropna(subset=['from', 'age'], inplace=True)\n",
    "X_train = df_train.drop(['train', 'y'],axis=1).values\n",
    "\n",
    "y_train = df_train['y'].values\n",
    "X_test = df_test.drop(['train', 'y'],axis=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time to scale the data\n",
    "# we can use the sklearn class for this\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try all the models baseline models I currently know\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "n_splits = 6\n",
    "random_state = 5\n",
    "kf = KFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "\n",
    "solver = 'liblinear'\n",
    "lr = LogisticRegression(solver=solver)\n",
    "cv_scores = cross_val_score(lr, X_train, y_train, cv=kf)\n",
    "print(np.mean(cv_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svm = SVC() # the non-linear SVM\n",
    "cv_scores = cross_val_score(svm, X_train, y_train, cv=kf)\n",
    "print(cv_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier()\n",
    "cv_scores = cross_val_score(svm, X_train, y_train, cv=kf)\n",
    "print(cv_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dt = DecisionTreeClassifier(random_state=2) \n",
    "cv_scores = cross_val_score(dt, X_train, y_train)\n",
    "print(cv_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "def model_best_version(model, model_name):\n",
    "    print(model_name)\n",
    "    print(\"parameters \\n\" + str(model.best_params_))\n",
    "    print(\"best f1score \\n\" + str(model.best_score_))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# tune the parameters\n",
    "lamda = np.array([10 ** x for x in np.linspace(-5, 0.1)])\n",
    "\n",
    "lr_params = {\"max_iter\": [2000], \"penalty\":['l2'], 'C': 1 / lamda, 'solver':['liblinear']}\n",
    "\n",
    "lr_best = GridSearchCV(lr, param_grid=lr_params, cv=6, n_jobs=-1, scoring='f1')\n",
    "\n",
    "lr_best.fit(X_train, y_train)\n",
    "\n",
    "model_best_version(lr_best, \"LogisticRegression\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_params = {'n_neighbors' : range(5, 20),\n",
    "              'weights' : ['uniform', 'distance'],\n",
    "              'algorithm' : ['auto', 'ball_tree','kd_tree'],\n",
    "              'p' : [1,2]}\n",
    "knn_best = GridSearchCV(knn, param_grid = knn_params, cv = 6,  n_jobs = -1)\n",
    "knn_best.fit(X_train,y_train)\n",
    "model_best_version(knn_best, \"KNN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svm_params = [{'kernel': ['rbf'], 'gamma': [.1,.5,1,2,5,10], 'C': 1 / lamda}]\n",
    "# {'kernel': ['poly'], 'degree' : [2,3,4,5], 'C': 1 / lamda}]\n",
    "\n",
    "svc_best = GridSearchCV(SVC(), param_grid=svm_params, cv=6, n_jobs=-1)\n",
    "svc_best.fit(X_train, y_train)\n",
    "model_best_version(svc_best, \"NON-linear SVM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_params = {\"max_depth\": [2, 3,4,5,6], \"min_samples_leaf\": [0.02, 0.04, 0.05, 0.1, 0.12, 0.15], \"max_features\":[\"log2\", \"sqrt\", None]}\n",
    "dt_best = GridSearchCV(dt, param_grid=dt_params, cv=6, n_jobs=-1)\n",
    "dt_best.fit(X_train, y_train)\n",
    "model_best_version(dt_best, \"DecisionTreeClassifier\")\n",
    "\n",
    "print(cross_val_score(dt_best, X_train, y_train, cv=kf).mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_pred = dt_best.predict(X_train)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(y_train, X_train_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_diff = pd.DataFrame(np.abs(X_train_pred - y_train), columns=['diff'])\n",
    "\n",
    "y_diff = y_diff[y_diff['diff'] == 1]\n",
    "\n",
    "train_mis = df_train.drop(['train'], axis=1).iloc[y_diff.index,:]\n",
    "\n",
    "print(train_mis[train_mis['y'] == 1].head(20))\n",
    "# print(train_mis[(train_mis['y'] == 0.0) & (train_mis['quartile_class'].isin([3,4]))])\n",
    "# print(\"#\" * 100)\n",
    "\n",
    "# print(train_mis[(train_mis['y'] == 1.0) & (train_mis['quartile_class']).isin([1,2])])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gather submissions\n",
    "lr_pred = lr_best.predict(X_test)\n",
    "knn_pred = knn_best.predict(X_test)\n",
    "dt_pred = dt_best.predict(X_test)\n",
    "\n",
    "\n",
    "# sub_1 = pd.DataFrame({\"PassengerId\": df_test_org['PassengerId'], \"Survived\": lr_pred}).astype(int)\n",
    "sub_knn = pd.DataFrame({\"PassengerId\": df_test_org['PassengerId'],\"Survived\": knn_pred}).astype(int)\n",
    "sub_dt = pd.DataFrame({\"PassengerId\": df_test_org['PassengerId'],\"Survived\": dt_pred}).astype(int)\n",
    "# sub_1.to_csv(\"sub1.csv\", index=False)\n",
    "sub_knn.to_csv(\"sub_knn.csv\", index=False)\n",
    "sub_dt.to_csv(\"sub_dt.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's consider more complicated models such as Random Forests model.\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# rf_basic = RandomForestClassifier()\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# let's try to tune a RandomForest model\n",
    "rf = RandomForestClassifier()\n",
    "print(rf.get_params())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rf_params = {'max_depth':[4, 5, 6, 7, 8], 'max_features': ['sqrt', 'log2'], 'min_samples_leaf':[0.02, 0.03, 0.04, 0.05, 0.08, 0.1]\n",
    ", 'max_samples':[0.8, 0.85, 0.9, 1]}\n",
    "\n",
    "rf_basic = RandomForestClassifier(random_state=8)\n",
    "num_folds=6\n",
    "rf_searcher = GridSearchCV(estimator=rf_basic, \n",
    "                            param_grid=rf_params, \n",
    "                            n_jobs=-1, \n",
    "                            scoring='accuracy', \n",
    "                            cv=num_folds)\n",
    "rf_searcher.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rf_best = rf_searcher.best_estimator_\n",
    "\n",
    "print(rf_best.score(X_train, y_train))\n",
    "\n",
    "splits = 5\n",
    "random_state = 3\n",
    "kf = KFold(n_splits=splits, random_state=random_state, shuffle=True)\n",
    "print(cross_val_score(rf_best, X_train, y_train, cv=kf).mean())\n",
    "# it seems that the estimator does not overfit the data (not too badly either way)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_accs = []\n",
    "cv_accs = []\n",
    "estimators = range(20, 251, 10)\n",
    "for i in estimators:\n",
    "    \n",
    "    rf_best.n_estimators = i\n",
    "    rf_best.fit(X_train, y_train)\n",
    "    train_accs.append(rf_best.score(X_train, y_train))\n",
    "    cv_accs.append(cross_val_score(rf_best, X_train, y_train, cv=kf).mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(estimators, train_accs, '-b', label='train')\n",
    "ax.plot(estimators, cv_accs, '--r', label='CV')\n",
    "leg = ax.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can see that the overall performance is more promising in the range [20, 50]\n",
    "# let's try to focus on this range\n",
    "min_train_performance = 0.82\n",
    "# the goal is to choose the model with a train accuracy larger than the treshhold while minimizing the difference between the train accuracy\n",
    "# and cross validation score\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "train_accs = []\n",
    "cv_accs = []\n",
    "estimators = range(20, 51)\n",
    "best_performance = 1\n",
    "best_model = None\n",
    "best_t_score = 0\n",
    "performances = []\n",
    "t_scores = []\n",
    "\n",
    "for r in range(0, 100):\n",
    "    rf_best.random_state = r\n",
    "    for i in estimators:    \n",
    "        rf_best.n_estimators = i\n",
    "        rf_best.fit(X_train, y_train)\n",
    "        \n",
    "        t_score = rf_best.score(X_train, y_train)\n",
    "        cv_score = cross_val_score(rf_best, X_train, y_train, cv=kf).mean()\n",
    "        \n",
    "        if t_score >= min_train_performance and abs(t_score - cv_score) <= best_performance:\n",
    "            best_t_score = t_score\n",
    "            best_performance = t_score - cv_score\n",
    "            best_model = deepcopy(rf_best) \n",
    "    performances.append(best_performance)\n",
    "    t_scores.append(best_t_score)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(range(0, 100), performances, '-b', label='per')\n",
    "ax.plot(range(0, 100), t_scores, '--r', label='score')\n",
    "leg = ax.legend();\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_pred = best_model.predict(X_test)\n",
    "\n",
    "# sub_1 = pd.DataFrame({\"PassengerId\": df_test_org['PassengerId'], \"Survived\": lr_pred}).astype(int)\n",
    "sub_rf = pd.DataFrame({\"PassengerId\": df_test_org['PassengerId'],\"Survived\": rf_pred}).astype(int)\n",
    "\n",
    "sub_rf.to_csv('sub_rf.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "xgc = xgb.XGBClassifier(seed=123, objective=\"reg:logistic\")\n",
    "\n",
    "params = {'n_estimators': range(50, 100),\n",
    "    'max_depth': [3, 4, 5], \n",
    "    \"eta\": [0.001, 0.01, 0.05, 0.1, 0.2],\n",
    "    \"subsample\": [0.8, 0.9, 1], \n",
    "    \"lambda\": [0.01, 0.05, 0.1, 0,5, 0.8], \n",
    "}\n",
    "\n",
    "grid_mse = GridSearchCV(xgc, param_grid=params, scoring='neg_mean_squared_error', cv=4, verbose=1)\n",
    "\n",
    "grid_mse.fit(X_train, y_train)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_pred = best_model.predict(X_test)\n",
    "\n",
    "# sub_1 = pd.DataFrame({\"PassengerId\": df_test_org['PassengerId'], \"Survived\": lr_pred}).astype(int)\n",
    "sub_xgb = pd.DataFrame({\"PassengerId\": df_test_org['PassengerId'],\"Survived\": xgb_pred}).astype(int)\n",
    "\n",
    "sub_xgb.to_csv('sub_xgb.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM: {'C': 6.866488450042998, 'gamma': 0.1, 'kernel': 'rbf'}  \n",
    "RF: {{'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 6, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': 0.9, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 0.02, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 68, 'n_jobs': None, 'oob_score': False, 'random_state': 6, 'verbose': 0, 'warm_start': False}}\n",
    "\n",
    "XGBOOST: xgb\n",
    "parameters \n",
    "{'eta': 0.001, 'lambda': 0.01, 'max_depth': 5, 'n_estimators': 90, 'subsample': 1}\n",
    "best f1score \n",
    "0.8369389568941139\n",
    "None\n",
    "\n",
    "update: \n",
    "\n",
    "XGboost\n",
    "parameters \n",
    "{'eta': 0.001, 'lambda': 0.01, 'max_depth': 3, 'n_estimators': 81, 'subsample': 0.8}\n",
    "best f1score \n",
    "0.8335555286227931\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('ds_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "006414dea9a04848ce797b510a25f3f28ac8668e3d3244e777242cca6bed477f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
