{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# House Pricing Predictions\n",
    "This is my attempt on the data science challenge on [Kaggle](https://www.kaggle.com/competitions/house-prices-advanced-regression-techniques/overview), predicting the house prices given a large number of features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary work: imports and loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variables to store the location of the training and test datasets\n",
    "train_file = \"train.csv\"\n",
    "test_file = \"test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_org = pd.read_csv(train_file)\n",
    "df_test_org = pd.read_csv(test_file)\n",
    "# df and df_test are copies of the original datasets with target value referred to as y\n",
    "Y = \"y\"\n",
    "df = df_train_org.rename(columns={'SalePrice': Y})\n",
    "df_test = df_test_org.rename(columns={'SalePrice':Y})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration \n",
    "In this section we explore the basic aspects of the provided dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function to drop an element to both train and test dataframes\n",
    "def drop_cols(col_names:list, on='both'):\n",
    "    global df, df_test\n",
    "    try:\n",
    "        df = df.drop(col_names, axis=1)\n",
    "        df_test = df_test.drop(col_names, axis=1)\n",
    "    except:\n",
    "        print(\"The column{s} {cols} have already been dropped\".\n",
    "        format(s= \"\" if (col_names is str or len(col_names) == 1) else \"s\", cols=str(col_names)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1460, 81) (1459, 80)\n",
      "True\n",
      "(1460, 81) (1459, 80)\n"
     ]
    }
   ],
   "source": [
    "# let's first understand the nature of our data\n",
    "print(df.shape, df_test.shape) \n",
    "# each sample is described by 81 features. This number if relatively high.\n",
    "\n",
    "print((df['Id'].values == range(1 ,len(df) + 1)).all()) \n",
    "# as we can see the Id column is merely for ennumeriation purposes. It can be either dropped or set as an index.\n",
    "drop_cols(\"Id\")\n",
    "print(df.shape, df_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical and Numerical features\n",
    "Let's consider the different types of features. First, we divide them into numerical and non-numerical. The non-numerical are definitely categorical (or can be made as such). As for numerical, columns with int values, can be considered categorical if the number of unique values is limited."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['MSZoning', 'Street', 'Alley', 'LotShape', 'LandContour', 'Utilities',\n",
      "       'LotConfig', 'LandSlope', 'Neighborhood', 'Condition1', 'Condition2',\n",
      "       'BldgType', 'HouseStyle', 'RoofStyle', 'RoofMatl', 'Exterior1st',\n",
      "       'Exterior2nd', 'MasVnrType', 'ExterQual', 'ExterCond', 'Foundation',\n",
      "       'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2',\n",
      "       'Heating', 'HeatingQC', 'CentralAir', 'Electrical', 'KitchenQual',\n",
      "       'Functional', 'FireplaceQu', 'GarageType', 'GarageFinish', 'GarageQual',\n",
      "       'GarageCond', 'PavedDrive', 'PoolQC', 'Fence', 'MiscFeature',\n",
      "       'SaleType', 'SaleCondition'],\n",
      "      dtype='object')\n",
      "Index(['Id', 'MSSubClass', 'LotFrontage', 'LotArea', 'OverallQual',\n",
      "       'OverallCond', 'YearBuilt', 'YearRemodAdd', 'MasVnrArea', 'BsmtFinSF1',\n",
      "       'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF',\n",
      "       'LowQualFinSF', 'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath',\n",
      "       'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr', 'TotRmsAbvGrd',\n",
      "       'Fireplaces', 'GarageYrBlt', 'GarageCars', 'GarageArea', 'WoodDeckSF',\n",
      "       'OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea',\n",
      "       'MiscVal', 'MoSold', 'YrSold', 'y'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# consider non-numerical values\n",
    "object_type = \"object\"\n",
    "cat_type = 'category'\n",
    "non_num_cols = df.select_dtypes([object_type, cat_type]).columns\n",
    "print(non_num_cols)\n",
    "num_cols = df.select_dtypes(np.number).columns\n",
    "print(num_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Id': 1460, 'MSSubClass': 15, 'LotArea': 1073, 'OverallQual': 10, 'OverallCond': 9, 'YearBuilt': 112, 'YearRemodAdd': 61, 'BsmtFinSF1': 637, 'BsmtFinSF2': 144, 'BsmtUnfSF': 780, 'TotalBsmtSF': 721, '1stFlrSF': 753, '2ndFlrSF': 417, 'LowQualFinSF': 24, 'GrLivArea': 861, 'BsmtFullBath': 4, 'BsmtHalfBath': 3, 'FullBath': 4, 'HalfBath': 3, 'BedroomAbvGr': 8, 'KitchenAbvGr': 4, 'TotRmsAbvGrd': 12, 'Fireplaces': 4, 'GarageCars': 5, 'GarageArea': 441, 'WoodDeckSF': 274, 'OpenPorchSF': 202, 'EnclosedPorch': 120, '3SsnPorch': 20, 'ScreenPorch': 76, 'PoolArea': 8, 'MiscVal': 21, 'MoSold': 12, 'YrSold': 5, 'y': 663}\n"
     ]
    }
   ],
   "source": [
    "# let's consider the subset of numerical columns with few discrete values\n",
    "num_discrete = df.select_dtypes('int64').columns\n",
    "num_dis_count = [len(df[num_d].unique()) for num_d in num_discrete]  \n",
    "print(dict(zip(num_discrete, num_dis_count)))\n",
    "\n",
    "# we can see that a \"MSSubClass\" is a categorical feature\n",
    "# a number of features are not categorical by say, but can be treated as such: Bath related features, Fireplaces, GarageCars, and most importantly\n",
    "# OverallQual and OveralCond\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "Certain columns might contain corrupted data and thus require cleaning. I will start with categorical columns. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning categorical columns \n",
    "The main procedure is as follows:\n",
    "* replace the values that do not belong to the data description set of values by the one described there, mainly typos in string typed values\n",
    "* in the worst case drop rows that have values significantly different from the pre-determined categories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSZoning\n",
      "['RL' 'RM' 'C (all)' 'FV' 'RH']\n",
      "\n",
      "Street\n",
      "['Pave' 'Grvl']\n",
      "\n",
      "Alley\n",
      "[nan 'Grvl' 'Pave']\n",
      "\n",
      "LotShape\n",
      "['Reg' 'IR1' 'IR2' 'IR3']\n",
      "\n",
      "LandContour\n",
      "['Lvl' 'Bnk' 'Low' 'HLS']\n",
      "\n",
      "Utilities\n",
      "['AllPub' 'NoSeWa']\n",
      "\n",
      "LotConfig\n",
      "['Inside' 'FR2' 'Corner' 'CulDSac' 'FR3']\n",
      "\n",
      "LandSlope\n",
      "['Gtl' 'Mod' 'Sev']\n",
      "\n",
      "Neighborhood\n",
      "['CollgCr' 'Veenker' 'Crawfor' 'NoRidge' 'Mitchel' 'Somerst' 'NWAmes'\n",
      " 'OldTown' 'BrkSide' 'Sawyer' 'NridgHt' 'NAmes' 'SawyerW' 'IDOTRR'\n",
      " 'MeadowV' 'Edwards' 'Timber' 'Gilbert' 'StoneBr' 'ClearCr' 'NPkVill'\n",
      " 'Blmngtn' 'BrDale' 'SWISU' 'Blueste']\n",
      "\n",
      "Condition1\n",
      "['Norm' 'Feedr' 'PosN' 'Artery' 'RRAe' 'RRNn' 'RRAn' 'PosA' 'RRNe']\n",
      "\n",
      "Condition2\n",
      "['Norm' 'Artery' 'RRNn' 'Feedr' 'PosN' 'PosA' 'RRAn' 'RRAe']\n",
      "\n",
      "BldgType\n",
      "['1Fam' '2fmCon' 'Duplex' 'TwnhsE' 'Twnhs']\n",
      "\n",
      "HouseStyle\n",
      "['2Story' '1Story' '1.5Fin' '1.5Unf' 'SFoyer' 'SLvl' '2.5Unf' '2.5Fin']\n",
      "\n",
      "RoofStyle\n",
      "['Gable' 'Hip' 'Gambrel' 'Mansard' 'Flat' 'Shed']\n",
      "\n",
      "RoofMatl\n",
      "['CompShg' 'WdShngl' 'Metal' 'WdShake' 'Membran' 'Tar&Grv' 'Roll'\n",
      " 'ClyTile']\n",
      "\n",
      "Exterior1st\n",
      "['VinylSd' 'MetalSd' 'Wd Sdng' 'HdBoard' 'BrkFace' 'WdShing' 'CemntBd'\n",
      " 'Plywood' 'AsbShng' 'Stucco' 'BrkComm' 'AsphShn' 'Stone' 'ImStucc'\n",
      " 'CBlock']\n",
      "\n",
      "Exterior2nd\n",
      "['VinylSd' 'MetalSd' 'Wd Shng' 'HdBoard' 'Plywood' 'Wd Sdng' 'CmentBd'\n",
      " 'BrkFace' 'Stucco' 'AsbShng' 'Brk Cmn' 'ImStucc' 'AsphShn' 'Stone'\n",
      " 'Other' 'CBlock']\n",
      "\n",
      "MasVnrType\n",
      "['BrkFace' 'None' 'Stone' 'BrkCmn' nan]\n",
      "\n",
      "ExterQual\n",
      "['Gd' 'TA' 'Ex' 'Fa']\n",
      "\n",
      "ExterCond\n",
      "['TA' 'Gd' 'Fa' 'Po' 'Ex']\n",
      "\n",
      "Foundation\n",
      "['PConc' 'CBlock' 'BrkTil' 'Wood' 'Slab' 'Stone']\n",
      "\n",
      "BsmtQual\n",
      "['Gd' 'TA' 'Ex' nan 'Fa']\n",
      "\n",
      "BsmtCond\n",
      "['TA' 'Gd' nan 'Fa' 'Po']\n",
      "\n",
      "BsmtExposure\n",
      "['No' 'Gd' 'Mn' 'Av' nan]\n",
      "\n",
      "BsmtFinType1\n",
      "['GLQ' 'ALQ' 'Unf' 'Rec' 'BLQ' nan 'LwQ']\n",
      "\n",
      "BsmtFinType2\n",
      "['Unf' 'BLQ' nan 'ALQ' 'Rec' 'LwQ' 'GLQ']\n",
      "\n",
      "Heating\n",
      "['GasA' 'GasW' 'Grav' 'Wall' 'OthW' 'Floor']\n",
      "\n",
      "HeatingQC\n",
      "['Ex' 'Gd' 'TA' 'Fa' 'Po']\n",
      "\n",
      "CentralAir\n",
      "['Y' 'N']\n",
      "\n",
      "Electrical\n",
      "['SBrkr' 'FuseF' 'FuseA' 'FuseP' 'Mix' nan]\n",
      "\n",
      "KitchenQual\n",
      "['Gd' 'TA' 'Ex' 'Fa']\n",
      "\n",
      "Functional\n",
      "['Typ' 'Min1' 'Maj1' 'Min2' 'Mod' 'Maj2' 'Sev']\n",
      "\n",
      "FireplaceQu\n",
      "[nan 'TA' 'Gd' 'Fa' 'Ex' 'Po']\n",
      "\n",
      "GarageType\n",
      "['Attchd' 'Detchd' 'BuiltIn' 'CarPort' nan 'Basment' '2Types']\n",
      "\n",
      "GarageFinish\n",
      "['RFn' 'Unf' 'Fin' nan]\n",
      "\n",
      "GarageQual\n",
      "['TA' 'Fa' 'Gd' nan 'Ex' 'Po']\n",
      "\n",
      "GarageCond\n",
      "['TA' 'Fa' nan 'Gd' 'Po' 'Ex']\n",
      "\n",
      "PavedDrive\n",
      "['Y' 'N' 'P']\n",
      "\n",
      "PoolQC\n",
      "[nan 'Ex' 'Fa' 'Gd']\n",
      "\n",
      "Fence\n",
      "[nan 'MnPrv' 'GdWo' 'GdPrv' 'MnWw']\n",
      "\n",
      "MiscFeature\n",
      "[nan 'Shed' 'Gar2' 'Othr' 'TenC']\n",
      "\n",
      "SaleType\n",
      "['WD' 'New' 'COD' 'ConLD' 'ConLI' 'CWD' 'ConLw' 'Con' 'Oth']\n",
      "\n",
      "SaleCondition\n",
      "['Normal' 'Abnorml' 'Partial' 'AdjLand' 'Alloca' 'Family']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for col, uni_values in zip(non_num_cols, [df[col].unique() for col in non_num_cols]):\n",
    "    print(col)\n",
    "    print(uni_values)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# investigating the output of the previous cell as well as inspecting the content of the documentation\n",
    "# suggests a couple of corrupted values in certain columns such as Exterior2nd\n",
    "\n",
    "# let's define a function to replace these values both in the training and test dataframes\n",
    "\n",
    "def replace_values(col_names:list , wrong_correct:list, on='both'):\n",
    "    assert (isinstance(col_names, str) and isinstance(wrong_correct, dict)) or all([isinstance(l, dict) for l in wrong_correct]) and isinstance(col_names, list) \n",
    "    assert on in ['both', 'df', 'df_test']\n",
    "    \n",
    "    global df, df_test\n",
    "    \n",
    "    if isinstance(col_names, str):\n",
    "        for k, v in wrong_correct.items():\n",
    "            df = df.replace(k, v)\n",
    "            df_test = df_test.replace(k, v)\n",
    "        return \n",
    "    \n",
    "    try:\n",
    "        for col, dic in zip(col_names, wrong_correct):\n",
    "            for k, v in dic.items():\n",
    "                df = df.replace(k, v)\n",
    "                df_test = df_test.replace(k, v)\n",
    "                \n",
    "    except:\n",
    "        print(\"Something is wrong check again !!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have the value C (all) is correputed in MSZoning\n",
    "mszoning = \"MSZoning\"\n",
    "correct_mszoning = {\"C (all)\": \"C\"}\n",
    "ext2 = \"Exterior2nd\"\n",
    "correct_ext2 = {\"Brk Cmn\": \"BrkComm\", \"CmentBd\": \"CemntBd\"}\n",
    "\n",
    "ms_ext = [mszoning, ext2]\n",
    "correct = [correct_mszoning, correct_ext2]\n",
    "\n",
    "replace_values(ms_ext, correct)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning numerical columns\n",
    "This task is slightly trickier as it might require domain expertise. The main procedure is as follows:\n",
    "1. replace (or drop) values that contradict general common sense, for instance negative areas, months cannot be more than $12$\n",
    "2. consider the relationship between certain rows. values in a certain columns cannot be smaller / larger than the corresponding values in other columns. This step require more careful study of the nature of the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Id', 'MSSubClass', 'LotFrontage', 'LotArea', 'OverallQual',\n",
      "       'OverallCond', 'YearBuilt', 'YearRemodAdd', 'MasVnrArea', 'BsmtFinSF1',\n",
      "       'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF',\n",
      "       'LowQualFinSF', 'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath',\n",
      "       'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr', 'TotRmsAbvGrd',\n",
      "       'Fireplaces', 'GarageYrBlt', 'GarageCars', 'GarageArea', 'WoodDeckSF',\n",
      "       'OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea',\n",
      "       'MiscVal', 'MoSold', 'YrSold', 'y'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# let's first display the numerical columns\n",
    "print(num_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['LotArea', 'MasVnrArea', 'GrLivArea', 'GarageArea', 'PoolArea', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'LowQualFinSF', 'WoodDeckSF', 'OpenPorchSF']\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# let's consider areas\n",
    "areas = [col for col in num_cols if (\"area\" in col.strip().lower())]\n",
    "# inverstiagating the data description, the term SF generally refers to surface (area)\n",
    "areas.extend([col for col in num_cols if \"SF\" in col.strip()]) \n",
    "print(areas)\n",
    "# verify all values are positive\n",
    "areas_with_neg = [any(df[area] < 0) for area in areas]\n",
    "areas_with_neg = [area for area, a in zip(areas, areas_with_neg) if a]\n",
    "print(areas_with_neg)\n",
    "# as we can see all areas-values are positive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data description indicats that \n",
    "* OverallQuall and OverallCond should belong to the interval [1, 10]\n",
    "* YearRemodAdd is year of remodel, thus it should be larger or equal to YearBuilt\n",
    "* The relationship between the the year where the garage was built and the other year features should be investigated.\n",
    "* MSSubClass represents a label encoding of the different types of houses: values should belong to predetermined set of values specified in the data description\n",
    "* MoSold is the month where the house was sold. it should belong to [1, 12]\n",
    "* it might be worthwhile investigatinv any abnormalities in the relationship between \"BsmtFinSF1\", \"BsmtUnfSF\", \"BsmtFinSF2\" and \"TotalBsmtSF\" \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first of all let's define a method to replace the names of features in both training and test datasets\n",
    "\n",
    "def new_col_names(old_new_names:dict):\n",
    "    global df, df_test\n",
    "    try:\n",
    "        \n",
    "        df = df.rename(columns=old_new_names)\n",
    "        df_test = df_test.rename(columns=old_new_names)\n",
    "    except:\n",
    "        df_no_col = [col for col in old_new_names.keys if col not in df.columns]\n",
    "        df_test_no_col = [col for col in old_new_names.keys if col not in df_test.columns]\n",
    "        print(\"{cols} are not in the {dataf}\".format(df_no_col, \"training dataset\"))\n",
    "        print(\"{cols} are not in the {dataf}\".format(df_test_no_col, \"test dataset\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_new_cols = {\"OverallQual\": \"qua\", \n",
    "\"OverallCond\": \"cond\", \"YearBuilt\": \"Yb\", \"YearRemodAdd\": \"Yr\", \"MSSubClass\": \"mss\" , \n",
    "\"BsmtFinSF1\": \"bSF1\", \"BsmtFinSF2\": \"bSF2\", \"BsmtUnfSF\": \"bubf\", \"GarageYrBlt\":\"GYb\"}\n",
    "new_col_names(old_new_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# verify the integrity of overall quality and condition features\n",
    "qua = \"qua\"\n",
    "cond = \"cond\"\n",
    "print(all(df[qua].isin(range(1, 11))))\n",
    "print(all(df[cond].isin(range(1, 11))))\n",
    "\n",
    "# verify the remodeling and building years features\n",
    "yb = \"Yb\"\n",
    "yr = \"Yr\"\n",
    "print(df[df[yb] > df[yr]].empty) # the resulting dataframe is empty: no problems with neither of these features\n",
    "\n",
    "# verify the MSubclass features\n",
    "mss = \"mss\"\n",
    "mss_values = [20, 30, 40, 45, 50, 60, 70, 75, 80, 85, 90, 120, 150, 160, 180, 190]\n",
    "\n",
    "print(df[~df[mss].isin(mss_values)].empty) # all values from \"MSsubclass\" feature are under check\n",
    "\n",
    "# verify the month feature\n",
    "\n",
    "print(df[~df[\"MoSold\"].isin(range(1, 13))].empty) # all values of the month feature are correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let' define a function that applies a function to either \n",
    "# the whole dataframe or certain columns on the dataframe\n",
    "\n",
    "def apply_functions(funcs, col_names=None, on='both'):\n",
    "    # either have one function passed that should be applied to the whole dataframe\n",
    "    # or have an equal number of columns and functions where each funtion will be applied to the corresponding column\n",
    "    all_data = callable(funcs) and col_names is None\n",
    "    col_funcs = True\n",
    "    #  if the funcs argument is indeed a function, then the code below will raise an error \n",
    "    try:\n",
    "        col_funcs = (all([callable(f) for f in funcs]) and len(funcs) == len(col_names))\n",
    "    except:\n",
    "        col_funcs = False\n",
    "    \n",
    "    assert all_data or col_funcs\n",
    "   \n",
    "    global df, df_test\n",
    "    if col_names is None: # if the function is to be applied to the whole dataframe\n",
    "        df = df.apply(funcs, axis=1)\n",
    "        df_test = df_test.apply(funcs, axis=1)\n",
    "    else:\n",
    "        for col, f in zip(col_names, funcs):\n",
    "            df = df.apply(f)\n",
    "            df_test = df_test.apply(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         GYb    Yb    Yr\n",
      "29    1920.0  1927  1950\n",
      "93    1900.0  1910  1998\n",
      "324   1961.0  1967  2007\n",
      "600   2003.0  2005  2005\n",
      "736   1949.0  1950  1950\n",
      "1103  1954.0  1959  1959\n",
      "1376  1925.0  1930  1950\n",
      "1414  1922.0  1923  2000\n",
      "1418  1962.0  1963  1963\n",
      "         GYb    Yb    Yr\n",
      "61    1956.0  1959  1959\n",
      "116   2009.0  2010  2010\n",
      "345   1920.0  1935  1998\n",
      "380   1960.0  1978  1978\n",
      "435   1940.0  1941  1950\n",
      "437   1926.0  1935  1950\n",
      "662   1925.0  1945  1995\n",
      "803   2005.0  2006  2007\n",
      "1049  2005.0  2006  2006\n",
      "True\n",
      "         GYb    Yb    Yr\n",
      "29    1920.0  1927  1950\n",
      "93    1900.0  1910  1998\n",
      "324   1961.0  1967  2007\n",
      "600   2003.0  2005  2005\n",
      "736   1949.0  1950  1950\n",
      "1103  1954.0  1959  1959\n",
      "1376  1925.0  1930  1950\n",
      "1414  1922.0  1923  2000\n",
      "1418  1962.0  1963  1963\n",
      "         GYb    Yb    Yr\n",
      "61    1956.0  1959  1959\n",
      "116   2009.0  2010  2010\n",
      "345   1920.0  1935  1998\n",
      "380   1960.0  1978  1978\n",
      "435   1940.0  1941  1950\n",
      "437   1926.0  1935  1950\n",
      "662   1925.0  1945  1995\n",
      "803   2005.0  2006  2007\n",
      "1049  2005.0  2006  2006\n"
     ]
    }
   ],
   "source": [
    "# let's consider the year where the garage was built\n",
    "gyb = \"GYb\"\n",
    "print(df[df[gyb] < df[yb]][[gyb, yb, yr]])\n",
    "\n",
    "print(df_test[df_test[gyb] < df_test[yb]][[gyb, yb, yr]]) \n",
    "# with few exceptions the year where the garage if it is before the year where the house was built is generally few years earilier\n",
    "# which suggests that the garage was meant to be built with the house, yet the house took slightly longer to complete.\n",
    "\n",
    "# if GYb is less than Yb then we will set to Yb.\n",
    "def set_garage_year(row):\n",
    "    if row[gyb] < row[yb]:\n",
    "        row[gyb] = row[yb]\n",
    "    return row\n",
    "# set the changes in both train and test data\n",
    "print(callable(set_garage_year))\n",
    "apply_functions(set_garage_year)\n",
    "\n",
    "print(df[df[gyb] < df[yb]][[gyb, yb, yr]]) \n",
    "print(df_test[df_test[gyb] < df_test[yb]][[gyb, yb, yr]])\n",
    "# the changes were applied to both data sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputing missing values\n",
    "The second step is imputing the missing values and making sure no Nan values are passed to our machine learning models. The main procedure is as follows:\n",
    "1. drop all columns with a nan ratio exceeding a certain treshhold\n",
    "2. imput the missing values. The strategy depends mainly on the feature in question:\n",
    "    * if the feature is highly related to other features, then a highly accurate and natural value can be deduced\n",
    "    * some statistical value could be used to impute the missing values\n",
    "    * investigating related features could help come up with an aggregated value when the data is grouped by a number of features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1st strategy: drop columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LotFrontage      259\n",
      "Alley           1369\n",
      "MasVnrType         8\n",
      "MasVnrArea         8\n",
      "BsmtQual          37\n",
      "BsmtCond          37\n",
      "BsmtExposure      38\n",
      "BsmtFinType1      37\n",
      "BsmtFinType2      38\n",
      "Electrical         1\n",
      "FireplaceQu      690\n",
      "GarageType        81\n",
      "GYb               81\n",
      "GarageFinish      81\n",
      "GarageQual        81\n",
      "GarageCond        81\n",
      "PoolQC          1453\n",
      "Fence           1179\n",
      "MiscFeature     1406\n",
      "dtype: int64\n",
      "Index(['Alley', 'PoolQC', 'Fence', 'MiscFeature'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# let's first discover which columns have Nan values\n",
    "nan_values = df.isna().sum()\n",
    "cols_nan = nan_values[nan_values > 0]\n",
    "print(cols_nan)\n",
    "NAN_THRESHOLD = 0.8 # all columns with more than 0.8 nan values will be dropped\n",
    "cols_nan = cols_nan / len(df)\n",
    "# print(cols_nan)\n",
    "cols_nan_drop = cols_nan[cols_nan > NAN_THRESHOLD].index\n",
    "print(cols_nan_drop) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see: Alley, PoolQC, fence and MiscFeature have an extremely high ratio of nan values. \n",
    "before dropping these columns, it is necessary to drop features tightly related to those.  \n",
    "Investigating the data description reveals that:  \n",
    "* PoolQC is related to PoolArea\n",
    "* MiscFeature is related to MiscVal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    1453\n",
      "Name: PoolArea, dtype: int64\n",
      "197     235000\n",
      "810     181000\n",
      "1170    171000\n",
      "1182    745000\n",
      "1298    160000\n",
      "1386    250000\n",
      "1423    274970\n",
      "Name: y, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# let's better understand those relations\n",
    "pq = \"PoolQC\"\n",
    "pa = \"PoolArea\"\n",
    "print(df[df[pq].isna()][pa].value_counts()) \n",
    "# Nan values are associated with 0 pool area, which means that there is no pool in the first place\n",
    "# an extremely high ratio of houses do not have a pool.\n",
    "# let's consider the prices of houses with pool\n",
    "print(df[~df[pq].isna()][Y]) # we can see that the values too few and quite variant as well\n",
    "# thus is it better to drop this column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    1406\n",
      "Name: MiscVal, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "misf = \"MiscFeature\"\n",
    "misv = \"MiscVal\"\n",
    "print(df[df[misf].isna()][misv].value_counts()) # we can see that houses with Nan on MiscFeature \n",
    "# have 0 on MiscVal which means that they do not any additional features to mention\n",
    "# both of these features should be dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's drop the features in questions\n",
    "cols_nan_drop = cols_nan_drop.values.tolist() + [misv, pa]\n",
    "drop_cols(cols_nan_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2nd strategy: deduce values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LotFrontage      259\n",
      "Alley           1369\n",
      "MasVnrType         8\n",
      "MasVnrArea         8\n",
      "BsmtQual          37\n",
      "BsmtCond          37\n",
      "BsmtExposure      38\n",
      "BsmtFinType1      37\n",
      "BsmtFinType2      38\n",
      "Electrical         1\n",
      "FireplaceQu      690\n",
      "GarageType        81\n",
      "GYb               81\n",
      "GarageFinish      81\n",
      "GarageQual        81\n",
      "GarageCond        81\n",
      "PoolQC          1453\n",
      "Fence           1179\n",
      "MiscFeature     1406\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# let's consider the columns left with nan values\n",
    "nan_values = df.isna().sum()\n",
    "cols_nan = nan_values[nan_values > 0]\n",
    "print(cols_nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values with Nan can be divided into categories:\n",
    "* Masonry veneer \n",
    "* Basement\n",
    "* Garage\n",
    "* FirePlace\n",
    "* LotFrontage (only one column) and Electrical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_nan(col_names:list, fill_values:list):\n",
    "    one = isinstance(col_names, str) and isinstance(fill_values, str)\n",
    "    try:\n",
    "        many = len(col_names) == len(fill_values)\n",
    "    except:\n",
    "        many = False\n",
    "    assert one or many \n",
    "    global df, df_test\n",
    "    if many: \n",
    "        for col, v in zip(col_names, fill_values):\n",
    "            df[col] = df[col].fillna(v)\n",
    "            df_test[col] = df_test[col].fillna(v)\n",
    "    else:\n",
    "        df[col_names] = df[col_names].fillna(fill_values)\n",
    "        df_test[col_names] = df_test[col_names].fillna(fill_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      MasVnrArea MasVnrType\n",
      "234          NaN        NaN\n",
      "529          NaN        NaN\n",
      "650          NaN        NaN\n",
      "936          NaN        NaN\n",
      "973          NaN        NaN\n",
      "977          NaN        NaN\n",
      "1243         NaN        NaN\n",
      "1278         NaN        NaN\n"
     ]
    }
   ],
   "source": [
    "# let's consider Masonry Veneer columns\n",
    "msvt = \"MasVnrType\"\n",
    "msva = \"MasVnrArea\"\n",
    "\n",
    "print(df[(df[msvt].isna()) | (df[msva].isna())][[msva, msvt]]) \n",
    "# we can see that type and are either both nan or both non-nan\n",
    "# a reasonable assumption is that there is no Masonry Veneer\n",
    "\n",
    "set_nan([msvt, msva], ['None', 0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    81\n",
      "Name: GarageArea, dtype: int64\n",
      "Series([], Name: GYb, dtype: int64)\n"
     ]
    }
   ],
   "source": [
    "# let's consider the garage columns\n",
    "gt = \"GarageType\"\n",
    "gf = \"GarageFinish\"\n",
    "gc = \"GarageCars\"\n",
    "ga = \"GarageArea\"\n",
    "gcond = \"GarageCond\"\n",
    "gqua = \"GarageQual\"\n",
    "\n",
    "g = [gt, gyb, gf, gc, ga, gcond, gqua]\n",
    "print(df[df[gt].isna() | df[gyb].isna() | df[gf].isna() | df[gcond].isna() | df[gqua].isna()][ga].value_counts())\n",
    "# if any of the garage features in a certain row is set to Nan, then its area is 0\n",
    "# which means there is no garage\n",
    "\n",
    "# let's first consider the Garage built year feature for such rows\n",
    "print(df[df[gt].isna() | df[gf].isna() | df[gcond].isna() | df[gqua].isna()][gyb].value_counts())\n",
    "\n",
    "set_nan(g, [\"NA\", 0, \"NA\", 0, 0, \"NA\", \"NA\"])\n",
    "\n",
    "garage_old_new = {gt: \"gt\", gf:\"gf\", gc:\"gc\", ga:\"ga\", gcond:\"gcond\", gqua:\"gqua\"}\n",
    "\n",
    "new_col_names(garage_old_new)\n",
    "\n",
    "gt = \"gt\"\n",
    "gf = \"gf\"\n",
    "gc = \"gc\"\n",
    "ga = \"ga\"\n",
    "gcond = \"gcond\"\n",
    "gqua = \"gqua\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    37\n",
      "Name: bSF1, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# let's consider the basement nan-values\n",
    "bqua = \"BsmtQual\"\n",
    "bcond = \"BsmtCond\"\n",
    "bexp = \"BsmtExposure\"\n",
    "bf2 = \"BsmtFinType2\"\n",
    "bf1 = \"BsmtFinType1\"\n",
    "baf1 = \"bSF1\"\n",
    "baf2 = \"bSF2\"\n",
    "\n",
    "# BsmtQual         37\n",
    "# BsmtCond         37\n",
    "# BsmtExposure     38\n",
    "# BsmtFinType1     37\n",
    "# BsmtFinType2     38\n",
    "\n",
    "print(df[df[bqua].isna() & df[bcond].isna() & df[bexp].isna() & df[bf1].isna() ][baf1].value_counts())\n",
    "# we can assume that Nan values for each of these basement features reflect No basement\n",
    "\n",
    "set_nan([bqua, bcond, bexp, bf1, bf2], [\"NA\"] * 5)\n",
    "# change the basement's features \n",
    "\n",
    "basement_new_old = {bqua: \"bqua\", bcond: \"bcond\", bexp: \"bexp\", bf1:\"bf1\", bf2: \"bf2\"}\n",
    "new_col_names(basement_new_old)\n",
    "\n",
    "bqua = \"bqua\"\n",
    "bcond = \"bcond\"\n",
    "bexp = \"bexp\"\n",
    "bf2 = \"bf2\"\n",
    "bf1 = \"bf1\"\n",
    "baf1 = \"baf1\"\n",
    "baf2 = \"baf2\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    690\n",
      "Name: Fireplaces, dtype: int64\n",
      "SBrkr    1334\n",
      "FuseA      94\n",
      "FuseF      27\n",
      "FuseP       3\n",
      "Mix         1\n",
      "Name: Electrical, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# let's check the rest real quick\n",
    "# LotFrontage     259\n",
    "# Electrical        1\n",
    "# FireplaceQu     690\n",
    "\n",
    "firequa = \"FireplaceQu\"\n",
    "firep = \"Fireplaces\"\n",
    "print(df[df[firequa].isna()][firep].value_counts())\n",
    "# so nan values refer to NOn existing fire places\n",
    "set_nan(firequa, \"NA\")\n",
    "\n",
    "e = \"Electrical\"\n",
    "print(df[e].value_counts())\n",
    "lf = \"LotFrontage\"\n",
    "# we can assume that the only missing value is SBrkr with a high statistical possibility\n",
    "# the simplest solution for LotFrontage is to nan values to 0\n",
    "set_nan([e, lf], [\"SBrkr\", 0])\n",
    "\n",
    "fire_new_old = {firequa:\"firequa\", firep:\"firep\"}\n",
    "new_col_names(fire_new_old)\n",
    "\n",
    "firequa = \"firequa\"\n",
    "firep = \"firep\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(df[df.isna()].sum().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode the categorial features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first let's consider once again categorical and numerical features\n",
    "non_num_cols = df.select_dtypes([object_type, cat_type]).columns\n",
    "# print(non_num_cols)\n",
    "num_cols = df.select_dtypes(np.number).columns.values.tolist()\n",
    "# print(num_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the current state of the dataframes of later modifications\n",
    "df_cat = df.copy()\n",
    "df_t_cat = df_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ordinal_non_num = [\"LotShape\", \"Utilities\", \"LandSlope\", \"ExterQual\"\n",
    "# , \"ExterCond\", \"bqua\", \"bcond\", \"bexp\", \"bf1\", \"bf2\", \"Heating\", \"HeatingQC\", \"Electrical\", \n",
    "# \"KitchenQual\", \"Functional\", \"firequa\", \"gf\", \"gqua\", \"gcond\", \"PavedDrive\"]\n",
    " \n",
    "# let's define the orders for each of the ordinal columns\n",
    "usual_levels = [\"NA\", \"Po\", \"Fa\", \"TA\", \"Gd\", \"Ex\"]\n",
    "\n",
    "ordered_levels = {\n",
    "    \"ExterQual\": usual_levels,\n",
    "    \"ExterCond\": usual_levels,\n",
    "    \"bqua\": usual_levels,\n",
    "    \"bcond\": usual_levels,\n",
    "    \"HeatingQC\": usual_levels,\n",
    "    \"KitchenQual\": usual_levels,\n",
    "    \"firequa\": usual_levels,\n",
    "    \"gqua\": usual_levels,\n",
    "    \"gcond\": usual_levels,\n",
    "    \"LotShape\": [\"Reg\", \"IR1\", \"IR2\", \"IR3\"],\n",
    "    \"LandSlope\": [\"Sev\", \"Mod\", \"Gtl\"],\n",
    "    \"bexp\": [\"NA\", \"No\", \"Mn\", \"Av\", \"Gd\"],\n",
    "    \"bf1\": [\"NA\", \"Unf\", \"LwQ\", \"Rec\", \"BLQ\", \"ALQ\", \"GLQ\"],\n",
    "    \"bf2\": [\"NA\", \"Unf\", \"LwQ\", \"Rec\", \"BLQ\", \"ALQ\", \"GLQ\"],\n",
    "    \"Functional\": [\"Sal\", \"Sev\", \"Maj1\", \"Maj2\", \"Mod\", \"Min2\", \"Min1\", \"Typ\"],\n",
    "    \"gf\": [\"NA\", \"Unf\", \"RFn\", \"Fin\"],\n",
    "    \"PavedDrive\": [\"N\", \"P\", \"Y\"],\n",
    "    \"Utilities\": [\"NoSeWa\", \"NoSewr\", \"AllPub\"],\n",
    "    \"CentralAir\": [\"N\", \"Y\"],\n",
    "    \"Electrical\": [\"Mix\", \"FuseP\", \"FuseF\", \"FuseA\", \"SBrkr\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's encode ordinal data\n",
    "def cat_to_ord(col:list, categories:list, ordered:bool=True):\n",
    "    global df, df_test\n",
    "    final_cat = categories if ordered else categories[::-1]\n",
    "    df[col] = df[col].apply(dict(zip(final_cat, range(0, len(final_cat)))).get)\n",
    "    df_test[col] = df_test[col].apply(dict(zip(final_cat, range(0, len(final_cat)))).get)\n",
    "\n",
    "\n",
    "for k, v in ordered_levels.items():\n",
    "    cat_to_ord(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3    906\n",
      "4    488\n",
      "5     52\n",
      "2     14\n",
      "Name: ExterQual, dtype: int64\n",
      "3    1282\n",
      "4     146\n",
      "2      28\n",
      "5       3\n",
      "1       1\n",
      "Name: ExterCond, dtype: int64\n",
      "3    649\n",
      "4    618\n",
      "5    121\n",
      "0     37\n",
      "2     35\n",
      "Name: bqua, dtype: int64\n",
      "3    1311\n",
      "4      65\n",
      "2      45\n",
      "0      37\n",
      "1       2\n",
      "Name: bcond, dtype: int64\n",
      "5    741\n",
      "3    428\n",
      "4    241\n",
      "2     49\n",
      "1      1\n",
      "Name: HeatingQC, dtype: int64\n",
      "3    735\n",
      "4    586\n",
      "5    100\n",
      "2     39\n",
      "Name: KitchenQual, dtype: int64\n",
      "0    690\n",
      "4    380\n",
      "3    313\n",
      "2     33\n",
      "5     24\n",
      "1     20\n",
      "Name: firequa, dtype: int64\n",
      "3    1311\n",
      "0      81\n",
      "2      48\n",
      "4      14\n",
      "5       3\n",
      "1       3\n",
      "Name: gqua, dtype: int64\n",
      "3    1326\n",
      "0      81\n",
      "2      35\n",
      "4       9\n",
      "1       7\n",
      "5       2\n",
      "Name: gcond, dtype: int64\n",
      "0    925\n",
      "1    484\n",
      "2     41\n",
      "3     10\n",
      "Name: LotShape, dtype: int64\n",
      "2    1382\n",
      "1      65\n",
      "0      13\n",
      "Name: LandSlope, dtype: int64\n",
      "1    953\n",
      "3    221\n",
      "4    134\n",
      "2    114\n",
      "0     38\n",
      "Name: bexp, dtype: int64\n",
      "1    430\n",
      "6    418\n",
      "5    220\n",
      "4    148\n",
      "3    133\n",
      "2     74\n",
      "0     37\n",
      "Name: bf1, dtype: int64\n",
      "1    1256\n",
      "3      54\n",
      "2      46\n",
      "0      38\n",
      "4      33\n",
      "5      19\n",
      "6      14\n",
      "Name: bf2, dtype: int64\n",
      "7    1360\n",
      "5      34\n",
      "6      31\n",
      "4      15\n",
      "2      14\n",
      "3       5\n",
      "1       1\n",
      "Name: Functional, dtype: int64\n",
      "1    605\n",
      "2    422\n",
      "3    352\n",
      "0     81\n",
      "Name: gf, dtype: int64\n",
      "2    1340\n",
      "0      90\n",
      "1      30\n",
      "Name: PavedDrive, dtype: int64\n",
      "2    1459\n",
      "0       1\n",
      "Name: Utilities, dtype: int64\n",
      "1    1365\n",
      "0      95\n",
      "Name: CentralAir, dtype: int64\n",
      "4    1335\n",
      "3      94\n",
      "2      27\n",
      "1       3\n",
      "0       1\n",
      "Name: Electrical, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "for k in ordered_levels.keys():\n",
    "    print(df[k].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's encode non ordinal data using the label encoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "non_ord_cat = [cat for cat in non_num_cols if cat not in ordered_levels.keys()]\n",
    "\n",
    "le = LabelEncoder()\n",
    "\n",
    "for col in non_ord_cat:\n",
    "    df[col] = le.fit_transform(df[col].values)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FeatureEngineering\n",
    "In this section we will work on improving the performance by creating a number of sythetic features out of the given ones. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's save the dataframe before introducing any new feature\n",
    "df_base = df.copy()\n",
    "df_test_base = df_test.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline performance\n",
    "Creating a baseline model with the initial (left) features gives us a ground on which I base my next decisions. I will use the same performance metric as in the competition. The baseline model would be a sophisticated RandomForest model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definining the cross validation procedure\n",
    "from sklearn.model_selection import KFold\n",
    "n_splits = 5 \n",
    "random_state = 3\n",
    "shuffle = True\n",
    "kf = KFold(n_splits=n_splits, random_state=random_state, shuffle=shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['accuracy', 'adjusted_mutual_info_score', 'adjusted_rand_score', 'average_precision', 'balanced_accuracy', 'completeness_score', 'explained_variance', 'f1', 'f1_macro', 'f1_micro', 'f1_samples', 'f1_weighted', 'fowlkes_mallows_score', 'homogeneity_score', 'jaccard', 'jaccard_macro', 'jaccard_micro', 'jaccard_samples', 'jaccard_weighted', 'matthews_corrcoef', 'max_error', 'mutual_info_score', 'neg_brier_score', 'neg_log_loss', 'neg_mean_absolute_error', 'neg_mean_absolute_percentage_error', 'neg_mean_gamma_deviance', 'neg_mean_poisson_deviance', 'neg_mean_squared_error', 'neg_mean_squared_log_error', 'neg_median_absolute_error', 'neg_root_mean_squared_error', 'normalized_mutual_info_score', 'precision', 'precision_macro', 'precision_micro', 'precision_samples', 'precision_weighted', 'r2', 'rand_score', 'recall', 'recall_macro', 'recall_micro', 'recall_samples', 'recall_weighted', 'roc_auc', 'roc_auc_ovo', 'roc_auc_ovo_weighted', 'roc_auc_ovr', 'roc_auc_ovr_weighted', 'top_k_accuracy', 'v_measure_score']\n"
     ]
    }
   ],
   "source": [
    "print(sklearn.metrics.get_scorer_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the main score of the competition is the square of log error squared\n",
    "# let's define a function to calculate a model's performance according to this metric\n",
    "scoring = \"neg_mean_squared_error\"\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from xgboost import XGBRegressor as xgr\n",
    "def model_performance(X, y, model=xgr(seed=0)):\n",
    "    global kf, scoring\n",
    "    log_y = np.log(y)\n",
    "    score = cross_val_score(model, X, log_y, cv=5, scoring=scoring)\n",
    "    return np.sqrt(-score.mean()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.13934807788542816\n"
     ]
    }
   ],
   "source": [
    "X = df.copy()\n",
    "y = X.pop(Y)\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rf_base = RandomForestRegressor(max_depth=5, n_estimators=100, random_state=3)\n",
    "base_score = model_performance(X, y)\n",
    "print(base_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Informative features: mutual information\n",
    "The mutual information is quite a powerful and general technique to determine the relevance of features with respect to the target variable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import mutual_info_regression\n",
    "\n",
    "def make_mi_scores(X, y):\n",
    "    X = X.copy()\n",
    "    # discrete features are the ones with type int\n",
    "    discrete_features = [pd.api.types.is_integer_dtype(t) for t in X.dtypes]\n",
    "\n",
    "    mi_scores = mutual_info_regression(X, y, discrete_features=discrete_features, random_state=0)\n",
    "    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)\n",
    "    mi_scores = mi_scores.sort_values(ascending=False)\n",
    "    return mi_scores\n",
    "\n",
    "\n",
    "def plot_mi_scores(scores):\n",
    "    scores = scores.sort_values(ascending=True)\n",
    "    width = np.arange(len(scores))\n",
    "    ticks = list(scores.index)\n",
    "    plt.barh(width, scores)\n",
    "    plt.yticks(width, ticks)\n",
    "    plt.title(\"Mutual Information Scores\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found array with 0 sample(s) (shape=(0, 1)) while a minimum of 1 is required.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/ayhem18/Ayhem18/DEV/Data_science/Competitions/Kaggle_Competitions/House_Prices/HP_2.ipynb Cell 60\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/ayhem18/Ayhem18/DEV/Data_science/Competitions/Kaggle_Competitions/House_Prices/HP_2.ipynb#Y113sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m mi_scores \u001b[39m=\u001b[39m make_mi_scores(X, y)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ayhem18/Ayhem18/DEV/Data_science/Competitions/Kaggle_Competitions/House_Prices/HP_2.ipynb#Y113sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m mi_scores\n",
      "\u001b[1;32m/home/ayhem18/Ayhem18/DEV/Data_science/Competitions/Kaggle_Competitions/House_Prices/HP_2.ipynb Cell 60\u001b[0m in \u001b[0;36mmake_mi_scores\u001b[0;34m(X, y)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ayhem18/Ayhem18/DEV/Data_science/Competitions/Kaggle_Competitions/House_Prices/HP_2.ipynb#Y113sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m# discrete features are the ones with type int\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ayhem18/Ayhem18/DEV/Data_science/Competitions/Kaggle_Competitions/House_Prices/HP_2.ipynb#Y113sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m discrete_features \u001b[39m=\u001b[39m [pd\u001b[39m.\u001b[39mapi\u001b[39m.\u001b[39mtypes\u001b[39m.\u001b[39mis_integer_dtype(t) \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m X\u001b[39m.\u001b[39mdtypes]\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/ayhem18/Ayhem18/DEV/Data_science/Competitions/Kaggle_Competitions/House_Prices/HP_2.ipynb#Y113sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m mi_scores \u001b[39m=\u001b[39m mutual_info_regression(X, y, discrete_features\u001b[39m=\u001b[39;49mdiscrete_features, random_state\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ayhem18/Ayhem18/DEV/Data_science/Competitions/Kaggle_Competitions/House_Prices/HP_2.ipynb#Y113sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m mi_scores \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mSeries(mi_scores, name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mMI Scores\u001b[39m\u001b[39m\"\u001b[39m, index\u001b[39m=\u001b[39mX\u001b[39m.\u001b[39mcolumns)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ayhem18/Ayhem18/DEV/Data_science/Competitions/Kaggle_Competitions/House_Prices/HP_2.ipynb#Y113sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m mi_scores \u001b[39m=\u001b[39m mi_scores\u001b[39m.\u001b[39msort_values(ascending\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/Ayhem18/DEV/Data_science/Towards_Data_Science/ds_env/lib/python3.10/site-packages/sklearn/feature_selection/_mutual_info.py:389\u001b[0m, in \u001b[0;36mmutual_info_regression\u001b[0;34m(X, y, discrete_features, n_neighbors, copy, random_state)\u001b[0m\n\u001b[1;32m    313\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmutual_info_regression\u001b[39m(\n\u001b[1;32m    314\u001b[0m     X, y, \u001b[39m*\u001b[39m, discrete_features\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mauto\u001b[39m\u001b[39m\"\u001b[39m, n_neighbors\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m, copy\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, random_state\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m\n\u001b[1;32m    315\u001b[0m ):\n\u001b[1;32m    316\u001b[0m     \u001b[39m\"\"\"Estimate mutual information for a continuous target variable.\u001b[39;00m\n\u001b[1;32m    317\u001b[0m \n\u001b[1;32m    318\u001b[0m \u001b[39m    Mutual information (MI) [1]_ between two random variables is a non-negative\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[39m           of a Random Vector\", Probl. Peredachi Inf., 23:2 (1987), 9-16\u001b[39;00m\n\u001b[1;32m    388\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 389\u001b[0m     \u001b[39mreturn\u001b[39;00m _estimate_mi(X, y, discrete_features, \u001b[39mFalse\u001b[39;49;00m, n_neighbors, copy, random_state)\n",
      "File \u001b[0;32m~/Ayhem18/DEV/Data_science/Towards_Data_Science/ds_env/lib/python3.10/site-packages/sklearn/feature_selection/_mutual_info.py:305\u001b[0m, in \u001b[0;36m_estimate_mi\u001b[0;34m(X, y, discrete_features, discrete_target, n_neighbors, copy, random_state)\u001b[0m\n\u001b[1;32m    298\u001b[0m     y \u001b[39m=\u001b[39m scale(y, with_mean\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m    299\u001b[0m     y \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (\n\u001b[1;32m    300\u001b[0m         \u001b[39m1e-10\u001b[39m\n\u001b[1;32m    301\u001b[0m         \u001b[39m*\u001b[39m np\u001b[39m.\u001b[39mmaximum(\u001b[39m1\u001b[39m, np\u001b[39m.\u001b[39mmean(np\u001b[39m.\u001b[39mabs(y)))\n\u001b[1;32m    302\u001b[0m         \u001b[39m*\u001b[39m rng\u001b[39m.\u001b[39mstandard_normal(size\u001b[39m=\u001b[39mn_samples)\n\u001b[1;32m    303\u001b[0m     )\n\u001b[0;32m--> 305\u001b[0m mi \u001b[39m=\u001b[39m [\n\u001b[1;32m    306\u001b[0m     _compute_mi(x, y, discrete_feature, discrete_target, n_neighbors)\n\u001b[1;32m    307\u001b[0m     \u001b[39mfor\u001b[39;00m x, discrete_feature \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(_iterate_columns(X), discrete_mask)\n\u001b[1;32m    308\u001b[0m ]\n\u001b[1;32m    310\u001b[0m \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39marray(mi)\n",
      "File \u001b[0;32m~/Ayhem18/DEV/Data_science/Towards_Data_Science/ds_env/lib/python3.10/site-packages/sklearn/feature_selection/_mutual_info.py:306\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    298\u001b[0m     y \u001b[39m=\u001b[39m scale(y, with_mean\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m    299\u001b[0m     y \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (\n\u001b[1;32m    300\u001b[0m         \u001b[39m1e-10\u001b[39m\n\u001b[1;32m    301\u001b[0m         \u001b[39m*\u001b[39m np\u001b[39m.\u001b[39mmaximum(\u001b[39m1\u001b[39m, np\u001b[39m.\u001b[39mmean(np\u001b[39m.\u001b[39mabs(y)))\n\u001b[1;32m    302\u001b[0m         \u001b[39m*\u001b[39m rng\u001b[39m.\u001b[39mstandard_normal(size\u001b[39m=\u001b[39mn_samples)\n\u001b[1;32m    303\u001b[0m     )\n\u001b[1;32m    305\u001b[0m mi \u001b[39m=\u001b[39m [\n\u001b[0;32m--> 306\u001b[0m     _compute_mi(x, y, discrete_feature, discrete_target, n_neighbors)\n\u001b[1;32m    307\u001b[0m     \u001b[39mfor\u001b[39;00m x, discrete_feature \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(_iterate_columns(X), discrete_mask)\n\u001b[1;32m    308\u001b[0m ]\n\u001b[1;32m    310\u001b[0m \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39marray(mi)\n",
      "File \u001b[0;32m~/Ayhem18/DEV/Data_science/Towards_Data_Science/ds_env/lib/python3.10/site-packages/sklearn/feature_selection/_mutual_info.py:161\u001b[0m, in \u001b[0;36m_compute_mi\u001b[0;34m(x, y, x_discrete, y_discrete, n_neighbors)\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[39mreturn\u001b[39;00m mutual_info_score(x, y)\n\u001b[1;32m    160\u001b[0m \u001b[39melif\u001b[39;00m x_discrete \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m y_discrete:\n\u001b[0;32m--> 161\u001b[0m     \u001b[39mreturn\u001b[39;00m _compute_mi_cd(y, x, n_neighbors)\n\u001b[1;32m    162\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m x_discrete \u001b[39mand\u001b[39;00m y_discrete:\n\u001b[1;32m    163\u001b[0m     \u001b[39mreturn\u001b[39;00m _compute_mi_cd(x, y, n_neighbors)\n",
      "File \u001b[0;32m~/Ayhem18/DEV/Data_science/Towards_Data_Science/ds_env/lib/python3.10/site-packages/sklearn/feature_selection/_mutual_info.py:138\u001b[0m, in \u001b[0;36m_compute_mi_cd\u001b[0;34m(c, d, n_neighbors)\u001b[0m\n\u001b[1;32m    135\u001b[0m c \u001b[39m=\u001b[39m c[mask]\n\u001b[1;32m    136\u001b[0m radius \u001b[39m=\u001b[39m radius[mask]\n\u001b[0;32m--> 138\u001b[0m kd \u001b[39m=\u001b[39m KDTree(c)\n\u001b[1;32m    139\u001b[0m m_all \u001b[39m=\u001b[39m kd\u001b[39m.\u001b[39mquery_radius(c, radius, count_only\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, return_distance\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m    140\u001b[0m m_all \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(m_all) \u001b[39m-\u001b[39m \u001b[39m1.0\u001b[39m\n",
      "File \u001b[0;32msklearn/neighbors/_binary_tree.pxi:833\u001b[0m, in \u001b[0;36msklearn.neighbors._kd_tree.BinaryTree.__init__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/Ayhem18/DEV/Data_science/Towards_Data_Science/ds_env/lib/python3.10/site-packages/sklearn/utils/validation.py:909\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    907\u001b[0m     n_samples \u001b[39m=\u001b[39m _num_samples(array)\n\u001b[1;32m    908\u001b[0m     \u001b[39mif\u001b[39;00m n_samples \u001b[39m<\u001b[39m ensure_min_samples:\n\u001b[0;32m--> 909\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    910\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mFound array with \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m sample(s) (shape=\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m) while a\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    911\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m minimum of \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m is required\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    912\u001b[0m             \u001b[39m%\u001b[39m (n_samples, array\u001b[39m.\u001b[39mshape, ensure_min_samples, context)\n\u001b[1;32m    913\u001b[0m         )\n\u001b[1;32m    915\u001b[0m \u001b[39mif\u001b[39;00m ensure_min_features \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m array\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[1;32m    916\u001b[0m     n_features \u001b[39m=\u001b[39m array\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]\n",
      "\u001b[0;31mValueError\u001b[0m: Found array with 0 sample(s) (shape=(0, 1)) while a minimum of 1 is required."
     ]
    }
   ],
   "source": [
    "mi_scores = make_mi_scores(X, y)\n",
    "mi_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "irrelevant_feats = mi_scores[mi_scores < 0.01].index.values.tolist()\n",
    "\n",
    "drop_cols(irrelevant_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.copy()\n",
    "y = X.pop(Y) \n",
    "\n",
    "new_score = model_performance(X, y)\n",
    "print(new_score) # a very small gain is achieved out of removing the unformative features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mi_scores.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quality feature\n",
    "This subsection I try to understand the Quality feature and its relations to the other features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's create a a function that returns three lists: num, ord, cat features\n",
    "def feature_types():\n",
    "    global df, df_test\n",
    "    # using the dataframe version before the numerical encoding\n",
    "    num = df_cat.select_dtypes(np.number).columns.values.tolist()\n",
    "    ord = [k for k in ordered_levels.keys()]\n",
    "    # filter the columns that are still in the dataframe\n",
    "    num = [col for col in num if col in df.columns]\n",
    "    ord = [col for col in ord if col in df.columns]\n",
    "    # the columns left in df that do not belong to the previous two are categorical\n",
    "    cat = [k for k in df.columns if k not in ord and k not in num]\n",
    "    return num, ord, cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols, ord_cols, cat_cols = feature_types()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_or_ordinal = num_cols  + ord_cols\n",
    "corr_with_qua = df.loc[:, num_or_ordinal].corr()[qua] \n",
    "corr_with_qua = corr_with_qua[(corr_with_qua > 0.4) | (corr_with_qua < -0.4)]\n",
    "print(corr_with_qua.sort_values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_scatters(col_names, x=None):\n",
    "    global df, df_test\n",
    "    if x is None:\n",
    "        x = df.index.values.tolist()\n",
    "    for col in col_names:\n",
    "        plt.scatter(x=x, y=df[col].values)\n",
    "        \n",
    "    plt.title(str(col_names) + \" variation\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_funcs = ['count', np.mean, np.median, np.min, np.max]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ord_corr_with_qua = [col for col in corr_with_qua.index if col in ord_cols]\n",
    "for col in ord_corr_with_qua:\n",
    "    print(pd.pivot_table(df, index=col, values=qua, aggfunc=agg_funcs + [pd.Series.mode]))\n",
    "    # plot_scatters([qua, col])\n",
    "# as for ordinal columns quality is hightly correlated with External, basement and kitchen qualityes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's consider the numerical features\n",
    "num_corr_with_qua = [col for col in corr_with_qua.index if col in num_cols]\n",
    "for col in num_corr_with_qua:\n",
    "    print(df.groupby(qua).agg({col:agg_funcs}))\n",
    "# as for the numerical features, we have:\n",
    "# total area for basement, living area, year of building\n",
    "# fullbath, total rooms, gc, garea "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's consider a new feature: qua * cond\n",
    "df['state'] = df[qua] * df[cond]\n",
    "# print(df['state'].value_counts().sort_index())\n",
    "print(df[['state', Y, qua, cond]].corr()[Y])\n",
    "# as the condition is not as significant as the quality, it might be worth trying a different tranformation\n",
    "df['state'] = df[qua] *  np.floor(np.sqrt(df[cond]))\n",
    "print(df[['state', Y, qua, cond]].corr()[Y])\n",
    "# apparently this feature makes the best out of both features let's verify its effect with and without the old features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.copy()\n",
    "y = X.pop(Y)\n",
    "print(model_performance(X, y))\n",
    "# there is some slight improvement of 0.01 \n",
    "# # the feature is worth keeping\n",
    "df_test['state'] = df_test[qua] *  np.floor(np.sqrt(df_test[cond]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Garage features\n",
    "Garage is clearly an informative element. Some more investigation is needed to extract all its power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "garage_cols = [col for col in df.columns if col.lower().startswith(\"g\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd.pivot_table(df, index=gqua, values=qua, aggfunc=agg_funcs))\n",
    "print(df[[gqua, gcond, Y]].corr()[Y])\n",
    "# the new state of garage should be similar to the one created for the whole whouse\n",
    "# let's suggest the new state\n",
    "# 0, 1,2 -> 1/2 : quality is low anyway\n",
    "# 3 -> 1: only the quality matters in this case\n",
    "# in general quality is high, a better condition means a better state\n",
    "\n",
    "gcond2= \"gcond2\"\n",
    "def new_gcond(row):\n",
    "    row[gcond2] = 0\n",
    "    if row[gcond] in [0, 1, 2]:\n",
    "        row[gcond2] = 0.5\n",
    "    elif row[gcond] == 3:\n",
    "        row[gcond2] = 1\n",
    "    else:\n",
    "        row[gcond2] = 1.5\n",
    "    return row\n",
    "apply_functions(new_gcond, on='df')\n",
    "df['gstate'] = df[qua] * df[gcond2]\n",
    "print(df[[Y, gcond, gqua, gcond2, 'gstate']].corr()[Y]) \n",
    "# as we can see is highly correlated with the target variable\n",
    "# let's test the new feature\n",
    "X = df.copy()\n",
    "y = X.pop(Y)\n",
    "g_mi_scores =  make_mi_scores(X, y)[[gcond, gqua, gcond2, 'gstate']]\n",
    "print(g_mi_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# at the moment the new feature seems quite promising, let's inspect its effect on the performance\n",
    "print(model_performance(X, y))\n",
    "\n",
    "# let's try removing the different subsets of [gcon, gqua, gcond2]\n",
    "X1 = df.drop([gcond, gqua, gcond2], axis=1).copy()\n",
    "y1 = X1.pop(Y)\n",
    "print(model_performance(X1, y1))\n",
    "\n",
    "## uncomment the lines of code below to \n",
    "## see the performance with the different subsets of removed featurs\n",
    "\n",
    "# X2= df.drop([gqua, gcond2], axis=1).copy()\n",
    "# y2 = X2.pop(Y)\n",
    "# print(model_performance(X2, y2))\n",
    "\n",
    "# X3= df.drop([gcond, gcond2], axis=1).copy()\n",
    "# y3 = X3.pop(Y)\n",
    "# print(model_performance(X3, y3))\n",
    "\n",
    "# X4= df.drop([gcond, gqua], axis=1).copy()\n",
    "# y4 = X4.pop(Y)\n",
    "# print(model_performance(X4, y4))\n",
    "\n",
    "# X5= df.drop([gcond], axis=1).copy()\n",
    "# y5 = X5.pop(Y)\n",
    "# print(model_performance(X5, y5))\n",
    "\n",
    "# X6= df.drop([gcond2], axis=1).copy()\n",
    "# y6 = X6.pop(Y)\n",
    "# print(model_performance(X6, y6))\n",
    "\n",
    "# X7= df.drop([gqua], axis=1).copy()\n",
    "# y7 = X7.pop(Y)\n",
    "# print(model_performance(X7, y7))\n",
    "\n",
    "## even though the performance did not improve, the mi scores as well a\n",
    "## as the correlation scores are solid proofs of the usefulness of this feature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "garage_cols = [col for col in df.columns if col.lower().startswith(\"g\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's consider the different relation between the two garage areas\n",
    "print(df[[ga, gc, Y]].corr())\n",
    "print(pd.pivot_table(df, values=ga, index=gc, aggfunc=agg_funcs + [pd.Series.mode]))\n",
    "\n",
    "# there is practically no difference between having 3 and 4 car capacity in a garage\n",
    "# let's experiment with setting the 5 4-car capacity garages to 3\n",
    "replace_values(gc, {4:3})\n",
    "\n",
    "# let's consider the area_by car feature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('ds_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "006414dea9a04848ce797b510a25f3f28ac8668e3d3244e777242cca6bed477f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
