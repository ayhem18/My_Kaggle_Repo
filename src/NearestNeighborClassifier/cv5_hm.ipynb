{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7dd187bb804f6d23",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Lab4: Softmax And its Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f34d51e37b72ac6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-02T12:46:03.712374085Z",
     "start_time": "2023-10-02T12:46:03.671168696Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softmax(x: np.ndarray, \n",
    "            normalization: bool = True) -> np.ndarray:\n",
    "    # first squeeze the array in case it is multi-dimentional\n",
    "    x = np.squeeze(x)\n",
    "\n",
    "    if len(x.shape) != 1:\n",
    "        raise ValueError(f\"The input is expected to be 1 dimensional. Found: {len(x.shape)} dimensions\")\n",
    "\n",
    "    # the main idea behind 'normalization' is to avoid numerical overflow with the softmax function \n",
    "    # (mainly with the denominator as a sum of exponential functions). The output of softmax for (x1, x2, ... xn) is the same as the output for\n",
    "    # (x1 - C, x2 - C, ... xn - C) where C is any constant. (other operations such as division will alter the output)\n",
    "    # consider the following link for the mathematical details : https://jaykmody.com/blog/stable-softmax/\n",
    "\n",
    "    max_x = max(x) if normalization else 0\n",
    "    x = x - max_x # if normalization is False, then 'x' is the same as the original output, otherwise the maximum element will be subtracted from each element\n",
    "    sum_exp = np.sum(np.exp(x))\n",
    "    return np.exp(x) / sum_exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-02T12:46:03.723966759Z",
     "start_time": "2023-10-02T12:46:03.711762838Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def softmax_grad1(x: np.ndarray, normalization: bool = True) -> np.ndarray:\n",
    "    # the first step is to calculate the output of softmax on x\n",
    "    s = softmax(x, normalization=normalization)\n",
    "    s = np.expand_dims(s, axis=-1)\n",
    "    assert s.shape == (len(x), 1)\n",
    "    gradient =  - s @ np.transpose(s)\n",
    "    # the current variable contains only - S_i . S_j for all i, j. the S_i term should be added to the diagonal entries\n",
    "    for i in range(len(x)):\n",
    "        gradient[i][i] += s[i][0]\n",
    "    return gradient\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37e408578a9dfef",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Lab5: Adding Backpropagation from Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b040e5df",
   "metadata": {},
   "source": [
    "## Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f4c3b88e922d1bac",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def softmax_grad2(downstream_grad: np.ndarray, \n",
    "                  x: np.ndarray):\n",
    "    \"\"\"This function returns the gradient of the Loss with respect to the input \n",
    "\n",
    "    Args:\n",
    "        downstream_grad (np.ndarray): the gradient of the loss with respect to the output of the softmax functions\n",
    "        x (np.ndarray): input to the function\n",
    "    \"\"\"\n",
    "    if len(downstream_grad.shape) > 2: \n",
    "        raise ValueError((f\"The downstream gradient is expected to be at most {2} dimensional\\n\"\n",
    "                          f\"found {len(downstream_grad.shape)}\"))\n",
    "    if len(downstream_grad.shape) == 1:\n",
    "        downstream_grad = np.expand_dims(downstream_grad, axis=-1)\n",
    "\n",
    "    if len(downstream_grad) != len(x):\n",
    "        raise ValueError((f\"The function expects the input and the downstream grad to be of the same dimensions\\n\"\n",
    "                          f\"Found: input of length {len(x)} and downstream of dimension: {len(downstream_grad)}\"))     \n",
    "\n",
    "    # calculate the local gradient\n",
    "    local_grad = softmax_grad1(x)\n",
    "    # The entry e(i, j) in this matrix will represent the dS_i / d_xj\n",
    "    \n",
    "    # downstream_grad = [dL / dS_1, dL/d_S2, ... dL/dSn]\n",
    "    # dL / d_k = sum of dL / dS_i * d_Si / d_xk\n",
    "    # this can be simply computed with the usual matrix multiplication\n",
    "    final_grad = local_grad @ downstream_grad\n",
    "    return final_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "dae4d4fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.12599116]\n",
      " [ 0.39170594]\n",
      " [-0.26571478]]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([1, 2, 3])\n",
    "downgrad = np.array([-1, 2, 0])\n",
    "print(softmax_grad2(downstream_grad=downgrad, x=x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5d02c9",
   "metadata": {},
   "source": [
    "## Relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6c16fbc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLU(x: np.ndarray):\n",
    "    return x * (x > 0)\n",
    "\n",
    "def ReLU_grad1(x: np.ndarray, jacobian_matrix: bool = True) -> np.ndarray:\n",
    "    # the gradient of the relu function is quite simple:\n",
    "    # if x > 0, it is 1, otherwise it is 0\n",
    "    \n",
    "    # nevertheless, the gradient can be expressed either a vector, or a Jacobian matrix (2d matrix)\n",
    "    # since the i-th input x_i only affects the i-th output we have: all the non-digonal entries will be zero \n",
    "    if jacobian_matrix:\n",
    "        grad = np.eye(N=len(x))\n",
    "        for i, v in enumerate(x):\n",
    "            grad[i][i] = float(v > 0)\n",
    "        return grad\n",
    "    return (x > 0).astype(float)\n",
    "\n",
    "\n",
    "def ReLU_grad2(downstream_grad: np.ndarray, \n",
    "               x: np.ndarray) -> np.ndarray:\n",
    "    # the main difference between Softmax and ReLU is that the i-th input x_i only affects the i-th output x_i\n",
    "    # and thus \n",
    "\n",
    "\n",
    "    if len(downstream_grad.shape) > 2: \n",
    "        raise ValueError((f\"The downstream gradient is expected to be at most {2} dimensional\\n\"\n",
    "                          f\"found {len(downstream_grad.shape)}\"))\n",
    "    if len(downstream_grad.shape) == 1:\n",
    "        downstream_grad = np.expand_dims(downstream_grad, axis=-1)\n",
    "\n",
    "    if len(downstream_grad) != len(x):\n",
    "        raise ValueError((f\"The function expects the input and the downstream grad to be of the same dimensions\\n\"\n",
    "                          f\"Found: input of length {len(x)} and downstream of dimension: {len(downstream_grad)}\"))     \n",
    "\n",
    "    # calculate the local gradient\n",
    "    local_grad = ReLU_grad1(x, jacobian_matrix=True)\n",
    "    final_grad = local_grad @ downstream_grad\n",
    "    return final_grad    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "07b080ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4.]\n",
      " [0.]\n",
      " [5.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "# let's use the example from the lecture\n",
    "x = np.array([1, -2, 3, -1])\n",
    "downgrad = np.array([4, -1, 5, 9])\n",
    "print(ReLU_grad2(downstream_grad=downgrad, x=x))\n",
    "# the results match the ones in the 5-th lecture: 26-th slide"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7a6ec6",
   "metadata": {},
   "source": [
    "## Linear Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fc358cd07fded426",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# this function simply computes the \n",
    "from typing import List\n",
    "\n",
    "def linear(x: np.ndarray,\n",
    "           weight_matrix: np.ndarray):\n",
    "    if len(x.shape) == 1:\n",
    "        x = np.expand_dims(x, axis=-1)\n",
    "    if len(x.shape) != 2 or x.shape[-1] != 1:\n",
    "        raise ValueError(f\"The input is expected to be a column vector: of the shape {'n', 1}\\n\"\n",
    "                         f\"Found: {x.shape}\")\n",
    "    # the weights are expected to be of the shape (None, x.shape[0])\n",
    "    if len(weight_matrix.shape) != 2 and weight_matrix.shape[-1] != x.shape[0]:\n",
    "        raise ValueError(f\"The weight matrix second dimension and the length of the vector are expected to match\")\n",
    "\n",
    "    return weight_matrix @ x\n",
    "\n",
    "def linear_grad1(x: np.ndarray, weight_matrix: np.ndarray) -> List[np.ndarray]:\n",
    "    n, k1 = weight_matrix.shape\n",
    "    k2, m = x.shape\n",
    "    if k1 != k2:\n",
    "        raise ValueError(f\"The dimensions do not match\")\n",
    "    k = k1\n",
    "\n",
    "    # the linear function accepts matrix input and generate matrix output\n",
    "    # this function will return '|W|' matrices where each matrix represents the gradients o\n",
    "    # of result with respect to w_ij\n",
    "\n",
    "    def gradient_matrix(i, j):\n",
    "        result = np.zeros(shape=(n, m), dtype=np.float32)\n",
    "        # set the i-th row of the result matrix to the j-th column of the 'x' matrix \n",
    "        for index in range(m):\n",
    "            result[i, index] = x[j, index] \n",
    "        return result\n",
    "    \n",
    "    grads = [gradient_matrix(i, j) for j in range(k1) for i in range(n)]\n",
    "    return grads\n",
    "\n",
    "def linear_grad2(downstream_grad, x: np.ndarray, weight_matrix: np.ndarray) -> np.ndarray:\n",
    "    if len(x.shape) == 1:\n",
    "        x = np.expand_dims(x, axis=-1)\n",
    "    n, k1 = weight_matrix.shape\n",
    "    k2, m = x.shape\n",
    "    \n",
    "    # make sure the downstream_grad is of the shape (n, m)\n",
    "    if downstream_grad.shape != (n, m):\n",
    "        raise ValueError(\"The downstream_grad must match the dimension of the output of the linear layer\")\n",
    "\n",
    "    local_grad = linear_grad1(x, weight_matrix)\n",
    "    final_grad = np.zeros(shape=(n, k1))\n",
    "\n",
    "    for i in range(n):\n",
    "        for j in range(k1):\n",
    "            final_grad[i, j] = np.sum(local_grad[j * k1 + i] * downstream_grad)\n",
    "    return final_grad    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "15889192",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.088, 0.176],\n",
       "       [0.104, 0.208]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([0.2, 0.4])\n",
    "wm = np.array([[0.1, 0.5], [-0.3, 0.8]])\n",
    "linear(x=x, weight_matrix=wm)\n",
    "downgrad = np.array([[0.44], [0.52]])\n",
    "linear_grad2(downstream_grad=downgrad, x=x, weight_matrix=wm)\n",
    "# the results match the results in the 5-th lecture, 28-th slide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1dfa308",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
