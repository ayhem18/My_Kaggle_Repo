{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# load the data from pytorch\n",
    "import torch\n",
    "import os\n",
    "from torchvision import datasets, transforms as tr\n",
    "data_folder = os.path.join(os.getcwd(), 'data')\n",
    "os.makedirs(data_folder, exist_ok=True)\n",
    "\n",
    "# apply toTensor transform to each of the images\n",
    "basic_transform  = tr.Compose([tr.ToTensor()])\n",
    "# use Pytorch to load the dataset\n",
    "cifar_train = datasets.CIFAR10(root=os.path.join(data_folder, 'train'), train=True, download=True, transform=basic_transform)\n",
    "cifar_test = datasets.CIFAR10(root=os.path.join(data_folder, 'test'), train=False, download=False, transform=basic_transform)\n",
    "\n",
    "# build a loader for each split\n",
    "from torch.utils.data import DataLoader\n",
    "# convert the dataset to a Dataloader for easier manipulation\n",
    "train_loader = DataLoader(cifar_train, batch_size=1000, shuffle=False) \n",
    "test_loader = DataLoader(cifar_test, batch_size=1000, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save each of test and train splits in a single tensor\n",
    "# the after stacking the different batches together: the output will be of the shape: 50, 1000, 3, 32, 32\n",
    "train_tensor = torch.stack([data for data, _ in train_loader])\n",
    "test_tensor = torch.stack([data for data, _ in test_loader])\n",
    "\n",
    "\n",
    "# converting the different batches into a single large one while flattening each image in the batch \n",
    "# in other words, we will have a large tensor of shape (number of samples, number of pixels )\n",
    "train_np = train_tensor.permute((0, 1, 3, 4, 2)).reshape(shape=(len(cifar_train), -1)).numpy()\n",
    "test_np = test_tensor.permute((0, 1, 3, 4, 2)).reshape(shape=(len(cifar_test), -1)).numpy()\n",
    "    \n",
    "train_labels = torch.stack([labels for _, labels in train_loader]).reshape((-1,)).numpy()\n",
    "test_labels = torch.stack([labels for _, labels in test_loader]).reshape((-1,)).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 3072)\n",
      "(10000, 3072)\n",
      "(50000,)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "# make sure the shapes are as expected\n",
    "print(train_np.shape)\n",
    "print(test_np.shape)\n",
    "print(train_labels.shape)\n",
    "print(test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "class KnnClassifier:\n",
    "    def __init__(self, k: int = 1) -> None:\n",
    "        self.k = k\n",
    "        self._train_data = None\n",
    "        self._train_labels = None           \n",
    "\n",
    "    def fit(self, train_data: np.ndarray, labels: np.ndarray):\n",
    "        # make sure to convert the data to numpy arrays\n",
    "        # saving the data as np.unint8 speeds the algorithm\n",
    "        self._train_data = train_data.astype(np.uint8)\n",
    "        self._train_labels = labels.astype(np.uint8)\n",
    "\n",
    "    def _predict(self, image: np.ndarray) -> int:        \n",
    "        # first apply np.abs(image - self._train_data): numpy will broadcast the image from (1, num_pixels) to (50000, num_pixes) \n",
    "        # np.abs(image-self._Train_data) will be the difference of difference in absolute value between the image and each item in the training data\n",
    "        # np sum() sums these difference and sum_difference is a numpy array of the shape (50000, 1)\n",
    "        sum_difference = np.sum(np.abs(image - self._train_data), axis=-1)\n",
    "        # find the indices of the 'k' nearest training samples: which correspond to the ones with the least difference        \n",
    "        k_indices = np.argsort(sum_difference, axis=-1)[:self.k].squeeze()\n",
    "        # use the extracted indices to extract the labels\n",
    "        k_labels = self._train_labels[k_indices] \n",
    "        # use majority voting: Counter counts the frequency of elements and most_common()functions returns a sorted list: [(item, frequency)]\n",
    "        # indexing by [0][0] returns the most common label in the neighborhood\n",
    "        return Counter(k_labels).most_common()[0][0]\n",
    "    \n",
    "    def predict(self, test_data:np.ndarray):\n",
    "        # apply list comprehension of efficency\n",
    "        return [self._predict(t) for t in tqdm(test_data)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [39:45<00:00,  4.19it/s]\n"
     ]
    }
   ],
   "source": [
    "# initialize a Knn classifier with k=5\n",
    "classifier = KnnClassifier(5)\n",
    "classifier.fit(train_np, train_labels)\n",
    "predictions = classifier.predict(test_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.16\n"
     ]
    }
   ],
   "source": [
    "accuray = np.mean((predictions == test_labels))\n",
    "print(round(accuray, 3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
