{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2 as cv\n",
    "import os\n",
    "from typing import Union\n",
    "from pathlib import Path\n",
    "import sys \n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ayhem18/DEV/My_Kaggle_Repo/Hack_visual_system'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "HOME = os.getcwd()\n",
    "DATA_FOLDER = os.path.join(HOME, 'data')\n",
    "current = HOME\n",
    "while 'src' not in os.listdir(current):\n",
    "    current = Path(current).parent\n",
    "\n",
    "PROJECT_DIR = str(current) \n",
    "sys.path.append(PROJECT_DIR)\n",
    "sys.path.append(os.path.join(str(current), 'src'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.path.realpath(os.path.dirname(__file__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from typing import Union, List, Dict\n",
    "from _collections_abc import Sequence\n",
    "from collections import defaultdict\n",
    "from ultralytics.engine.results import Results\n",
    "\n",
    "from face.custom_tracker import CustomByteTracker\n",
    "from face.utilities import FR_SingletonInitializer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def images_to_numpy(images: Sequence[Union[str, Path]]) -> List[np.ndarray]:\n",
    "    if isinstance(images, (str, Path)):\n",
    "        images = [images]\n",
    "    return [np.asarray(cv.imread(img)) for img in images]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "expected ':' (3730525695.py, line 88)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[18], line 88\u001b[0;36m\u001b[0m\n\u001b[0;31m    def _detect(person_dict: Dict[int, ])\u001b[0m\n\u001b[0m                                         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m expected ':'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class YoloFaceRecognizer():\n",
    "    _tracker_URL = 'https://raw.githubusercontent.com/voyager-108/ml/main/apps/nn/yolov8/trackers/tracker.yaml'\n",
    "    @classmethod\n",
    "    def _file_dir(cls):\n",
    "        try:\n",
    "            return os.path.realpath(os.path.dirname(__file__))\n",
    "        except NameError:\n",
    "            return os.getcwd()\n",
    "    \n",
    "    @classmethod\n",
    "    def _tracker_file(cls):\n",
    "        # download the tracker file as needed\n",
    "        req = requests.get(cls._tracker_URL, allow_redirects=True)\n",
    "        with open(os.path.join(cls._file_dir(), 'tracker.yaml'), 'wb') as f:\n",
    "            f.write(req.content)\n",
    "\n",
    "    def __init__(self, \n",
    "                top_person_detection: int = 5,   \n",
    "                top_face_prediction: int = 2,\n",
    "                yolo_path: Union[str, Path] = 'yolov8n.pt',\n",
    "                ) -> None:\n",
    "        self.person_detected = top_person_detection\n",
    "        self.face_detected = top_face_prediction, \n",
    "\n",
    "        # download the 'tracker.yaml' file if needed\n",
    "        if not os.path.isfile(os.path.join(self._file_dir(), 'tracker.yaml')):\n",
    "            # download the file in this case\n",
    "            self._tracker_file()\n",
    "\n",
    "        # the yolo components\n",
    "        self.yolo = YOLO(yolo_path)\n",
    "        self.tracker = CustomByteTracker(os.path.join(self._file_dir(), 'tracker.yaml'))\n",
    "\n",
    "        singleton = FR_SingletonInitializer()\n",
    "        self.face_detector = singleton.get_face_detector() \n",
    "        self.face_encoder = singleton.get_encoder()\n",
    "        self.device = singleton.get_device()\n",
    "    \n",
    "    def _track(self, frames: Sequence[Union[Path, str, np.ndarray, torch.Tensor]]) -> List[Results]:\n",
    "        \"\"\"This function tracks the different people detected across the given \n",
    "\n",
    "        Args:\n",
    "            frames (Sequence[Union[Path, str, np.ndarray, torch.Tensor]]): a sequence of frames. The assumption is\n",
    "            that frames are consecutive in time.\n",
    "\n",
    "        Returns:\n",
    "            List[Results]: a list of Yolo Results objects\n",
    "        \"\"\"\n",
    "        # this is the first step is the face detection pipeline: Detecting people in the image + tracking them\n",
    "        tracking_results = self.yolo.track(source=frames, \n",
    "                                           persist=True, \n",
    "                                           classes=0, # only detect people in the image\n",
    "                                           device=self.device,\n",
    "                                           show=False)   \n",
    "        \n",
    "        # remove the extra ids by calling the custom tracker's method\n",
    "        self.tracker.track(tracking_results)\n",
    "        return tracking_results\n",
    "    \n",
    "    def _identify(self, tracking_results: List[Results]):\n",
    "        # create a dictionary to save the information about each id detected in the results\n",
    "        # the dictionary will be of the form {id: [(frame_index, boxes, probs)]}\n",
    "        ids_dict = defaultdict(lambda: [])\n",
    "        \n",
    "        # iterate through the results to extract the ids\n",
    "        for frame_index, results in enumerate(tracking_results):\n",
    "            \n",
    "            boxes = results.boxes\n",
    "            \n",
    "            if boxes is None:\n",
    "                continue\n",
    "\n",
    "            probs = results.probs \n",
    "            ids = boxes.id.int().cpu().tolist()\n",
    "\n",
    "            assert len(ids) == len(boxes) == len(probs), \"Check the lengths of ids, probabilities and boxes\"\n",
    "\n",
    "            for i, bb, p in zip(ids, boxes.xywh.cpu().tolist(), probs):\n",
    "                ids_dict[i].append((frame_index, bb, p))\n",
    "            \n",
    "        # the final step is to filter the results\n",
    "        # keep only the top self.person_detected boxes for each id\n",
    "        for person_id, info in enumerate(ids_dict):\n",
    "            ids_dict[person_id] = sorted(info, key=lambda x: x[-1], reverse=True)[:self.person_detected]\n",
    "        \n",
    "        return ids_dict\n",
    "\n",
    "    def _detect(self, frames, \n",
    "                person_dict: Dict[int, List], \n",
    "                crop_person: bool = True):\n",
    "        \n",
    "        if crop_person: \n",
    "            frames = images_to_numpy(frames)        \n",
    "        \n",
    "        self.face_detector.keep_all = False\n",
    "        \n",
    "\n",
    "\n",
    "    def detect_faces():\n",
    "        # the first step is to pass the sequence to the\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ayhem18/DEV/My_Kaggle_Repo/Hack_visual_system'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yrf = YoloFaceRecognizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.pytorch_modular.directories_and_files import process_save_path\n",
    "\n",
    "def video_to_images(video_path: Union[str, Path], \n",
    "                    output_dir: Union[Path, str], \n",
    "                    frame_stride: int = 32) -> None:\n",
    "    if os.path.isdir(output_dir):\n",
    "        # remove the directory if it already exists\n",
    "        shutil.rmtree(output_dir)\n",
    "\n",
    "    output_dir = process_save_path(output_dir, file_ok=False, dir_ok=True)    \n",
    "    count = 0\n",
    "    # create the cv2 capture video object\n",
    "    video_iterator = cv.VideoCapture(video_path)\n",
    "    \n",
    "    # count the total number of frames in the video\n",
    "    s = True\n",
    "    total_count = 0\n",
    "    \n",
    "    while s:\n",
    "        s, _ = video_iterator.read()\n",
    "        total_count += 1\n",
    "    video_iterator = cv.VideoCapture(video_path)\n",
    "    \n",
    "    while True:\n",
    "        frame_exists, image = video_iterator.read()\n",
    "\n",
    "        if not frame_exists: \n",
    "            # the video is over\n",
    "            break\n",
    "\n",
    "        count += 1\n",
    "        if count % frame_stride == frame_stride - 1:\n",
    "            frame_num_str = f\"{(len(str(total_count)) - len(str(count))) * '0'}{count}\"\n",
    "            cv.imwrite(os.path.join(output_dir, f'frame_{frame_num_str}.jpg'), image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = os.path.join(DATA_FOLDER, 'v1.mp4')\n",
    "frames_save_path = os.path.join(DATA_FOLDER, 'frames')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_to_images(video_path=video_path,\n",
    "                output_dir=frames_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ayhem18/DEV/My_Kaggle_Repo/FaceRecognition/data/frames/frame_0031.jpg\n",
      "/home/ayhem18/DEV/My_Kaggle_Repo/FaceRecognition/data/frames/frame_0063.jpg\n",
      "/home/ayhem18/DEV/My_Kaggle_Repo/FaceRecognition/data/frames/frame_0095.jpg\n",
      "/home/ayhem18/DEV/My_Kaggle_Repo/FaceRecognition/data/frames/frame_0127.jpg\n",
      "/home/ayhem18/DEV/My_Kaggle_Repo/FaceRecognition/data/frames/frame_0159.jpg\n",
      "/home/ayhem18/DEV/My_Kaggle_Repo/FaceRecognition/data/frames/frame_0191.jpg\n",
      "/home/ayhem18/DEV/My_Kaggle_Repo/FaceRecognition/data/frames/frame_0223.jpg\n",
      "/home/ayhem18/DEV/My_Kaggle_Repo/FaceRecognition/data/frames/frame_0255.jpg\n",
      "/home/ayhem18/DEV/My_Kaggle_Repo/FaceRecognition/data/frames/frame_0287.jpg\n",
      "/home/ayhem18/DEV/My_Kaggle_Repo/FaceRecognition/data/frames/frame_0319.jpg\n",
      "/home/ayhem18/DEV/My_Kaggle_Repo/FaceRecognition/data/frames/frame_0351.jpg\n",
      "/home/ayhem18/DEV/My_Kaggle_Repo/FaceRecognition/data/frames/frame_0383.jpg\n",
      "/home/ayhem18/DEV/My_Kaggle_Repo/FaceRecognition/data/frames/frame_0415.jpg\n",
      "/home/ayhem18/DEV/My_Kaggle_Repo/FaceRecognition/data/frames/frame_0447.jpg\n",
      "/home/ayhem18/DEV/My_Kaggle_Repo/FaceRecognition/data/frames/frame_0479.jpg\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "frames = sorted(os.listdir(frames_save_path))\n",
    "frames = [os.path.join(frames_save_path, f) for f in frames]\n",
    "test_frames = frames[:15] \n",
    "\n",
    "for image in test_frames:\n",
    "    print(image)\n",
    "    image = os.path.join(frames_save_path, image)\n",
    "    img = cv.imread(image)\n",
    "    cv.imshow('image',img)\n",
    "    cv.waitKey(0)\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from _collections_abc import Sequence\n",
    "import torch\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "DEF_YOLO = YOLO('yolov8n.pt')\n",
    "def track(frames: Sequence[Union[Path, str], np.array], \n",
    "          yolo_model: YOLO = None, \n",
    "          device: str = DEVICE):\n",
    "    \n",
    "    if yolo_model is None:\n",
    "        yolo_model = DEF_YOLO\n",
    "\n",
    "    tracking_results = yolo_model.track(source=frames, \n",
    "                                        persist=True, \n",
    "                                        classes=0, # only detect people in the image\n",
    "                                        device=device,\n",
    "                                        show=False)   \n",
    "    return tracking_results       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 person, 1: 384x640 1 person, 2: 384x640 1 person, 3: 384x640 1 person, 4: 384x640 1 person, 5: 384x640 2 persons, 6: 384x640 2 persons, 7: 384x640 2 persons, 8: 384x640 1 person, 9: 384x640 1 person, 10: 384x640 1 person, 11: 384x640 1 person, 12: 384x640 1 person, 13: 384x640 1 person, 14: 384x640 1 person, 22.1ms\n",
      "Speed: 0.9ms preprocess, 1.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    }
   ],
   "source": [
    "res = track(test_frames)\n",
    "# pass the result through the tracker\n",
    "from bytetrack import embed_tracking_into_results\n",
    "new_res = embed_tracking_into_results(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for r in res:\n",
    "    # Get the boxes and track IDs\n",
    "    track_ids = r.boxes.id\n",
    "\n",
    "    if track_ids is None:\n",
    "        continue\n",
    "\n",
    "    track_ids = track_ids.int().cpu().tolist()\n",
    "    # Visualize the results on the frame\n",
    "    annotated_frame = r.plot()\n",
    "    cv.imshow('frame', annotated_frame)\n",
    "    cv.waitKey(0)\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1], [1], [1], [1], [5], [2, 1], [1], [1, 2], [1], [1], [1], [1], [1], [17], [2]]\n"
     ]
    }
   ],
   "source": [
    "boxes = [r.boxes for r in res]\n",
    "print([b.id.int().cpu().tolist() for b in boxes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
