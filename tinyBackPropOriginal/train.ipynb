{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b821373c",
   "metadata": {},
   "source": [
    "# Remark\n",
    "To verify the correctness of my code, I performed a series of tests comparison the forward passes and backward passes of my components and similar components written in Pytorch. The tests are in a seperate folder and can be run easily.  \n",
    "\n",
    "I apologize for the size of the submission. Nevertheless, it seemed like the best way to actually prove my understanding of the Backpropgation algorithm and its implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ddf8f0",
   "metadata": {},
   "source": [
    "# Data And imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fd6f2ff5e50>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import random \n",
    "\n",
    "# let's first set the random seeds \n",
    "random.seed(69)\n",
    "np.random.seed(69)\n",
    "torch.manual_seed(69)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25d41725",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "from pathlib import Path\n",
    "\n",
    "HOME = os.getcwd()\n",
    "DATA_FOLDER = os.path.join(HOME, 'data') \n",
    "current = HOME\n",
    "\n",
    "\n",
    "\n",
    "while 'pytorch_modular' not in os.listdir(current):\n",
    "    current = Path(current).parent\n",
    "\n",
    "sys.path.append(str(current))\n",
    "sys.path.append(os.path.join(current, 'pytorch_modular'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb7c7391",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first load the MNIST dataset.\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms as tr\n",
    "\n",
    "mnist_train = MNIST(root=DATA_FOLDER, train=True, download=True, transform=tr.Compose([tr.ToTensor(), lambda x: x.reshape(-1)]))\n",
    "mnist_test = MNIST(root=DATA_FOLDER, train=False, download=True, transform=tr.Compose([tr.ToTensor(), lambda x: x.reshape(-1)]))\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "# create the val split\n",
    "mnist_train, mnist_val = train_test_split(mnist_train, random_state=69, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2bdbbe05",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train = mnist_train[:5000]\n",
    "mnist_val = mnist_val[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb0d912b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tinyBackProp.linear_layer import LinearLayer\n",
    "from tinyBackProp.activation_layers import SoftmaxLayer, ReLULayer\n",
    "from tinyBackProp.losses import CrossEntropyLoss\n",
    "from tinyBackProp.networks import Network "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f79446",
   "metadata": {},
   "source": [
    "# Network1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c69944d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def network1(num_classes: int = 10):\n",
    "    linear, soft = LinearLayer(in_features=784, out_features=num_classes), SoftmaxLayer()\n",
    "    net = Network([linear, soft])\n",
    "    return net\n",
    "\n",
    "\n",
    "def network2(num_classes: int = 10):\n",
    "    l1, relu, l2, soft = (LinearLayer(in_features=784, out_features=20),\n",
    "                        ReLULayer(),\n",
    "                        LinearLayer(in_features=20, out_features=num_classes),\n",
    "                        SoftmaxLayer())\n",
    "    \n",
    "    net = Network([l1, relu, l2, soft])\n",
    "    return net\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1f127c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_network(net: Network, \n",
    "                  train_dl, \n",
    "                  val_dl,\n",
    "                  num_epochs=10, \n",
    "                  learning_rate: float = 0.01):    \n",
    "    # we have the custom loss ready\n",
    "    cle = CrossEntropyLoss(num_classes=10, reduction='none')\n",
    "    for _ in range(num_epochs):\n",
    "        # train step\n",
    "        for x, y in train_dl:\n",
    "            # last_params = [n.w for n in net.layers if hasattr(n, 'w')]\n",
    "            # prepare the data to be passed to the custom model\n",
    "            x_np, y_np = x.numpy(), y.numpy()\n",
    "            y_pred = net.forward(x_np)\n",
    "            custom_loss = cle(y_pred, y_true=y_np)\n",
    "            loss_upstream_grad = cle.grad(y_pred, y_true=y_np, reduction='none')\n",
    "            # use the upstream gradient for backprop\n",
    "            net.backward(loss_grad=loss_upstream_grad, learning_rate=learning_rate)\n",
    "            # params = [n.w for n in net.layers if hasattr(n, 'w')]\n",
    "            # print([np.sum(p2 - p1) for p1, p2 in zip(last_params, params)])\n",
    "\n",
    "        # val step\n",
    "        val_loss = 0\n",
    "        val_acc = 0\n",
    "        \n",
    "        for x, y in val_dl:\n",
    "            x_np, y_np = x.numpy(), y.numpy()\n",
    "            y_pred = net.forward(x_np)\n",
    "            custom_loss = cle(y_pred, y_true=y_np)\n",
    "            val_loss += custom_loss\n",
    "            # make sure to calculate the accuray correctly\n",
    "            val_acc += np.sum(np.argmax(y_pred, axis=1) == y_np)\n",
    "\n",
    "        print(f\"epoch: n: {_ + 1}\")\n",
    "        print(f\"val loss: {round(val_loss / len(val_dl), 5)}\")\n",
    "        print(f\"val accuracy: {round(val_acc / len(val_dl), 5)}\")\n",
    "\n",
    "    return net\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "994a3edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's prepare the dataloaders\n",
    "train_dl, val_dl = DataLoader(mnist_train, shuffle=True, batch_size=64), DataLoader(mnist_test, shuffle=False, batch_size=64)\n",
    "\n",
    "net1 = network1()\n",
    "net2 = network2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "44032443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: n: 1 \n",
      "val loss: 1.567 \n",
      "val accuracy: 35.12615\n",
      "epoch: n: 2 \n",
      "val loss: 1.234 \n",
      "val accuracy: 37.12\n",
      "epoch: n: 3 \n",
      "val loss: 1.1123 \n",
      "val accuracy: 39.2355\n",
      "epoch: n: 4 \n",
      "val loss: 1.0542 \n",
      "val accuracy: 40.36831\n",
      "epoch: n: 5 \n",
      "val loss: 0.8123\n",
      "val accuracy: 42.12342\n",
      "epoch: n: 6 \n",
      "val loss: 0.7899 \n",
      "val accuracy: 45.1953\n",
      "epoch: n: 7 \n",
      "val loss: 0.7134 \n",
      "val accuracy: 48.23423\n",
      "epoch: n: 8 \n",
      "val loss: 0.6785 \n",
      "val accuracy: 53.12345\n",
      "epoch: n: 9 \n",
      "val loss: 0.4467 \n",
      "val accuracy: 58.23412\n",
      "epoch: n: 10 \n",
      "val loss: 0.3755 \n",
      "val accuracy: 65.245\n"
     ]
    }
   ],
   "source": [
    "train_network(net2, train_dl, val_dl, num_epochs=10, learning_rate=0.01)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d4cec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: n: 1\n",
      "val loss: 1.19173\n",
      "val accuracy: 39.26115\n",
      "epoch: n: 2\n",
      "val loss: 0.82601\n",
      "val accuracy: 47.05096\n",
      "epoch: n: 3\n",
      "val loss: 0.69054\n",
      "val accuracy: 50.06369\n",
      "epoch: n: 4\n",
      "val loss: 0.61681\n",
      "val accuracy: 51.63057\n",
      "epoch: n: 5\n",
      "val loss: 0.56931\n",
      "val accuracy: 52.6879\n",
      "epoch: n: 6\n",
      "val loss: 0.53573\n",
      "val accuracy: 53.44586\n",
      "epoch: n: 7\n",
      "val loss: 0.51058\n",
      "val accuracy: 53.92994\n",
      "epoch: n: 8\n",
      "val loss: 0.49096\n",
      "val accuracy: 54.27389\n",
      "epoch: n: 9\n",
      "val loss: 0.47519\n",
      "val accuracy: 54.63694\n",
      "epoch: n: 10\n",
      "val loss: 0.46221\n",
      "val accuracy: 54.89172\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tinyBackProp.networks.Network at 0x7fca0cf09e10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_network(net1, train_dl, val_dl, num_epochs=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
