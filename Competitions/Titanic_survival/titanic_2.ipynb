{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "This is my 2nd attempt to solve the data science challenge: predicting Titanic survival on [Kaggle](https://www.kaggle.com/competitions/titanic).\n",
    "I got inspired by this great [notebook](https://www.kaggle.com/code/kenjee/titanic-project-example/notebook) as it served as my starting point in this overwhelming journey."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary work: imports and loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the local location of the datasets\n",
    "train_file = os.path.join(\"datasets\", \"train.csv\")\n",
    "test_file = os.path.join(\"datasets\", \"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_org = pd.read_csv(train_file)\n",
    "df = df_train_org.copy()# .set_index(\"PassengerId\")\n",
    "print(df.head())\n",
    "df_test_org = pd.read_csv(test_file)\n",
    "df_test = df_test_org.copy()# .set_index(\"PassengerId\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['train'] = pd.Series([1 for _ in range(len(df) + 1)])\n",
    "df_test['train'] = pd.Series([0.0 for _ in range(len(df_test) + 1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.set_index('PassengerId')\n",
    "df_test = df_test.set_index(\"PassengerId\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.concat([df,df_test], ignore_index=True)\n",
    "print(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = all_data.rename(mapper=str.lower, axis=1) \n",
    "all_data = all_data.rename(columns={\"survived\":\"y\", \"embarked\":\"from\", \"pclass\":\"c\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring The data\n",
    "In this section, I will explore the data and consider the different contributions of each feature in the final prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Engineering the fam_name\n",
    "The name feature as it is might not be of major relevance. However, that does not mean it does not bring any valuable insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what matters here in the name is the first part representing the family name\n",
    "all_data['fam_name'] = all_data['name'].apply(lambda x: re.sub('.;:?', \",\", x).strip().lower().split(\",\")[0]) \n",
    "# print(all_data['fam_name'])\n",
    "\n",
    "fam_sizes = all_data.pivot_table(columns='y', index='fam_name', values='ticket', aggfunc='count')\n",
    "# print(fam_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fam_sizes = all_data['fam_name'].value_counts() \n",
    "\n",
    "all_data['fam_size'] = all_data['fam_name'].apply(lambda x: fam_sizes[x])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring and visualizing the numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = all_data[all_data['train'] == 1].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_num_names = [\"y\", \"c\", \"age\", \"sibsp\", \"parch\", \"fare\"]\n",
    "X_cat_names = [\"name\", \"sex\", \"ticket\", \"cabin\", \"from\"]\n",
    "df_num = df.loc[:, X_num_names]\n",
    "df_cat = df.loc[:, X_cat_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_num.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df_num.columns:\n",
    "    plt.hist(df_num[col])\n",
    "    plt.title(col)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_survive = df.drop('train', axis=1)[df['y']==1]\n",
    "print(df_survive.describe())\n",
    "print(\"#\" * 50)\n",
    "df_dead = df.drop('train', axis=1)[df['y'] == 0]\n",
    "print(df_dead.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_survive.reset_index().plot(kind='scatter', x='index', y='fare', title='fare variation for survivors')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dead.reset_index().plot(kind='scatter', x='index', y='fare', title='fare variation for the dead')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Fare feature\n",
    "The fare feature is quite an interesting feature as it interacts with a most of the features in the dataset. A careful investigation might reveal a number of useful and significant interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ultra_rich = df.drop(['ticket', 'name', 'train', 'cabin', 'from', 'fam_name'], axis=1)[df['fare'] >= 200]\n",
    "print(df_ultra_rich[df_ultra_rich['y'] != 0])\n",
    "print(\"**\" * 100)\n",
    "print(df_ultra_rich[df_ultra_rich['y'] == 0]) \n",
    "\n",
    "## so if the passengers are ultra rich, then a female has quite high probability of surviving.\n",
    "## the males with the most expensive fares are the ones to survive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's consider the passengers paying fare in the range [100, 200]\n",
    "df_rich = df.drop(['ticket', 'name', 'train', 'cabin', 'from', 'fam_name'], axis=1)[(df['fare'] >= 100) & (df['fare'] <200)]\n",
    "print(df_rich[df_rich['y'] == 1])\n",
    "print(\"*\" * 100)\n",
    "print(df_rich[df_rich['y'] == 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_3 = df.drop(['ticket', 'name', 'train', 'cabin', 'from', 'fam_name'], axis=1)[(df['fare'] >= 50) & (df['fare'] <100)]\n",
    "df_3.reset_index().plot(kind='scatter', x='index', y='fare', title='third category fare')\n",
    "df_3_sur = df_3[df_3['y'] == 1]\n",
    "df_3_dead = df_3[df_3['y'] == 0]\n",
    "df_3_sur.reset_index().plot(kind='scatter', x='index', y='fare', title = 'third category fare survivors')\n",
    "df_3_dead.reset_index().plot(kind='scatter', x='index', y='fare', title = 'third category fare dead')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_3_sur.describe())\n",
    "print(df_3_dead.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(df_3_sur['sex'].value_counts())\n",
    "# print(df_3_dead['sex'].value_counts())\n",
    "print(df_3[df_3['sex'] == 'male'][['y', 'fam_size', 'fare', 'age']].sort_values(['fam_size', 'y'], ascending=[False, False]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_4 = df.drop(['ticket', 'name', 'train', 'cabin', 'from', 'fam_name'], axis=1)[(df['fare'] < 50) & (df['fare'] >= 40)]\n",
    "print(df_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_5 = df.drop(['ticket', 'name', 'train', 'cabin', 'from', 'fam_name'], axis=1)[(df['fare'] < 40)]\n",
    "df_5.reset_index().plot(kind='scatter', x='index', y='fare')\n",
    "print(df_5[df_5['y']==1]['c'].value_counts())\n",
    "print(df_5[df_5['y']==0]['c'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fun_list = [pd.Series.count, np.sum, np.mean, np.std, np.max, np.min]\n",
    "\n",
    "df_survived_class = df_survive.groupby(\"c\").agg({\"fare\":fun_list})\n",
    "df_dead_class = df_dead.groupby(\"c\").agg({\"fare\":fun_list})\n",
    "print(df_survived_class)\n",
    "print(df_dead_class)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_num.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring and visualizing the Categorical features\n",
    "As I considered the numerical features in the previous subsection, it is time to investigate the effect of the categorical ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Categorical features in general"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd.pivot_table(df, index='y', values=['c','age', 'fare'], aggfunc=[pd.Series.count, np.mean]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparing survivors' values with respect to categorical variables.\n",
    "print(pd.pivot_table(df, index='y', columns=['c'], values='ticket', aggfunc='count'))\n",
    "print(pd.pivot_table(df, index='y', columns=['sex'], aggfunc='count', values='ticket'))\n",
    "print(pd.pivot_table(df, index='y', columns='from', aggfunc='count', values='ticket'))\n",
    "# at first glance it might seem that passengers embarking from \"C\" are more likely to survive. Yet, it might be useful to consider\n",
    "# the social class of people coming from the different stations.\n",
    "\n",
    "print(pd.pivot_table(df, index='c', columns=['from'], values='name', aggfunc='count'))\n",
    "# the last observation did not provide evidence to completely rool out the possibility of positive correlation between the embarkment \n",
    "# point and survival, more investigation is needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 0: male, 1: female\n",
    "genre_mapper = {\"male\":0, \"female\":1}\n",
    "all_data[\"sex\"] = pd.Series([genre_mapper[x] for x in all_data['sex']])\n",
    "\n",
    "from_mapper = {\"C\":1, \"Q\":2, \"S\":3}\n",
    "all_data[\"from\"] = pd.Series([from_mapper[x] if x in from_mapper else x for x in all_data[\"from\"]])\n",
    "\n",
    "all_data['from'] = all_data['from'].astype(float)\n",
    "print(all_data.loc[:, [\"y\", \"from\"]].corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = all_data[all_data['train'] == 1].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# understand the relation between classes and the embarkment station\n",
    "from_class_ana = df.groupby(\"from\").agg({\"c\":['count', 'mean'], \"fare\":'mean'})\n",
    "print(from_class_ana)\n",
    "# so we can say the embarkment station has little to no correlation with the social class\n",
    "\n",
    "print(pd.pivot_table(df, index='from',columns='c',values='ticket', aggfunc='count'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cabin Feature\n",
    "This subsection was created to better understand the cabin column. It is currently commented as no useful insight was extracted and the column is dropped later in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # understanding the cabin\n",
    "\n",
    "# print(df[\"cabin\"].isna().sum())\n",
    "# print(df[\"cabin\"].copy().dropna().count())\n",
    "# so only 204 passengers bought cabins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # we assume that the cabins are indeed separated by spaces\n",
    "# df['num_cabins'] = df.cabin.apply(lambda x: 0 if pd.isna(x) else len(x.split(\" \")))\n",
    "# print(df.num_cabins.value_counts())\n",
    "# # let's check the relation between number of cabins and social class\n",
    "# print(pd.pivot_table(df, index='num_cabins', columns='c', values='ticket', aggfunc='count'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fare Column ...Again\n",
    "In this subsection, I delve a bit deeper in the fare column to perceive the relationship of this feature with the rest of the categorial features: mainly the class feature. Most of the next cells are commented as the attempt to extract meaningful features did not produce desirable results.\n",
    "\n",
    "\n",
    "***More investigation on this course of action might be needed.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# understanding the fare column:\n",
    "print (df.loc[:, [\"fare\"]].describe())\n",
    "fare_s = df['fare']\n",
    "fare_df = df.loc[:, [\"fare\"]]\n",
    "# consider the nan values\n",
    "print(fare_s.isna().sum())\n",
    "# there is no nan values: such a delight !!!\n",
    "\n",
    "print(fare_s[lambda x : x == 0].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can see that the survival probability is higher for upper social classes. However, It might be worth noting\n",
    "# that the ticket's price: fare is as well influencial. In other words, a 3rd class passenger who paid more than a 1st passenger\n",
    "# might be more likely to survive, let's consider this subtle detail: it might lead to a helpful feature\n",
    "\n",
    "# df_no_fare = df[df['fare'] == 0]\n",
    "# df_fare = df[df['fare'] != 0]\n",
    "# fare_np = df_fare[\"fare\"].values\n",
    "\n",
    "# quantiles_values = [0, 0.25, 0.5, 0.75, 1]\n",
    "# fare_quantiles = {}\n",
    "# for i in range(1, 4):\n",
    "#     fare_quantiles[\"fare_q_c\" + str(i)] = np.quantile(df_fare[df_fare[\"c\"] == i][\"fare\"].values, quantiles_values)\n",
    "\n",
    "# for key, value in fare_quantiles.items():\n",
    "#     print(str(key) + \": \" + str(value)) \n",
    "\n",
    "# def quartile_number(value, quantiles):\n",
    "#     # value assumed to be at least larger or equal then the lowest value\n",
    "#     assert (value >= quantiles_values[0])\n",
    "#     for i in range(len(quantiles) - 1):\n",
    "#         if value >= quantiles[i] and value < quantiles[i + 1]:\n",
    "#             return i + 1\n",
    "#     return len(quantiles) - 1 \n",
    "\n",
    "# def classify_passenger(row):\n",
    "#     return quartile_number(row['fare'], fare_quantiles[\"fare_q_c\" + str(int(row['c']))])    \n",
    "\n",
    "\n",
    "# df_fare['quartile_class'] = df_fare.loc[:, ['fare', 'c']].apply(lambda row: classify_passenger(row), axis=1)\n",
    "# print(df_fare.loc[:, ['fare', 'c', 'quartile_class']].head(15))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# fare_class_quartile_effect = pd.pivot_table(df_fare, index='y', columns=['c','quartile_class'], values='name', aggfunc='count')\n",
    "# print(fare_class_quartile_effect)\n",
    "# print(\"#\" * 50)\n",
    "# fare_class_quartile_effect.loc[2] = fare_class_quartile_effect.loc[1] / fare_class_quartile_effect.loc[0]\n",
    "# print(fare_class_quartile_effect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the results are promissing and thus it is worthy experimenting with quartile class feature\n",
    "# let's add the values to the all_data dataframe\n",
    "# first impute the zero values with the column mean\n",
    "\n",
    "fare_by_class_mean = all_data[all_data['fare'] > 0].groupby(\"c\").agg({\"fare\": np.mean}).squeeze()\n",
    "print(fare_by_class_mean)\n",
    "\n",
    "\n",
    "def fill_up_fare(row):\n",
    "    if row['fare'] == 0 or np.isnan(row['fare']):\n",
    "        row['fare'] = fare_by_class_mean[row['c']]\n",
    "    return row\n",
    "\n",
    "# we can see that there is positive correlation between the quartile_class feature and survival\n",
    "# it is necessary to impute the row: the mean seems like a reasonable choice\n",
    "\n",
    "all_data = all_data.apply(lambda row: fill_up_fare(row) , axis=1)\n",
    "\n",
    "print(all_data['fare'].isna().sum()) # there is no Nan values anymore\n",
    "print(all_data[all_data['fare'] <= 0]['fare'].sum()) # there is no 0 fare values anymore\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now the quartile class should be added to the add_data DF\n",
    "# quantiles_values = [0, 0.25, 0.5, 0.75, 1]\n",
    "# fare_quantiles = {}\n",
    "# for i in range(1, 4):\n",
    "#     fare_quantiles[\"fare_q_c\" + str(i)] = np.quantile(all_data[all_data[\"c\"] == i][\"fare\"].values, quantiles_values)\n",
    "\n",
    "# all_data['quartile_class'] = all_data.loc[:, ['fare', 'c']].apply(lambda row: classify_passenger(row), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(df.loc[:, [\"y\", \"num_cabins\"]].corr())\n",
    "\n",
    "# There are 3 features that might reflect in a passenger's social image:\n",
    "# * class * fare * num_cabins\n",
    "# let's consider each individually\n",
    "\n",
    "# print((pd.pivot_table(df, index='y', columns=[ 'num_cabins', 'c'], values='ticket', aggfunc=['count'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## the number of cabins is quite a helpful feature as well so it seems reasonable to add it to the all_data df\n",
    "# all_data['num_cabins'] = all_data.cabin.apply(lambda x: 0 if pd.isna(x) else len(x.split(\" \")))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Engineering the Title feature\n",
    "Once again with the name feature, The first term used to address is the title and can reflect a number of things such as social status, age..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's first consider the title associated with a passenger's name\n",
    "\n",
    "df['title'] = df['name'].apply(lambda x: re.sub('[:?;.]', \",\", x).split(\",\")[1].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.title.value_counts().index.sort_values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's consider the non-uniform titles and their relevance to the survival\n",
    "non_comm_title = df[df['title'].isin(['Capt', 'Col', 'Don', 'Dr', 'Jonkheer', 'Lady', 'Major','Rev', 'Sir', 'the Countess'])]\n",
    "print(pd.pivot_table(non_comm_title, index='y', columns='title', values='ticket', aggfunc='count'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comm_title = df[df['title'].isin(['Master','Miss', 'Mlle', 'Mme', 'Mr', 'Mrs', 'Ms'])]\n",
    "# print(pd.pivot_table(comm_title, index='y', columns='title', values='ticket', aggfunc='count'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputing Missing Values\n",
    "It is known that machine learning models do not accept Nan values. Thus, imputing the missing values is a crucial preliminary step in the ML pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputing Age\n",
    "The age is highly correlated with the final prediction. Yet, it has a large number of missing values. A more careful imputing is needed. After investigating and little research, I decided to impute the age as follows:\n",
    "1. for classes 1 and 2: fill the missing values with the mean age\n",
    "2. for class 3: ages are grouped by title (as indicated later the two values are related), the missing values are filled with the mean of the group with the corresponding title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's impute the age\n",
    "print(df['age'].isna().sum())\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_3_age = df[(~np.isnan(df['age'])) &(df['c'] == 3) ]['age'].values\n",
    "print(np.nanstd(class_3_age))\n",
    "print(np.amax(class_3_age), np.amin(class_3_age))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = all_data[all_data['train'] != 1]\n",
    "print(df_test['age'].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_class = all_data.groupby('c').agg({\"age\":[np.mean, np.median]}).iloc[:, 0]\n",
    "print(age_class)\n",
    "# count the number of missing age values in each class \n",
    "print(all_data[np.isnan(all_data['age'])].groupby('c').agg({\"name\":'count'})) \n",
    "# as we can see there are only few values missing values for first and second class, thus it might not be harmful\n",
    "# to fill the missing values with the class's mean age\n",
    "# however, since the 3rd class has a large number of missing values, a more careful imputation might be needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as we can see there 4 main titles associated with passenger from the 3rd class.\n",
    "According to the following [link](https://prowritingaid.com/art/968/mr%2c-mrs%2c-ms-and-miss%3a-everything-you-need-to-know-about-titles.aspx), the title is generally associated with \n",
    "an age category or a matrial status (that indeed correlates with age...). It might be a good idea to associate these two features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's first consider the title associated with a passenger's name\n",
    "\n",
    "all_data['title'] = all_data['name'].apply(lambda x: re.sub('[:?;.]', \",\", x).split(\",\")[1].strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's fill the third class missing ages with the mean of the associated title\n",
    "print(all_data[all_data['c'] == 3]['title'].value_counts())\n",
    "\n",
    "title_age_class_3 = all_data[all_data['c'] == 3].groupby('title').agg({\"age\":np.nanmean}).iloc[:, 0]\n",
    "print(title_age_class_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_up_age_class_3(row):\n",
    "    value1 = np.round(title_age_class_3[row['title']])\n",
    "    value2 = np.round(age_class[row['c']]) # used only for one passenger with title Ms.\n",
    "    if np.isnan(row['age']):\n",
    "        row['age'] =  value2 if np.isnan(value1) else value1\n",
    "    return row\n",
    "\n",
    "def fill_up_age_class_1_2(row):\n",
    "    if np.isnan(row['age']):\n",
    "        row['age'] = np.round(age_class[row['c']])\n",
    "    return row\n",
    "\n",
    "def fill_up_age(row):\n",
    "    if row['c'] == 3:\n",
    "        return fill_up_age_class_3(row)\n",
    "    return fill_up_age_class_1_2(row)\n",
    "\n",
    "all_data = all_data.apply(fill_up_age, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the data for modeling\n",
    "After engineering new features, imputing missing values and exploring different interactions, it is time to prepare the final dataframe for modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it is time to drop the unncessary columns\n",
    "print(all_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = all_data.drop(['name', 'ticket', 'cabin', 'title','sibsp', 'parch', 'fam_name'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = all_data[all_data['train'] == 1].copy()\n",
    "df_test = all_data[all_data['train'] != 1].copy()\n",
    "# df_train.dropna(subset=['from','age'], inplace=True) # drop nan values \n",
    "\n",
    "print(df_train.columns, df_test.columns)\n",
    "df_train.dropna(subset=['from', 'age'], inplace=True)\n",
    "X_train = df_train.drop(['train', 'y'],axis=1).values\n",
    "\n",
    "y_train = df_train['y'].values\n",
    "X_test = df_test.drop(['train', 'y'],axis=1).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling data (normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time to scale the data\n",
    "# we can use the sklearn class for this\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try all the models baseline models I currently know\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "n_splits = 6\n",
    "random_state = 5\n",
    "kf = KFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "\n",
    "solver = 'liblinear'\n",
    "lr = LogisticRegression(solver=solver)\n",
    "cv_scores = cross_val_score(lr, X_train, y_train, cv=kf)\n",
    "print(np.mean(cv_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models\n",
    "It is time for the final phase in the Machine Learning pipeline: creating models for predicting the passengers' survival"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First set of Models\n",
    "Let's start with basic models such as:\n",
    "* Logistic regression\n",
    "* KNN classifier\n",
    "* SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svm = SVC() # the non-linear SVM\n",
    "cv_scores = cross_val_score(svm, X_train, y_train, cv=kf)\n",
    "print(cv_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier()\n",
    "cv_scores = cross_val_score(svm, X_train, y_train, cv=kf)\n",
    "print(cv_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dt = DecisionTreeClassifier(random_state=2) \n",
    "cv_scores = cross_val_score(dt, X_train, y_train)\n",
    "print(cv_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "def model_best_version(model, model_name):\n",
    "    print(model_name)\n",
    "    print(\"parameters \\n\" + str(model.best_params_))\n",
    "    print(\"best f1score \\n\" + str(model.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HyperTune the first set of models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# tune the parameters\n",
    "lamda = np.array([10 ** x for x in np.linspace(-5, 0.1)])\n",
    "\n",
    "lr_params = {\"max_iter\": [2000], \"penalty\":['l2'], 'C': 1 / lamda, 'solver':['liblinear']}\n",
    "\n",
    "lr_best = GridSearchCV(lr, param_grid=lr_params, cv=6, n_jobs=-1, scoring='f1')\n",
    "\n",
    "lr_best.fit(X_train, y_train)\n",
    "\n",
    "model_best_version(lr_best, \"LogisticRegression\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_params = {'n_neighbors' : range(5, 20),\n",
    "              'weights' : ['uniform', 'distance'],\n",
    "              'algorithm' : ['auto', 'ball_tree','kd_tree'],\n",
    "              'p' : [1,2]}\n",
    "knn_best = GridSearchCV(knn, param_grid = knn_params, cv = 6,  n_jobs = -1)\n",
    "knn_best.fit(X_train,y_train)\n",
    "model_best_version(knn_best, \"KNN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svm_params = [{'kernel': ['rbf'], 'gamma': [.1,.5,1,2,5,10], 'C': 1 / lamda}]\n",
    "# {'kernel': ['poly'], 'degree' : [2,3,4,5], 'C': 1 / lamda}]\n",
    "\n",
    "svc_best = GridSearchCV(SVC(), param_grid=svm_params, cv=6, n_jobs=-1)\n",
    "svc_best.fit(X_train, y_train)\n",
    "model_best_version(svc_best, \"NON-linear SVM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_params = {\"max_depth\": [2, 3,4,5,6], \"min_samples_leaf\": [0.02, 0.04, 0.05, 0.1, 0.12, 0.15], \"max_features\":[\"log2\", \"sqrt\", None]}\n",
    "dt_best = GridSearchCV(dt, param_grid=dt_params, cv=6, n_jobs=-1)\n",
    "dt_best.fit(X_train, y_train)\n",
    "model_best_version(dt_best, \"DecisionTreeClassifier\")\n",
    "\n",
    "print(cross_val_score(dt_best, X_train, y_train, cv=kf).mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_pred = dt_best.predict(X_train)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(y_train, X_train_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_diff = pd.DataFrame(np.abs(X_train_pred - y_train), columns=['diff'])\n",
    "\n",
    "y_diff = y_diff[y_diff['diff'] == 1]\n",
    "\n",
    "train_mis = df_train.drop(['train'], axis=1).iloc[y_diff.index,:]\n",
    "\n",
    "print(train_mis[train_mis['y'] == 1].head(20))\n",
    "# print(train_mis[(train_mis['y'] == 0.0) & (train_mis['quartile_class'].isin([3,4]))])\n",
    "# print(\"#\" * 100)\n",
    "\n",
    "# print(train_mis[(train_mis['y'] == 1.0) & (train_mis['quartile_class']).isin([1,2])])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gather submissions\n",
    "lr_pred = lr_best.predict(X_test)\n",
    "knn_pred = knn_best.predict(X_test)\n",
    "dt_pred = dt_best.predict(X_test)\n",
    "\n",
    "\n",
    "# sub_1 = pd.DataFrame({\"PassengerId\": df_test_org['PassengerId'], \"Survived\": lr_pred}).astype(int)\n",
    "sub_knn = pd.DataFrame({\"PassengerId\": df_test_org['PassengerId'],\"Survived\": knn_pred}).astype(int)\n",
    "sub_dt = pd.DataFrame({\"PassengerId\": df_test_org['PassengerId'],\"Survived\": dt_pred}).astype(int)\n",
    "# sub_1.to_csv(\"sub1.csv\", index=False)\n",
    "sub_knn.to_csv(\"sub_knn.csv\", index=False)\n",
    "sub_dt.to_csv(\"sub_dt.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More Sophisticated Models: RandomForest and XGboost Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's consider more complicated models such as Random Forests model.\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# rf_basic = RandomForestClassifier()\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# let's try to tune a RandomForest model\n",
    "rf = RandomForestClassifier()\n",
    "print(rf.get_params())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rf_params = {'max_depth':[4, 5, 6, 7, 8], 'max_features': ['sqrt', 'log2'], 'min_samples_leaf':[0.02, 0.03, 0.04, 0.05, 0.08, 0.1]\n",
    ", 'max_samples':[0.8, 0.85, 0.9, 1]}\n",
    "\n",
    "rf_basic = RandomForestClassifier(random_state=68)\n",
    "num_folds=6\n",
    "rf_searcher = GridSearchCV(estimator=rf_basic, \n",
    "                            param_grid=rf_params, \n",
    "                            n_jobs=-1, \n",
    "                            scoring='accuracy', \n",
    "                            cv=num_folds)\n",
    "rf_searcher.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_t = rf_searcher.best_estimator_\n",
    "print(rf_t.score(X_train, y_train))\n",
    "splits = 5\n",
    "random_state = 3\n",
    "kf = KFold(n_splits=splits, random_state=random_state, shuffle=True)\n",
    "print(cross_val_score(rf_t, X_train, y_train, cv=kf).mean())\n",
    "# it seems that the estimator does not overfit the data (not too badly either way)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_accs = []\n",
    "estimators = range(20, 251, 10)\n",
    "for i in estimators:\n",
    "    \n",
    "    rf_t.n_estimators = i\n",
    "    rf_t.fit(X_train, y_train)\n",
    "    cv_accs.append(cross_val_score(rf_t, X_train, y_train, cv=kf).mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the cross validation accuracies\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(estimators, cv_accs, '--r', label='CV')\n",
    "leg = ax.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can see that the overall performance is more promising in the range [20, 50]\n",
    "# let's try to focus on this range\n",
    "min_train_performance = 0.82\n",
    "\n",
    "rf_t_params = {\"n_estimators\": range(20, 51, 2)}\n",
    "rf_searcher = GridSearchCV(estimator=rf_t, \n",
    "                            param_grid=rf_params, \n",
    "                            n_jobs=-1, \n",
    "                            scoring='accuracy', \n",
    "                            cv=5)\n",
    "\n",
    "rf_searcher.fit(X_train, y_train)\n",
    "\n",
    "rf_t = rf_searcher.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_pred = rf_t.predict(X_test)\n",
    "\n",
    "# sub_1 = pd.DataFrame({\"PassengerId\": df_test_org['PassengerId'], \"Survived\": lr_pred}).astype(int)\n",
    "sub_rf = pd.DataFrame({\"PassengerId\": df_test_org['PassengerId'],\"Survived\": rf_pred}).astype(int)\n",
    "\n",
    "sub_rf.to_csv('sub_rf.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "xgc = xgb.XGBClassifier(seed=123, objective=\"reg:logistic\")\n",
    "\n",
    "params = {'n_estimators': range(50, 100),\n",
    "    'max_depth': [3, 4, 5], \n",
    "    \"eta\": [0.001, 0.01, 0.05, 0.1, 0.2],\n",
    "    \"subsample\": [0.8, 0.9, 1], \n",
    "    \"lambda\": [0.01, 0.05, 0.1, 0,5, 0.8], \n",
    "}\n",
    "\n",
    "xgb_t = GridSearchCV(xgc, param_grid=params, scoring='accuracy', cv=4, verbose=1)\n",
    "\n",
    "xgb_t.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_pred = xgb_t.predict(X_test)\n",
    "\n",
    "sub_xgb = pd.DataFrame({\"PassengerId\": df_test_org['PassengerId'],\"Survived\": xgb_pred}).astype(int)\n",
    "\n",
    "sub_xgb.to_csv('sub_xgb.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model_to_file(model_name, model, file_name):\n",
    "  with open(file_name, \"a\") as f:\n",
    "    f.write(model_name + \":\\n\")\n",
    "    f.write(str(model.get_params()) + \"\\n\")\n",
    "    f.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model_to_file(\"Random_Forest\", rf_t, \"best_models.txt\")\n",
    "save_model_to_file(\"XGBoost\", xgb_t, \"best_models.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_params = {'gamma': [.1,0.2, 0.3, .5, 0.8, 0.9]}\n",
    "svm_t = GridSearchCV(SVC(), param_grid=svm_params, cv=6, n_jobs=-1)\n",
    "svm_t.fit(X_train, y_train)\n",
    "model_best_version(svc_best, \"NON-linear SVM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model_to_file(\"SVM\", svm_t, \"best_models.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Learning\n",
    "One promising approach is to gather the 3 best performing models: RandomForest, SVM and XGBoost in one model and use an ensemble learning technique such as votingClassifie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_splits = 6\n",
    "random_state = 5\n",
    "kf = KFold(n_splits=n_splits, shuffle=True, random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "vote_class = VotingClassifier(estimators=[(\"svm\", svm_t), ('rf', rf_t), ('xgb', xgb_t)], voting='hard')\n",
    "vote_class.fit(X_train, y_train)\n",
    "res = cross_val_score(vote_class, X_train, y_train, cv=kf)\n",
    "\n",
    "save_model_to_file(\"voting classifier\", vote_class, \"best_models.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(res)\n",
    "print(res.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vote_pred = vote_class.predict(X_test)\n",
    "\n",
    "sub_vote = pd.DataFrame({\"PassengerId\": df_test_org['PassengerId'],\"Survived\": vote_pred}).astype(int)\n",
    "\n",
    "sub_vote.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('ds_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "006414dea9a04848ce797b510a25f3f28ac8668e3d3244e777242cca6bed477f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
