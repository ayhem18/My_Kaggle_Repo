{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# House Pricing Predictions\n",
    "This is my attempt on the data science challenge on [Kaggle](https://www.kaggle.com/competitions/house-prices-advanced-regression-techniques/overview), predicting the house prices given a large number of features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I got inspired by great notebooks such as:\n",
    "* the feature-engineering lesson by [***RYAN HOLBROOK***](https://www.kaggle.com/code/ryanholbrook/feature-engineering-for-house-prices)\n",
    "* the amazing data exploration notebook by [***Pedro Marcelino***](https://www.kaggle.com/code/pmarcelino/comprehensive-data-exploration-with-python)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary work: imports and loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variables to store the location of the training and test datasets\n",
    "train_file = \"train.csv\"\n",
    "test_file = \"test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_org = pd.read_csv(train_file)\n",
    "df_test_org = pd.read_csv(test_file)\n",
    "# df and df_test are copies of the original datasets with target value referred to as y\n",
    "Y = \"y\"\n",
    "df = df_train_org.rename(columns={'SalePrice': Y})\n",
    "df_test = df_test_org.rename(columns={'SalePrice':Y})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration \n",
    "In this section we explore the basic aspects of the provided dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "# a function to drop an element to both train and test dataframes\n",
    "def drop_cols(cols):\n",
    "    global df, df_test\n",
    "    \n",
    "    if isinstance(cols, str):\n",
    "        df = df.drop(cols, axis=1)\n",
    "        df_test = df_test.drop(cols, axis=1)\n",
    "    else:\n",
    "        for col in cols:\n",
    "            df = df.drop(col, axis=1)\n",
    "            df_test = df_test.drop(col, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's first understand the nature of our data\n",
    "print(df.shape, df_test.shape) \n",
    "# each sample is described by 81 features. This number if relatively high.\n",
    "\n",
    "print((df['Id'].values == range(1 ,len(df) + 1)).all()) \n",
    "# as we can see the Id column is merely for ennumeriation purposes. It can be either dropped or set as an index.\n",
    "drop_cols(\"Id\")\n",
    "print(df.shape, df_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical and Numerical features\n",
    "Let's consider the different types of features. First, we divide them into numerical and non-numerical. The non-numerical are definitely categorical (or can be made as such). As for numerical, columns with int values, can be considered categorical if the number of unique values is limited."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# consider non-numerical values\n",
    "object_type = \"object\"\n",
    "cat_type = 'category'\n",
    "non_num_cols = df.select_dtypes([object_type, cat_type]).columns\n",
    "print(non_num_cols)\n",
    "num_cols = df.select_dtypes(np.number).columns\n",
    "print(num_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's consider the subset of numerical columns with few discrete values\n",
    "num_discrete = df.select_dtypes('int64').columns\n",
    "num_dis_count = [len(df[num_d].unique()) for num_d in num_discrete]  \n",
    "print(dict(zip(num_discrete, num_dis_count)))\n",
    "\n",
    "# we can see that a \"MSSubClass\" is a categorical feature\n",
    "# a number of features are not categorical by say, but can be treated as such: Bath related features, Fireplaces, GarageCars, and most importantly\n",
    "# OverallQual and OveralCond\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "Certain columns might contain corrupted data and thus require cleaning. I will start with categorical columns. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning categorical columns \n",
    "The main procedure is as follows:\n",
    "* replace the values that do not belong to the data description set of values by the one described there, mainly typos in string typed values\n",
    "* in the worst case drop rows that have values significantly different from the pre-determined categories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col, uni_values in zip(non_num_cols, [df[col].unique() for col in non_num_cols]):\n",
    "    print(col)\n",
    "    print(uni_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# investigating the output of the previous cell as well as inspecting the content of the documentation\n",
    "# suggests a couple of corrupted values in certain columns such as Exterior2nd\n",
    "\n",
    "# let's define a function to replace these values both in the training and test dataframes\n",
    "def replace_values(col_names:list , wrong_correct:list):\n",
    "    assert (isinstance(col_names, str) and isinstance(wrong_correct, dict)) or all([isinstance(l, dict) for l in wrong_correct]) and isinstance(col_names, list) \n",
    "    \n",
    "    global df, df_test\n",
    "    \n",
    "    if isinstance(col_names, str):\n",
    "        for k, v in wrong_correct.items():\n",
    "            df = df.replace(k, v)\n",
    "            df_test = df_test.replace(k, v)\n",
    "        return \n",
    "    \n",
    "    else:\n",
    "        for col, dic in zip(col_names, wrong_correct):\n",
    "            for k, v in dic.items():\n",
    "                df = df.replace(k, v)\n",
    "                df_test = df_test.replace(k, v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the value C (all) is corrupted in MSZoning\n",
    "mszoning = \"MSZoning\"\n",
    "correct_mszoning = {\"C (all)\": \"C\"}\n",
    "ext2 = \"Exterior2nd\"\n",
    "correct_ext2 = {\"Brk Cmn\": \"BrkComm\", \"CmentBd\": \"CemntBd\"}\n",
    "\n",
    "ms_ext = [mszoning, ext2]\n",
    "correct = [correct_mszoning, correct_ext2]\n",
    "\n",
    "replace_values(ms_ext, correct)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning numerical columns\n",
    "This task is slightly trickier as it might require domain expertise. The main procedure is as follows:\n",
    "1. replace (or drop) values that contradict general common sense, for instance negative areas, months cannot be more than $12$\n",
    "2. consider the relationship between certain rows. values in a certain columns cannot be smaller / larger than the corresponding values in other columns. This step require more careful study of the nature of the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's first display the numerical columns\n",
    "print(num_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's consider areas\n",
    "areas = [col for col in num_cols if (\"area\" in col.strip().lower())]\n",
    "# inverstiagating the data description, the term SF generally refers to surface (area)\n",
    "areas.extend([col for col in num_cols if \"SF\" in col.strip()]) \n",
    "print(areas)\n",
    "# verify all values are positive\n",
    "areas_with_neg = [any(df[area] < 0) for area in areas]\n",
    "areas_with_neg = [area for area, a in zip(areas, areas_with_neg) if a]\n",
    "print(areas_with_neg)\n",
    "# as we can see all areas-values are positive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data description indicats that \n",
    "* OverallQuall and OverallCond should belong to the interval [1, 10]\n",
    "* YearRemodAdd is year of remodel, thus it should be larger or equal to YearBuilt\n",
    "* The relationship between the the year where the garage was built and the other year features should be investigated.\n",
    "* MSSubClass represents a label encoding of the different types of houses: values should belong to predetermined set of values specified in the data description\n",
    "* MoSold is the month where the house was sold. it should belong to [1, 12]\n",
    "* it might be worthwhile investigating any abnormalities in the relationship between \"BsmtFinSF1\", \"BsmtUnfSF\", \"BsmtFinSF2\" and \"TotalBsmtSF\" \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first of all let's define a method to replace the names of features in both training and test datasets\n",
    "\n",
    "def new_col_names(old_new_names:dict):\n",
    "    global df, df_test\n",
    "    try:    \n",
    "        df = df.rename(columns=old_new_names)\n",
    "        df_test = df_test.rename(columns=old_new_names)\n",
    "    except:\n",
    "        df_no_col = [col for col in old_new_names.keys if col not in df.columns]\n",
    "        df_test_no_col = [col for col in old_new_names.keys if col not in df_test.columns]\n",
    "        print(\"{cols} are not in the {dataf}\".format(df_no_col, \"training dataset\"))\n",
    "        print(\"{cols} are not in the {dataf}\".format(df_test_no_col, \"test dataset\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_new_cols = {\"OverallQual\": \"qua\", \n",
    "\"OverallCond\": \"cond\", \"YearBuilt\": \"Yb\", \"YearRemodAdd\": \"Yr\", \"MSSubClass\": \"mss\" , \n",
    "\"BsmtFinSF1\": \"bSF1\", \"BsmtFinSF2\": \"bSF2\", \"BsmtUnfSF\": \"bubf\", \"GarageYrBlt\":\"GYb\"}\n",
    "new_col_names(old_new_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let' define a function that applies a function to either \n",
    "# the whole dataframe or certain columns on the dataframe\n",
    "\n",
    "def apply_functions(funcs, col_names=None):\n",
    "    # either have one function passed that should be applied to the whole dataframe\n",
    "    # or have an equal number of columns and functions where each funtion will be applied to the corresponding column\n",
    "    all_data = callable(funcs) and col_names is None\n",
    "    col_funcs = True\n",
    "    #  if the funcs argument is indeed a function, then the code below will raise an error \n",
    "    try:\n",
    "        col_funcs = (all([callable(f) for f in funcs]) and len(funcs) == len(col_names))\n",
    "    except:\n",
    "        col_funcs = False\n",
    "    \n",
    "    assert all_data or col_funcs\n",
    "   \n",
    "    global df, df_test\n",
    "    if col_names is None: # if the function is to be applied to the whole dataframe\n",
    "        df = df.apply(funcs, axis=1)\n",
    "        df_test = df_test.apply(funcs, axis=1)\n",
    "    else:\n",
    "        for col, f in zip(col_names, funcs):\n",
    "            df = df.apply(f)\n",
    "            df_test = df_test.apply(f)\n",
    "\n",
    "# verify the remodeling and building years features\n",
    "yb = \"Yb\"\n",
    "yr = \"Yr\"\n",
    "            \n",
    "def set_built_remodel_years(row):\n",
    "    if row[yr] < row[yb]:\n",
    "        row[yr] = row[yb]\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify the integrity of overall quality and condition features\n",
    "qua = \"qua\"\n",
    "cond = \"cond\"\n",
    "\n",
    "assert all(df[qua].isin(range(1, 11)))\n",
    "assert all(df[cond].isin(range(1, 11)))\n",
    "\n",
    "assert df[df[yb] > df[yr]].empty # the resulting dataframe is empty: no problems with either of these features\n",
    "apply_functions(set_built_remodel_years)\n",
    "assert df_test[df_test[yb] > df_test[yr]].empty\n",
    "\n",
    "# verify the MSubclass features\n",
    "mss = \"mss\"\n",
    "mss_values = [20, 30, 40, 45, 50, 60, 70, 75, 80, 85, 90, 120, 150, 160, 180, 190]\n",
    "\n",
    "assert df[~df[mss].isin(mss_values)].empty # all values from \"MSsubclass\" feature are under check\n",
    "assert df_test[~df_test[mss].isin(mss_values)].empty\n",
    "# verify the month feature\n",
    "\n",
    "assert df[~df[\"MoSold\"].isin(range(1, 13))].empty  # all values of the month feature are correct\n",
    "assert df_test[~df_test[\"MoSold\"].isin(range(1, 13))].empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's consider the year where the garage was built\n",
    "gyb = \"GYb\"\n",
    "print(df[df[gyb] < df[yb]][[gyb, yb, yr]])\n",
    "\n",
    "print(df_test[df_test[gyb] < df_test[yb]][[gyb, yb, yr]]) \n",
    "# with few exceptions the year where the garage if it is before the year where the house was built is generally few years earilier\n",
    "# which suggests that the garage was meant to be built with the house, yet the house took slightly longer to complete.\n",
    "\n",
    "# if GYb is less than Yb then we will set to Yb.\n",
    "def set_garage_year(row):\n",
    "    if row[gyb] < row[yb]:\n",
    "        row[gyb] = row[yb]\n",
    "    return row\n",
    "# set the changes in both train and test data\n",
    "print(callable(set_garage_year))\n",
    "apply_functions(set_garage_year)\n",
    "\n",
    "assert df[df[gyb] < df[yb]].empty\n",
    "assert df_test[df_test[gyb] < df_test[yb]].empty \n",
    "\n",
    "# the changes were applied to both data sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputing missing values\n",
    "The second step is imputing the missing values and making sure no Nan values are passed to our machine learning models. The main procedure is as follows:\n",
    "1. drop all columns with a nan ratio exceeding a certain treshhold\n",
    "2. imput the missing values. The strategy depends mainly on the feature in question:\n",
    "    * if the feature is highly related to other features, then a highly accurate and natural value can be deduced\n",
    "    * some statistical value could be used to impute the missing values\n",
    "    * investigating related features could help come up with an aggregated value when the data is grouped by a number of features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1st strategy: drop columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's first discover which columns have Nan values\n",
    "nan_val_train = df.isna().sum()\n",
    "cols_nan = nan_val_train[nan_val_train > 0]\n",
    "print(cols_nan)\n",
    "NAN_THRESHOLD = 0.8 # all columns with more than 0.8 nan values will be dropped\n",
    "cols_nan = cols_nan / len(df)\n",
    "# print(cols_nan)\n",
    "cols_nan_drop = cols_nan[cols_nan > NAN_THRESHOLD].index.values\n",
    "print(cols_nan_drop) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_val_test = df_test.isna().sum()\n",
    "nan_val_test = nan_val_test[nan_val_test > 0]\n",
    "print(nan_val_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see: Alley, PoolQC, fence and MiscFeature have an extremely high ratio of nan values. \n",
    "before dropping these columns, it is necessary to drop features tightly related to those.  \n",
    "Investigating the data description reveals that:  \n",
    "* PoolQC is related to PoolArea\n",
    "* MiscFeature is related to MiscVal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's better understand those relations\n",
    "pq = \"PoolQC\"\n",
    "pa = \"PoolArea\"\n",
    "print(df[df[pq].isna()][pa].value_counts()) \n",
    "# Nan values are associated with 0 pool area, which means that there is no pool in the first place\n",
    "# an extremely high ratio of houses do not have a pool.\n",
    "# let's consider the prices of houses with pool\n",
    "print(df[~df[pq].isna()][Y]) #  there is only few values with relatively high variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "misf = \"MiscFeature\"\n",
    "misv = \"MiscVal\"\n",
    "print(df[df[misf].isna()][misv].value_counts()) # we can see that houses with Nan on MiscFeature \n",
    "# have 0 on MiscVal which means that they do not any additional features to mention\n",
    "# both of these features should be dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's drop the features in questions\n",
    "cols_nan_drop_final = cols_nan_drop.tolist() + [misv, pa]\n",
    "drop_cols(cols_nan_drop_final)\n",
    "\n",
    "assert all(df.drop(Y, axis=1).columns.values == df_test.columns.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2nd strategy: deduce values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's consider the columns left with nan values\n",
    "nan_val_train = df.isna().sum()\n",
    "cols_nan = nan_val_train[nan_val_train > 0]\n",
    "print(cols_nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values with Nan can be divided into categories:\n",
    "* Masonry veneer \n",
    "* Basement\n",
    "* Garage\n",
    "* FirePlace\n",
    "* LotFrontage (only one column) and Electrical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_nan(col_names:list, fill_values:list):\n",
    "    one = isinstance(col_names, str) and isinstance(fill_values, str)\n",
    "    try:\n",
    "        many = len(col_names) == len(fill_values)\n",
    "    except:\n",
    "        many = False\n",
    "    assert one or many \n",
    "    global df, df_test\n",
    "    if many: \n",
    "        for col, v in zip(col_names, fill_values):\n",
    "            df[col] = df[col].fillna(v)\n",
    "            df_test[col] = df_test[col].fillna(v)\n",
    "    else:\n",
    "        df[col_names] = df[col_names].fillna(fill_values)\n",
    "        df_test[col_names] = df_test[col_names].fillna(fill_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's consider Masonry Veneer columns\n",
    "msvt = \"MasVnrType\"\n",
    "msva = \"MasVnrArea\"\n",
    "\n",
    "print(df[(df[msvt].isna()) | (df[msva].isna())][[msva, msvt]]) \n",
    "# we can see that type and are either both nan or both non-nan\n",
    "# a reasonable assumption is that there is no Masonry Veneer\n",
    "set_nan([msvt, msva], ['None', 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's consider the garage columns\n",
    "gt = \"GarageType\"\n",
    "gf = \"GarageFinish\"\n",
    "gc = \"GarageCars\"\n",
    "ga = \"GarageArea\"\n",
    "gcond = \"GarageCond\"\n",
    "gqua = \"GarageQual\"\n",
    "\n",
    "g = [gt, gyb, gf, gc, ga, gcond, gqua]\n",
    "print(df[df[gt].isna() | df[gyb].isna() | df[gf].isna() | df[gcond].isna() | df[gqua].isna()][ga].value_counts())\n",
    "# if any of the garage features in a certain row is set to Nan, then its area is 0\n",
    "# which means there is no garage\n",
    "\n",
    "# let's first consider the Garage built year feature for such rows\n",
    "print(df[df[gt].isna() | df[gf].isna() | df[gcond].isna() | df[gqua].isna()][gyb].value_counts())\n",
    "\n",
    "set_nan(g, [\"NA\", 0, \"NA\", 0, 0, \"NA\", \"NA\"])\n",
    "\n",
    "garage_old_new = {gt: \"gt\", gf:\"gf\", gc:\"gc\", ga:\"ga\", gcond:\"gcond\", gqua:\"gqua\"}\n",
    "\n",
    "new_col_names(garage_old_new)\n",
    "\n",
    "gt = \"gt\"\n",
    "gf = \"gf\"\n",
    "gc = \"gc\"\n",
    "ga = \"ga\"\n",
    "gcond = \"gcond\"\n",
    "gqua = \"gqua\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's consider the basement nan-values\n",
    "bqua = \"BsmtQual\"\n",
    "bcond = \"BsmtCond\"\n",
    "bexp = \"BsmtExposure\"\n",
    "bf2 = \"BsmtFinType2\"\n",
    "bf1 = \"BsmtFinType1\"\n",
    "baf1 = \"bSF1\"\n",
    "baf2 = \"bSF2\"\n",
    "\n",
    "# BsmtQual         37\n",
    "# BsmtCond         37\n",
    "# BsmtExposure     38\n",
    "# BsmtFinType1     37\n",
    "# BsmtFinType2     38\n",
    "\n",
    "print(df[df[bqua].isna() & df[bcond].isna() & df[bexp].isna() & df[bf1].isna() ][baf1].value_counts())\n",
    "# we can assume that Nan values for each of these basement features reflect No basement\n",
    "\n",
    "set_nan([bqua, bcond, bexp, bf1, bf2], [\"NA\"] * 5)\n",
    "set_nan([\"bSF1\", \"bSF2\", \"bubf\", \"TotalBsmtSF\", \"BsmtFullBath\", \"BsmtHalfBath\"], [0] * 6)\n",
    "# change the basement's features \n",
    "\n",
    "basement_new_old = {bqua: \"bqua\", bcond: \"bcond\", bexp: \"bexp\", bf1:\"bf1\", bf2: \"bf2\"}\n",
    "new_col_names(basement_new_old)\n",
    "\n",
    "bqua = \"bqua\"\n",
    "bcond = \"bcond\"\n",
    "bexp = \"bexp\"\n",
    "bf2 = \"bf2\"\n",
    "bf1 = \"bf1\"\n",
    "baf1 = \"baf1\"\n",
    "baf2 = \"baf2\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's check the rest real quick\n",
    "# LotFrontage     259\n",
    "# Electrical        1\n",
    "# FireplaceQu     690\n",
    "\n",
    "firequa = \"FireplaceQu\"\n",
    "firep = \"Fireplaces\"\n",
    "print(df[df[firequa].isna()][firep].value_counts())\n",
    "# so nan values refer to NOn existing fire places\n",
    "set_nan(firequa, \"NA\")\n",
    "\n",
    "e = \"Electrical\"\n",
    "print(df[e].value_counts())\n",
    "lf = \"LotFrontage\"\n",
    "# we can assume that the only missing value is SBrkr with a high statistical possibility\n",
    "# the simplest solution for LotFrontage is to nan values to 0\n",
    "set_nan([e, lf], [\"SBrkr\", 0])\n",
    "\n",
    "fire_new_old = {firequa:\"firequa\", firep:\"firep\"}\n",
    "new_col_names(fire_new_old)\n",
    "\n",
    "firequa = \"firequa\"\n",
    "firep = \"firep\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_cols():\n",
    "    global test, df_test\n",
    "    cat_train = df.drop(Y, axis=1).select_dtypes(['category', 'object']).columns.values.tolist()\n",
    "    cat_test = df_test.select_dtypes(['category', 'object']).columns.values.tolist()\n",
    "    \n",
    "    num_train = df.drop(Y, axis=1).select_dtypes(np.number).columns.values.tolist()\n",
    "    num_test = df_test.select_dtypes(np.number).columns.values.tolist()\n",
    "    assert (num_train == num_test) and (cat_train == cat_test)\n",
    "    return num_train, cat_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols, cat_cols = classify_cols()\n",
    "print(num_cols, cat_cols, sep='\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# as all these types with Nan values are categorial a reasonable statistics to choose is mode\n",
    "# let's import an Imputer to take care of missing values for us\n",
    "from sklearn.impute import SimpleImputer\n",
    "imp = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n",
    "df_cols = df.columns.values.tolist()\n",
    "df_test_cols = df_cols.copy()\n",
    "df_test_cols.remove(Y)\n",
    "\n",
    "# due the imputing process the columns types are set to object\n",
    "num_cols, cat_cols = classify_cols() \n",
    "\n",
    "df = imp.fit_transform(df)\n",
    "df = pd.DataFrame(df, columns=df_cols)\n",
    "df_test = imp.fit_transform(df_test)\n",
    "df_test = pd.DataFrame(df_test, columns=df_test_cols)\n",
    "\n",
    "for c in num_cols:\n",
    "    df[c] = df[c].astype('float64')\n",
    "    df_test[c] = df_test[c].astype('float64')\n",
    "\n",
    "df[Y] = df[Y].astype('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert ((num_cols, cat_cols) == classify_cols())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert df_test.isna().sum().sum() == 0 and df.isna().sum().sum() == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode the categorial features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the current state of the dataframes of later modifications\n",
    "df_cat = df.copy()\n",
    "df_t_cat = df_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's define the orders for each of the ordinal columns\n",
    "usual_levels = [\"NA\", \"Po\", \"Fa\", \"TA\", \"Gd\", \"Ex\"]\n",
    "\n",
    "ordered_levels = {\n",
    "    \"ExterQual\": usual_levels,\n",
    "    \"ExterCond\": usual_levels,\n",
    "    \"bqua\": usual_levels,\n",
    "    \"bcond\": usual_levels,\n",
    "    \"HeatingQC\": usual_levels,\n",
    "    \"KitchenQual\": usual_levels,\n",
    "    \"firequa\": usual_levels,\n",
    "    \"gqua\": usual_levels,\n",
    "    \"gcond\": usual_levels,\n",
    "    \"LotShape\": [\"Reg\", \"IR1\", \"IR2\", \"IR3\"],\n",
    "    \"LandSlope\": [\"Sev\", \"Mod\", \"Gtl\"],\n",
    "    \"bexp\": [\"NA\", \"No\", \"Mn\", \"Av\", \"Gd\"],\n",
    "    \"bf1\": [\"NA\", \"Unf\", \"LwQ\", \"Rec\", \"BLQ\", \"ALQ\", \"GLQ\"],\n",
    "    \"bf2\": [\"NA\", \"Unf\", \"LwQ\", \"Rec\", \"BLQ\", \"ALQ\", \"GLQ\"],\n",
    "    \"Functional\": [\"Sal\", \"Sev\", \"Maj1\", \"Maj2\", \"Mod\", \"Min2\", \"Min1\", \"Typ\"],\n",
    "    \"gf\": [\"NA\", \"Unf\", \"RFn\", \"Fin\"],\n",
    "    \"PavedDrive\": [\"N\", \"P\", \"Y\"],\n",
    "    \"Utilities\": [\"NoSeWa\", \"NoSewr\", \"AllPub\"],\n",
    "    \"CentralAir\": [\"N\", \"Y\"],\n",
    "    \"Electrical\": [\"Mix\", \"FuseP\", \"FuseF\", \"FuseA\", \"SBrkr\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's encode ordinal data\n",
    "def cat_to_ord(col:list, categories:list, ordered:bool=True):\n",
    "    global df, df_test\n",
    "    final_cat = categories if ordered else categories[::-1]\n",
    "    df[col] = df[col].apply(dict(zip(final_cat, range(0, len(final_cat)))).get)\n",
    "    df_test[col] = df_test[col].apply(dict(zip(final_cat, range(len(final_cat) ))).get)\n",
    "\n",
    "\n",
    "for k, v in ordered_levels.items():\n",
    "    cat_to_ord(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_c, cat_c = classify_cols()\n",
    "for k in ordered_levels.keys():\n",
    "    assert k not in cat_c "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's encode non ordinal data using the label encoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "non_ord_cat = [cat for cat in classify_cols()[1] if cat not in ordered_levels.keys()]\n",
    "\n",
    "le = LabelEncoder()\n",
    "\n",
    "for col in non_ord_cat:\n",
    "    df[col] = le.fit_transform(df[col].values)    \n",
    "    df_test[col] = le.transform(df_test[col])\n",
    "# changing some column names for ease of manipulation\n",
    "\n",
    "new_col_names({\"TotalBsmtSF\": \"bSF\", \"bubf\": \"b_unfSF\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert df.select_dtypes(['object', 'category']).empty and df_test.select_dtypes(['object', 'category']).empty\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FeatureEngineering\n",
    "In this section we will work on improving the performance by creating a number of synthetic features out of the given ones. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's save the dataframe before introducing any new feature\n",
    "df_base = df.copy()\n",
    "df_test_base = df_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.isna().sum().sum())\n",
    "print(df_test.isna().sum().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline performance\n",
    "Creating a baseline model with the initial (left) features gives me a ground on which I can base my next decisions. I will use the same performance metric as in the competition. The baseline model would be a sophisticated XGBoostRegressor model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definining the cross validation procedure\n",
    "from sklearn.model_selection import KFold\n",
    "n_splits = 5 \n",
    "random_state = 3\n",
    "shuffle = True\n",
    "kf = KFold(n_splits=n_splits, random_state=random_state, shuffle=shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sklearn.metrics.get_scorer_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the main score of the competition is the square of log error squared\n",
    "# let's define a function to calculate a model's performance according to this metric\n",
    "scoring = \"neg_mean_squared_error\"\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from xgboost import XGBRegressor as xgr\n",
    "def model_performance(X, y, model=xgr(seed=0)):\n",
    "    global kf, scoring\n",
    "    log_y = np.log(y)\n",
    "    score = cross_val_score(model, X, log_y, cv=5, scoring=scoring)\n",
    "    return np.sqrt(-score.mean()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.copy()\n",
    "y = X.pop(Y)\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "base_score = model_performance(X, y)\n",
    "print(base_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Informative features: mutual information\n",
    "The mutual information is quite a powerful and general technique to determine the relevance of features with respect to the target variable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import mutual_info_regression\n",
    "\n",
    "def make_mi_scores(X, y):\n",
    "    X = X.copy()\n",
    "    # discrete features are the ones with type int\n",
    "    discrete_features = [pd.api.types.is_integer_dtype(t) for t in X.dtypes]\n",
    "\n",
    "    mi_scores = mutual_info_regression(X, y, discrete_features=discrete_features, random_state=0)\n",
    "    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)\n",
    "    mi_scores = mi_scores.sort_values(ascending=False)\n",
    "    return mi_scores\n",
    "\n",
    "\n",
    "def plot_mi_scores(scores):\n",
    "    scores = scores.sort_values(ascending=True)\n",
    "    width = np.arange(len(scores))\n",
    "    ticks = list(scores.index)\n",
    "    plt.barh(width, scores)\n",
    "    plt.yticks(width, ticks)\n",
    "    plt.title(\"Mutual Information Scores\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mi_scores = make_mi_scores(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the all features that do not scroe higher than a certain predetermined threshold.\n",
    "mi_threshhold = 0.02\n",
    "\n",
    "print(df.columns)\n",
    "print(df_test.columns)\n",
    "irrelevant_feats = mi_scores[mi_scores < mi_threshhold].index.values.tolist()\n",
    "\n",
    "drop_cols(irrelevant_feats)\n",
    "print(df.columns)\n",
    "print(df_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.copy()\n",
    "y = X.pop(Y) \n",
    "\n",
    "new_score = model_performance(X, y)\n",
    "print(new_score) # a very small gain is achieved out of removing the unformative features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mi_scores.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quality feature\n",
    "This subsection I try to understand the Quality feature and its relations to the other features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's create a a function that returns three lists: num, ord, cat features\n",
    "def feature_types():\n",
    "    global df, df_test\n",
    "    # using the dataframe version before the numerical encoding\n",
    "    num = df_cat.select_dtypes(np.number).columns.values.tolist()\n",
    "    ord = [k for k in ordered_levels.keys()]\n",
    "    # filter the columns that are still in the dataframe\n",
    "    num = [col for col in num if col in df.columns]\n",
    "    ord = [col for col in ord if col in df.columns]\n",
    "    # the columns left in df that do not belong to the previous two are categorical\n",
    "    cat = [k for k in df.columns if k not in ord and k not in num]\n",
    "    return num, ord, cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols, ord_cols, cat_cols = feature_types()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_or_ordinal = num_cols  + ord_cols\n",
    "corr_with_qua = df.loc[:, num_or_ordinal].corr()[qua] \n",
    "corr_with_qua = corr_with_qua[(corr_with_qua > 0.4) | (corr_with_qua < -0.4)]\n",
    "print(corr_with_qua.sort_values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_scatters(col_names, x=None):\n",
    "    global df, df_test\n",
    "    if x is None:\n",
    "        x = df.index.values.tolist()\n",
    "    for col in col_names:\n",
    "        plt.scatter(x=x, y=df[col].values)\n",
    "        \n",
    "    plt.title(str(col_names) + \" variation\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_funcs = ['count', np.mean, np.median, np.min, np.max]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's consider a new feature: qua * cond\n",
    "df['state'] = df[qua] * df[cond]\n",
    "# print(df['state'].value_counts().sort_index())\n",
    "print(df[['state', Y, qua, cond]].corr()[Y])\n",
    "# as the condition is not as significant as the quality, it might be worth trying a different tranformation\n",
    "df['state'] = df[qua] *  np.floor(np.sqrt(df[cond]))\n",
    "print(df[['state', Y, qua, cond]].corr()[Y])\n",
    "# apparently this feature makes the best out of both features let's verify its effect with and without the old features\n",
    "X = df.copy()\n",
    "y = X.pop(Y)\n",
    "print(model_performance(X, y))\n",
    "# there is some slight improvement of 0.01 \n",
    "# # the feature is worth keeping\n",
    "df_test['state'] = df_test[qua] *  np.floor(np.sqrt(df_test[cond]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.copy()\n",
    "y = X.pop(Y)\n",
    "print(model_performance(X, y))\n",
    "# there is some slight improvement of 0.01 \n",
    "# # the feature is worth keeping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Garage features\n",
    "Garage is clearly an informative element. Some more investigation is needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "garage_cols = [col for col in df.columns if col.lower().startswith(\"g\")]\n",
    "garage_cols.remove(\"GrLivArea\")\n",
    "print(df[garage_cols].corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's inspect gt, GYB, and gf\n",
    "# for c in [gt, gyb, gf]:\n",
    "#     print(df_cat[c].value_counts())\n",
    "\n",
    "print(df_cat[(df_cat[gyb] != df_cat[yb]) & (df_cat[gyb]!= df_cat[yr])][gt].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd.pivot_table(df, values=gyb, index=gqua, aggfunc=agg_funcs + [pd.Series.mode]))\n",
    "print(pd.pivot_table(df, values=gyb, index=gcond, aggfunc=agg_funcs + [pd.Series.mode]))\n",
    "# we can see that the older the more likely for its quality as well as condition to degrade\n",
    "# let's try to incorporate this idea\n",
    "print(pd.pivot_table(df, values=gqua, index=gf, aggfunc=agg_funcs + [pd.Series.mode]))\n",
    "print(pd.pivot_table(df, values=gcond, index=gf, aggfunc=agg_funcs + [pd.Series.mode]))\n",
    "\n",
    "# the gf seems of little to no significance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd.pivot_table(df, index=gqua, values=qua, aggfunc=agg_funcs))\n",
    "print(df[[gqua, gcond, Y]].corr()[Y])\n",
    "# the new state of garage should be similar to the one created for the whole whouse\n",
    "# let's suggest the new state\n",
    "# 0, 1,2 -> 1/2 : quality is low anyway\n",
    "# 3 -> 1: only the quality matters in this case\n",
    "# in general quality is high, a better condition means a better state\n",
    "\n",
    "gcond2= \"gcond2\"\n",
    "def new_gcond(row):\n",
    "    row[gcond2] = 0\n",
    "    if row[gcond] in [0, 1, 2]:\n",
    "        row[gcond2] = 0.5\n",
    "    elif row[gcond] == 3:\n",
    "        row[gcond2] = 1\n",
    "    else:\n",
    "        row[gcond2] = 1.5\n",
    "    return row\n",
    "apply_functions(new_gcond)\n",
    "\n",
    "df['gstate'] = df[qua] * df[gcond2]\n",
    "df_test['gstate'] = df_test[qua] * df_test[gcond2]\n",
    "\n",
    "print(df[[Y, gcond, gqua, gcond2, 'gstate']].corr()[Y]) \n",
    "# as we can see is highly correlated with the target variable\n",
    "# let's test the new feature\n",
    "X = df.copy()\n",
    "y = X.pop(Y)\n",
    "g_mi_scores =  make_mi_scores(X, y)[[gcond, gqua, gcond2, 'gstate']]\n",
    "print(g_mi_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# at the moment the new feature seems quite promising, let's inspect its effect on the performance\n",
    "print(model_performance(X, y))\n",
    "\n",
    "# let's try removing the different subsets of [gcon, gqua, gcond2]\n",
    "X1 = df.drop([gcond, gqua, gcond2], axis=1).copy()\n",
    "y1 = X1.pop(Y)\n",
    "print(model_performance(X1, y1))\n",
    "\n",
    "## uncomment the lines of code below to \n",
    "## see the performance with the different subsets of removed featurs\n",
    "\n",
    "# X2= df.drop([gqua, gcond2], axis=1).copy()\n",
    "# y2 = X2.pop(Y)\n",
    "# print(model_performance(X2, y2))\n",
    "\n",
    "# X3= df.drop([gcond, gcond2], axis=1).copy()\n",
    "# y3 = X3.pop(Y)\n",
    "# print(model_performance(X3, y3))\n",
    "\n",
    "# X4= df.drop([gcond, gqua], axis=1).copy()\n",
    "# y4 = X4.pop(Y)\n",
    "# print(model_performance(X4, y4))\n",
    "\n",
    "# X5= df.drop([gcond], axis=1).copy()\n",
    "# y5 = X5.pop(Y)\n",
    "# print(model_performance(X5, y5))\n",
    "\n",
    "# X6= df.drop([gcond2], axis=1).copy()\n",
    "# y6 = X6.pop(Y)\n",
    "# print(model_performance(X6, y6))\n",
    "\n",
    "# X7= df.drop([gqua], axis=1).copy()\n",
    "# y7 = X7.pop(Y)\n",
    "# print(model_performance(X7, y7))\n",
    "\n",
    "## even though the performance did not improve, the mi scores as well a\n",
    "## as the correlation scores are solid proofs of the usefulness of this feature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_gyb = df[gyb].min()\n",
    "max_gyb = df[gyb].max()\n",
    "# there are two possible ways, either consider age divided by the number of garages\n",
    "# or consider the square root or log of the age\n",
    "gage = \"gage\"\n",
    "df[gage] = np.log(np.sqrt((max_gyb + 2) - df[gyb]))\n",
    "\n",
    "df_test[gage] = np.log(np.sqrt((max_gyb + 2) - df[gyb]))\n",
    "\n",
    "gage_counts = df[gage].value_counts()\n",
    "plt.bar(gage_counts.index.values, gage_counts.values)\n",
    "plt.show()\n",
    "# the gage feature is promising\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's drop gt, gf for the moment, as they do not seem to be as informative \n",
    "drop_cols([gt, gf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "garage_cols = [col for col in df.columns if col.lower().startswith(\"g\")]\n",
    "garage_cols.remove(\"GrLivArea\")\n",
    "print(garage_cols )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[ga] = df_base[ga]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's consider the different relation between the two garage areas\n",
    "print(df[[ga, gc, Y]].corr())\n",
    "print(pd.pivot_table(df, values=ga, index=gc, aggfunc=agg_funcs))\n",
    "\n",
    "# there is practically no difference between having 3 and 4 car capacity in a garage\n",
    "# let's experiment with setting the 5 4-car capacity garages to 3\n",
    "replace_values([gc, gc], [{4:3}, {5: 3}])\n",
    "print(df[garage_cols].corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's investigate the effect of the area within the garage with the same car capacity\n",
    "g1cap = df[df[gc] == 1]\n",
    "g2cap = df[df[gc] == 2]\n",
    "g3cap = df[df[gc] == 3]\n",
    "gs = [g1cap, g2cap, g3cap]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for g in [g1cap, g2cap, g3cap]:\n",
    "    print(g.loc[:, [Y, ga]].corr()[Y])  # numerically a garage's area is not of much relevance given the capacity is either 1 or 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcs = [g1cap[ga].values, g2cap[ga].values, g3cap[ga].values]\n",
    "plt.boxplot(gcs)\n",
    "plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's check the case of garage capacity\n",
    "ga_outliers = {1:[120, 450], 2: [250, 780], 3:[600, 1100]}\n",
    "# let's try rendering the data to normal distribution\n",
    "for g, out in zip(gs, ga_outliers):\n",
    "    g_no_out = g[(g[ga] <= ga_outliers[out][1]) & (g[ga] >= ga_outliers[out][0])]\n",
    "    g_no_out[ga] = np.log((g_no_out[ga]))\n",
    "    g_no_out[Y] = g_no_out[Y] / (10 ** 4)\n",
    "    plt.bar(g_no_out[ga].values, g_no_out[Y].values)\n",
    "    plt.show()\n",
    "    \n",
    "# add the the outliers for the non-garage homes    \n",
    "ga_outliers[0] = [0.0, 0.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_garage_areas(row):\n",
    "    # ga_outliers[row[gc]][0]: represents the minimum (no -outlier) area associated with the capacity of the garage \n",
    "    row[gc] = int(row[gc])\n",
    "    row[ga] = max(ga_outliers[row[gc]][0], row[ga])\n",
    "    row[ga] = min(ga_outliers[row[gc]][1], row[ga])\n",
    "    row[ga] = np.log(np.sqrt(row[ga])) if row[ga] > 0 else 0\n",
    "    return row   \n",
    "apply_functions(transform_garage_areas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(df[ga].values, df[Y].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the garage area is to be dropped\n",
    "drop_cols([gcond2, gcond, gqua, gyb])\n",
    "garage_cols = [col for col in df.columns if col.lower().startswith(\"g\")]\n",
    "garage_cols.remove(\"GrLivArea\")\n",
    "print(garage_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.copy()\n",
    "y = X.pop(Y)\n",
    "print(model_performance(X, y))\n",
    "classify_cols() # added to make sure both"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basement Features\n",
    "The large number of basement-related features requires a better investigation of the relation between them. Feature engineering could pay off well in this aspect of the problem\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['bSF2'] = df_base['bSF2']\n",
    "df_test['bSF2'] = df_test_base['bSF2']\n",
    "new_col_names({\"TotalBsmtSF\": \"bSF\", \"bubf\": \"b_unfSF\"})\n",
    "bas_cols_init = [col for col in df.columns.values if col.lower().startswith(\"b\")]\n",
    "bas_cols_init.remove(\"BldgType\")\n",
    "bas_cols_init.remove( \"BedroomAbvGr\")\n",
    "print(bas_cols_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying PCA on basement features\n",
    "Due to the large number (8 features for one aspect), complexity and (potential) importance of basement features, it is worth experimenting with some "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's try to apply PCA\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def apply_pca(X, features, n_comp=None, standardize=True):\n",
    "    # Standardize the data passed\n",
    "    if standardize:\n",
    "        scaler = StandardScaler()\n",
    "        # scale the data useler the built-in scaler\n",
    "        X = scaler.fit_transform(X)        \n",
    "    \n",
    "    # Create principal components\n",
    "    if n_comp is None:\n",
    "        pca = PCA()\n",
    "    else:\n",
    "        pca = PCA(n_components=n_comp)\n",
    "        \n",
    "    X_pca = pca.fit_transform(X)\n",
    "    # Convert to dataframe\n",
    "    component_names = [f\"PC{i+1}\" for i in range(X_pca.shape[1])]\n",
    "    X_pca = pd.DataFrame(X_pca, columns=component_names)\n",
    "    # Create loadings\n",
    "    loadings = pd.DataFrame(\n",
    "        pca.components_.T,  # transpose the matrix of loadings\n",
    "        columns=component_names,  # so the columns are the principal components\n",
    "        index=features,  # and the rows are the original features\n",
    "    )\n",
    "    return pca, X_pca, loadings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bas = df.loc[:, bas_cols_init + [Y]]\n",
    "X = df_bas.copy()\n",
    "y = X.pop(Y)\n",
    "\n",
    "bas_pca, bas_X_pca, bas_loadings = apply_pca(X, bas_cols_init)\n",
    "\n",
    "# print(bas_X_pca, bas_loadings, sep='\\n')\n",
    "\n",
    "bas_pca_mi_scores = make_mi_scores(bas_X_pca, y)\n",
    "print(bas_pca_mi_scores)\n",
    "\n",
    "bas_X_pca[Y] = y\n",
    "print(bas_X_pca.corr()[Y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I will take PC3 and PC1 as the final basement features\n",
    "df['bas1'] = bas_X_pca['PC3']\n",
    "df['bas2'] = bas_X_pca['PC1']\n",
    "df['bas3'] = bas_X_pca['PC8']\n",
    "# drop_cols(bas_cols_init)\n",
    "BAS_PCA = ['bas1', 'bas2', 'bas3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_one_distribution(iterable, label, n_unique=30):\n",
    "    if len(np.unique(iterable)) <= n_unique:\n",
    "        print(iterable.value_counts().sort_index())\n",
    "    else:\n",
    "        print(\"boxplot for \" + label)\n",
    "        plt.boxplot(iterable, labels=label)\n",
    "        plt.show()\n",
    "        \n",
    "def display_distributions(iterables, labels=None, n_unique=30):\n",
    "    assert iter(iterables)\n",
    "\n",
    "    if all([iter(i) for i in iterables]):\n",
    "        cat_iter = []\n",
    "        num_iter = []\n",
    "        cat_l = []\n",
    "        num_l = []\n",
    "        for i, l in  zip(iterables, ['c' + str(i) for i in range(len(iterables))] if labels is None else labels):\n",
    "                \n",
    "            if len(np.unique(i)) <= n_unique:\n",
    "                print(\"values for \" + l)\n",
    "                print()\n",
    "                print(i.value_counts().sort_index())\n",
    "            else:\n",
    "                num_iter.append(i)\n",
    "                num_l.append(l)\n",
    "        \n",
    "        if num_iter:\n",
    "            plt.boxplot(num_iter, labels=num_l)  \n",
    "            plt.show()\n",
    "                                    \n",
    "    else:\n",
    "        display_one_distribution(iterables)         \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bSF = \"bSF\"\n",
    "print(pd.pivot_table(df, index=bqua, values=bSF, aggfunc=agg_funcs))\n",
    "# let's investigate the relation between the total area, finished and unfinished area\n",
    "# using the base dataframe as the area of the 2nd basement has been deemed irrelevant\n",
    "bSF1 = \"bSF1\"\n",
    "df_base['b_sum_SF'] = df_base[bSF1] + df_base[\"b_unfSF\"] + df_base[\"bSF2\"]\n",
    "total_no_sum = df_base[df[bSF] != df_base['b_sum_SF']].copy() \n",
    "print(total_no_sum.empty)\n",
    "# so as indicated by the names of the columns, there is a relation between both the 4 surface features\n",
    "# let's consider 2 main new features: ration_finished, finished surface\n",
    "\n",
    "df['bfSF'] = df_base['bSF1'] + df_base['bSF2'] \n",
    "df_test['bfSF'] = df_test_base['bSF1'] + df_test_base['bSF2']\n",
    "\n",
    "\n",
    "def bf_ratio(row):\n",
    "    row['bf_ratio'] = (row['bfSF'] / row[bSF])if row[bSF] != 0 else 0\n",
    "    return row\n",
    "\n",
    "apply_functions(bf_ratio)\n",
    "\n",
    "bs_ratio = 'bs_ratio'\n",
    "bfSF = 'bfSF'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd.pivot_table(df, index=bcond, values=bqua, aggfunc=agg_funcs))\n",
    "# let's consider the qualities of the condition 2 \n",
    "print(df[df[bcond] == 2][bqua].value_counts()) # the majority of has quality 3\n",
    "print(df[df[bcond] == 3][bqua].value_counts())\n",
    "print(df[bqua].value_counts())\n",
    "\n",
    "# so the point here is clear: bcondition is informative in a very specific way:\n",
    "# condition = 0 or condition = 5 then quality = condition\n",
    "# otherwise, no clear relation is visible for the moment\n",
    "\n",
    "# let's consider the categorical feature b_is_ex\n",
    "df['b_is_ex'] = (df[bcond] == 5).astype(int)\n",
    "df_test['b_is_ex'] = (df_test[bcond] == 5).astype(int)\n",
    "\n",
    "print(pd.pivot_table(df, index=bexp, values=bqua, aggfunc=agg_funcs)) # the feature does not seem to influence the \n",
    "# the quality of the basement.\n",
    "\n",
    "print(pd.pivot_table(df, index=bf1, values=bqua, aggfunc=agg_funcs)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to fanalize the state of the basement\n",
    "# I will introduce the bstate feature\n",
    "# as the product to the (quality + b_is_ex) * np.log(b_exp + 2)\n",
    "df['bstate'] = (df['bqua'] + 0.5 * df['b_is_ex']) * np.floor(np.log(df['bexp'] + np.exp(1)))\n",
    "df_test['bstate'] = (df_test['bqua'] + df_test['b_is_ex']) * np.floor(np.log(df_test['bexp'] + np.exp(1)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = df.drop(BAS_PCA, axis=1)\n",
    "y1 = X1.pop(Y)\n",
    "\n",
    "print(model_performance(X1, y1))\n",
    "\n",
    "X2 = df.drop(['bstate', 'bfSF', 'bf_ratio'], axis=1)\n",
    "y2 = X2.pop(Y)\n",
    "\n",
    "print(model_performance(X2, y2))\n",
    "# it is settled, I will keep my features\n",
    "df.drop(BAS_PCA, axis=1, inplace=True)\n",
    "bas_cols = ['bqua', 'bcond','bexp', 'bf1', 'bSF1', 'bf2', 'b_unfSF', 'bSF', 'bSF2', 'b_is_ex', \"BsmtFullBath\"]\n",
    "drop_cols(bas_cols)\n",
    "classify_cols()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Area features \n",
    "There is a number of features that represent a sort of areas in the house. I will try to investigate the different possible relations between those areas and how to make the most out of them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "area_features = [col for col in df.columns.values if 'area' in col.strip().lower() or 'sf' in col.strip().lower()]\n",
    "print(area_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_col_names({\"1stFlrSF\": \"f1SF\",\"2ndFlrSF\": \"f2SF\", \"LotArea\": \"area\", \"GrLivArea\": \"liv_area\"})\n",
    "df['floor_SF'] = df['f1SF'] + df['f2SF']\n",
    "\n",
    "print(len(df[df['floor_SF'] != df['liv_area']])) # one valid assumption for this difference is that some houses have a\n",
    "\n",
    "# df['liv_ratio'] = df['liv_area'] /  df['area']\n",
    "\n",
    "new_col_names({\"TotRmsAbvGrd\": \"n_rooms\"})\n",
    "\n",
    "df['area_by_room'] = df['liv_area'] / df['n_rooms']## unused\n",
    "\n",
    "df['liv_ratio'] = df['liv_area'] / df['area'] ## unused\n",
    "\n",
    "X_area = df[['f1SF', 'f2SF', 'area', 'liv_area', 'area_by_room', 'liv_ratio', Y]]\n",
    "\n",
    "print(make_mi_scores(X_area.copy(), X_area.pop(Y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(\"area_by_room\", axis=1, inplace=True)\n",
    "df.drop(\"liv_ratio\", axis=1, inplace=True)\n",
    "df.drop(\"floor_SF\", axis=1, inplace=True)\n",
    "\n",
    "cv = df['n_rooms'].value_counts()\n",
    "x = cv.index.values\n",
    "y = cv.values\n",
    "plt.bar(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the main idea here is to create a feature that increases with the living area, but punishes having too many rooms or too few rooms\n",
    "\n",
    "df['area+room'] = 4 * np.log(df['area']) / (np.abs(df['n_rooms'] - 7) + 1) \n",
    "df_test['area+room'] = 4 * np.log(df_test['area']) / (np.abs(df_test['n_rooms'] - 7) + 1) \n",
    "\n",
    "df.plot(kind='scatter', x='area+room', y=Y)\n",
    "\n",
    "X = df.copy()\n",
    "y = X.pop(Y)\n",
    "\n",
    "print(make_mi_scores(X, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first floor, second floor, number of rooms are to be dropped now\n",
    "drop_cols(['f1SF', 'f2SF'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.columns)\n",
    "print(df_test.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # we will consider the first 5 besides the MasVrnArea\n",
    "# new_col_names({\"1stFlrSF\": \"f1SF\",\"2ndFlrSF\": \"f2SF\", \"LotArea\": \"area\", \"GrLivArea\": \"liv_area\"})\n",
    "\n",
    "# df['liv_ratio'] = df['liv_area'] /  df['area']\n",
    "# df.plot(kind='scatter', x='liv_ratio', y=Y)\n",
    "# # df.plot(kind='scatter', x='area', y=Y)\n",
    "# # the lot area's plot it remarkably skewed. Let's do two things:\n",
    "# ## consider the values with extremely large lot areas (possible outliers)\n",
    "# ## consider applying a mathematical transformation\n",
    "\n",
    "# # df[df['area'] <= 5 * 10 ** 4].plot(kind='scatter', x='area', y=Y)\n",
    "# print(df['area'].describe())\n",
    "# # the fourth quartile starts from 11600\n",
    "# fth_q_lot_area = 11600\n",
    "\n",
    "# sus_area = df[df['area'] > fth_q_lot_area]\n",
    "# sus_area.plot(kind='scatter', x='area', y=Y)\n",
    "# # apparently outliers start from 30k \n",
    "\n",
    "# sus_area = df[df['area'] > 3 * 10 ** 4]\n",
    "# norm_area = df[df['area'] <= 3 * 10 ** 4]\n",
    "# # norm_area.plot(kind='scatter', x='area', y=Y)\n",
    "# norm_area['area'] = np.log(norm_area['area'])\n",
    "\n",
    "# plt.bar(norm_area['area'], norm_area[Y])\n",
    "# plt.show()\n",
    "\n",
    "# # norm_area.plot(kind='scatter', x='area', y=Y) # close to normal distribution with a slightly high mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Porches: outside areas\n",
    "We have 4 features that represent certain outside areas that might add values to the house."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wd = \"WoodDeckSF\"\n",
    "p = 'porch'\n",
    "p_list = [col for col in df.columns if \"porch\" in col.lower()]\n",
    "p_list.append(wd)\n",
    "\n",
    "df[p] = 0\n",
    "for pr in p_list:\n",
    "    df[p] += (df[pr] != 0)\n",
    "print(df[p].value_counts())\n",
    "\n",
    "# a house in the training data has at most one porch\n",
    "# the final feature for porchs is the sum of all prochs + the wood deck\n",
    "\n",
    "def area_out(row):\n",
    "    row[p] = 0\n",
    "    for por in p_list:\n",
    "        row[p] += row[por]\n",
    "    return row\n",
    "\n",
    "apply_functions(area_out)\n",
    "\n",
    "drop_cols(p_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Heat and Electricity\n",
    "There is a number of features related to heating and electricity that can be combined to forma more informative feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hq = \"HeatingQC\"\n",
    "ca = \"CentralAir\"\n",
    "es = \"Electrical\"\n",
    "hf = [hq, ca, es]\n",
    "for v in hf:\n",
    "    print(df[v].value_counts())\n",
    "\n",
    "# the distribution of such values is significantly unbalanced\n",
    "# let's verify whether the difference is impactful or not\n",
    "\n",
    "def set_heating(row):\n",
    "    row['heat'] = ((row[hq] >= 3) and (row[ca] == 1) and (row[es] == 'SBrkr'))\n",
    "    row['heat'] = row['heat'].astype(int)\n",
    "    return row\n",
    "\n",
    "apply_functions(set_heating)\n",
    "drop_cols(hf)\n",
    "classify_cols() # making sure all changes are reflected on both datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Years features\n",
    "It might be more meanignful to consider the age of the house than the it was built"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's first start with converting the years to ages as they are more intuitive\n",
    "age = 'age'\n",
    "n_age = \"new_age\"\n",
    "\n",
    "df[age] = df[yb].max() -  df[yb] + 1\n",
    "df[n_age] = df[yb].max() - df[yr] + 1\n",
    "\n",
    "df_test[age] = df_test[yb].max() -  df_test[yb] + 1\n",
    "df_test[n_age] = df_test[yb].max() - df_test[yr] + 1\n",
    "\n",
    "drop_cols([yb, yr])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # let's try clustering the age\n",
    "\n",
    "# df['age_group'] = pd.qcut(df[n_age], 8)\n",
    "# df_test['age_group'] = pd.qcut(df[n_age], 8)\n",
    "\n",
    "# age_groups = df['age_group'].cat.categories.tolist()\n",
    "# age_groups.sort()\n",
    "\n",
    "# def set_age_group(row):\n",
    "#     row['age_group'] = age_groups.index(row['age_group']) + 1\n",
    "#     return row\n",
    "\n",
    "# apply_functions(set_age_group)\n",
    "# X = df.copy()\n",
    "# y = X.pop(Y)\n",
    "# print(make_mi_scores(X, y)[[age, n_age, 'age_group']])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.drop(age_eval, axis=1)\n",
    "# drop_cols(n_age)\n",
    "# X = df.copy()\n",
    "# y = X.pop(Y)\n",
    "# print(model_performance(X, y))\n",
    "\n",
    "# X1 = df.drop(age_eval, axis=1).copy()\n",
    "# y1 = X1.pop(Y)\n",
    "# print(model_performance(X1, y1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning\n",
    "I am using the XGboostRegressor which is quite a powerful model. Other considerations should be taken into account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df.copy()\n",
    "y_train = X_train.pop(Y)\n",
    "\n",
    "xgb_params = dict(\n",
    "    max_depth=6,           # maximum depth of each tree - try 2 to 10\n",
    "    learning_rate=0.01,    # effect of each tree - try 0.0001 to 0.1\n",
    "    n_estimators=1000,     # number of trees (that is, boosting rounds) - try 1000 to 8000\n",
    "    min_child_weight=1,    # minimum number of houses in a leaf - try 1 to 10\n",
    "    colsample_bytree=0.7,  # fraction of features (columns) per tree - try 0.2 to 1.0\n",
    "    subsample=0.7,         # fraction of instances (rows) per tree - try 0.2 to 1.0\n",
    "    reg_alpha=0.5,         # L1 regularization (like LASSO) - try 0.0 to 10.0\n",
    "    reg_lambda=1.0,        # L2 regularization (like Ridge) - try 0.0 to 10.0\n",
    "    num_parallel_tree=1,   # set > 1 for boosted random forests\n",
    ")\n",
    "\n",
    "xgb = xgr(**xgb_params)\n",
    "model_performance(X_train, y_train, xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test =df_test.values\n",
    "# XGB minimizes MSE, but competition loss is RMSLE\n",
    "# So, we need to log-transform y to train and exp-transform the predictions\n",
    "xgb.fit(X_train, np.log(y_train))\n",
    "predictions = np.exp(xgb.predict(X_test))\n",
    "\n",
    "def create_sub(y_pred, sub_name):\n",
    "    global df_test_org\n",
    "    sub_df = pd.DataFrame({\"Id\": df_test_org['Id'],\"SalePrice\": y_pred})\n",
    "    sub_df.to_csv(sub_name, index=False)\n",
    "\n",
    "create_sub(predictions, \"XGBoostRegressor.csv\")\n",
    "print(\"Your submission was successfully saved!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('ds_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "006414dea9a04848ce797b510a25f3f28ac8668e3d3244e777242cca6bed477f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
