{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# House Prices: Advanced Regression techniques\n",
    "This notebook is my attempt in the [House Prices](https://www.kaggle.com/competitions/house-prices-advanced-regression-techniques/overview)  data science competition hosted by Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First exploration steps\n",
    "### removing columns with a high Nan ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = \"train.csv\"\n",
    "test_file = \"test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_org = pd.read_csv(train_file)\n",
    "df_test_org = pd.read_csv(test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# understand the data\n",
    "# print(df_train_org.head()) \n",
    "# so we can see that there  are 79 features \n",
    "print(len(df_train_org))\n",
    "# and 1460 training samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_train_org.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df.loc[:, \"SalePrice\"]\n",
    "df = df.rename(columns={\"SalePrice\":\"y\"})\n",
    "# df = df.drop(\"y\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = df.isna().sum().tolist()\n",
    "null_lists = [i for i in range(len(l)) if l[i] > 0]\n",
    "print(df.columns[null_lists])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's consider the ratio of non-Na values\n",
    "l_ratio = np.array(l) / len(df)\n",
    "\n",
    "nan_threshold = 0.8 # any feature with a non ration higher than 0.8 will be droppped\n",
    "drop = l_ratio >= nan_threshold\n",
    "valid_cols = [i for i in range(len(drop)) if not drop[i]]\n",
    "\n",
    "invalid_cols = [i for i in range(len(drop)) if drop[i]]\n",
    "print(df.columns[invalid_cols])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df_test_org.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = df.iloc[:, valid_cols].drop(\"Id\", axis=1)\n",
    "df_test = df_test.iloc[:, valid_cols[:-1]].drop(\"Id\", axis=1)\n",
    "combined = [df, df_test]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.copy() \n",
    "y = \"y\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data exploration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_func = ['count', np.mean, np.max, np.min]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_COLS = summary.columns.tolist()\n",
    "print(len(NUM_COLS))\n",
    "CAT_COLS = [col for col in df.columns.tolist() if col not in NUM_COLS]\n",
    "print(len(CAT_COLS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_num_feature_price(start, number):\n",
    "    global df, agg_func\n",
    "    \n",
    "    for j in range(start, min(start+number, len(NUM_COLS) - 1)):\n",
    "        if NUM_COLS[j] in df.columns:\n",
    "            try:\n",
    "                df.plot(kind='scatter', x=NUM_COLS[j], y='y', title=\"{}'s effect on price\".format(NUM_COLS[j]))\n",
    "            except:\n",
    "                print(pd.pivot_table(df, index=NUM_COLS[j], values=y, aggfunc=agg_func))\n",
    "        else:\n",
    "            print(NUM_COLS[j] + \" has already been dropped in the previous sections\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_num_feature_price(0, 5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insights after considering these plots:\n",
    "* certain zones set a limit to the lot's price, others don't: consider pivoting the table on zones\n",
    "* lot's frontage and lot's aread  might not be good indicators\n",
    "* it might be a good idea to combine quality and condition to find a better feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Working on the house's class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# even though this feature is numerical, it semantically categorical. \n",
    "# My suggestion is to limit its possible categories by the price of its most expensive house\n",
    "ms_class = \"MSSubClass\"\n",
    "for d in combined:\n",
    "    d.rename(columns={ms_class:\"sub_c\"}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_c  = \"sub_c\"\n",
    "price_per_class = pd.pivot_table(df, index=sub_c, values='y', aggfunc=['count', np.median, np.mean, np.min, np.max])\n",
    "\n",
    "print(price_per_class.sort_values((\"median\", \"y\"), ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in combined:\n",
    "    d['sub_c']= d['sub_c'].apply(lambda x: x if x != 150 else 120)\n",
    "\n",
    "for d in combined:\n",
    "   d[\"sub_c\"] = d[\"sub_c\"].apply(lambda x: price_per_class.loc[x, (\"amax\", \"y\")].squeeze() // (10 ** 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### working on quality + condition\n",
    "let's consider combining both these features into one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, df_test = combined\n",
    "qua = \"OverallQual\"\n",
    "con = \"OverallCond\"\n",
    "\n",
    "price_by_con = pd.pivot_table(df, index=con, values='y', aggfunc=['count', np.nanmean, np.min, np.max])\n",
    "print(price_by_con)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pri_qua_con = df.loc[:, [con, qua, y]]\n",
    "qua_price = {} #to store all the pair of houses where one house's quality is better than the other while the price is not in the same direction\n",
    "l = len(pri_qua_con)\n",
    "for i in range(l):\n",
    "    for j in range(i + 1, l):\n",
    "        if pri_qua_con.iloc[i, 1].squeeze() > pri_qua_con.iloc[j, 1].squeeze() \\\n",
    "        and pri_qua_con.iloc[i, 2].squeeze() < pri_qua_con.iloc[j, 2].squeeze() \\\n",
    "        and pri_qua_con.iloc[i, 0].squeeze() != pri_qua_con.iloc[j, 0].squeeze(): \n",
    "            qua_price[(i, j)] = [pri_qua_con.iloc[i, :], pri_qua_con.iloc[j, :]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the two feature condition and quality will be transformed into a single feature: evalution = con * coeff + qua\n",
    "# it is time to calculate coeff\n",
    "coeff = 0\n",
    "\n",
    "for value in qua_price.values():\n",
    "    coeff += abs(value[0][con] - value[1][con]) / abs(value[0][qua] - value[1][qua])  - 1\n",
    "coeff /= len(qua_price)\n",
    "print(coeff)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(2):\n",
    "    combined[i]['eval'] = (combined[i][qua] + coeff * combined[i][con]).astype(int)\n",
    "\n",
    "for i in range(2):\n",
    "    try:\n",
    "        combined[i].drop([con, qua], axis=1, inplace=True)\n",
    "    except:\n",
    "        print(\"already dropped\")\n",
    "\n",
    "df, df_test = combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# also drop the lot's aread column: it is replaced with more significant areas\n",
    "for i in range(2):\n",
    "    try:\n",
    "        combined[i].drop(\"LotArea\", axis=1, inplace=True)\n",
    "    except:\n",
    "        print(\"already dropped\")\n",
    "NUM_COLS.remove(\"LotArea\")\n",
    "df, df_test = combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_num_feature_price(0, 5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insights on the considered features:\n",
    "* the MSVerArea should not be used as it is\n",
    "* as for the first Basement, the larger the better\n",
    "* the surface of the 2nd basement does not seem to affect the price much\n",
    "* It might be worth considering only YearRemoddAdd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Working the Basement's features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping the unfinished area\n",
    "for i in range(2):\n",
    "    try:\n",
    "        combined[i].drop('BsmtUnfSF', axis=1, inplace=True)\n",
    "    except:\n",
    "        print(\"already dropped\")\n",
    "# NUM_COLS.remove('BsmtUnfSF')\n",
    "df, df_test = combined\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hypothese, with a second basement present, the first basement is not that large\n",
    "sec_bas_area = \"BsmtFinSF2\"\n",
    "# print(np.unique(df[sec_bas_area].values))\n",
    "fir_bas_area = \"BsmtFinSF1\"\n",
    "only_one_basement = df[(df[sec_bas_area] == 0) & (df[fir_bas_area] > 0)] \n",
    "with_sec_bas = df[(df[sec_bas_area] > 0) & (df[fir_bas_area] > 0)]\n",
    "print(only_one_basement[fir_bas_area].describe())\n",
    "print(\"#\" * 70)\n",
    "print(with_sec_bas[fir_bas_area].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## let's consider more the basement related features as more correlations probably exist\n",
    "bas_h = \"BsmtQual\"\t\t\n",
    "bas_c = \"BsmtCond\" \n",
    "bas_x = \"BsmtExposure\"\n",
    "bas_r1 = \"BsmtFinType1\"\n",
    "bas_r2 = \"BsmtFinType2\"\n",
    "\n",
    "bas_h_v = [\"Ex\",\"Gd\",\"TA\",\"Fa\",\"Po\",\"NA\"]\n",
    "bas_c_v= bas_h_v\n",
    "bas_x_v = [\"Gd\", \"Av\", \"Mn\", \"No\", \"NA\"]\n",
    "bas_r1_v = [\"GLQ\",\"ALQ\",\"BLQ\"\t,\"Rec\"\t,\"LwQ\"\t,\"Unf\",\t\"NA\"]\n",
    "bas_r2_v = bas_r1_v\n",
    "# let's first set them to ordincal categorical columns\n",
    "feats = [bas_h, bas_c, bas_x, bas_r1, bas_r2]\n",
    "order = [bas_h_v, bas_c_v, bas_x_v, bas_r1_v, bas_r2_v]\n",
    "\n",
    "for f, o in zip(feats, order):\n",
    "    d[f] = df[f].astype(\"category\").cat.set_categories(o[::-1], ordered=True)\n",
    "\n",
    "for f, o in zip(feats, order):\n",
    "    df_test[f] = df_test[f].astype(\"category\").cat.set_categories(o[::-1], ordered=True)\n",
    "\n",
    "## this part was added later after experiementing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bas_feats = [fir_bas_area, sec_bas_area]\n",
    "bas_feats.extend(feats)\n",
    "# print(df[feats].isna().sum())\n",
    "# let's work on imputing the missing values.\n",
    "df_bas_mis = df[(df[bas_h].isna()) | (df[bas_c].isna()) | (df[bas_x].isna()) | (df[bas_r1].isna()) | (df[bas_r2].isna())] \n",
    "\n",
    "print(df_bas_mis.loc[:, bas_feats])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the ordinal data to numerical:\n",
    "def ord_to_num(unique_values, in_order=True):\n",
    "    uni_v = unique_values if in_order else unique_values[::-1]\n",
    "    return dict(zip(uni_v, range(0, len(unique_values))))\n",
    "\n",
    "for f, o in zip(feats, order):\n",
    "    df[f] = df[f].apply(ord_to_num(o, in_order=False).get)\n",
    "\n",
    "for f, o in zip(feats, order):\n",
    "    df_test[f] = df_test[f].apply(ord_to_num(o, in_order=False).get)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as we can see from the output, rows associated with Nan vaulues in the basement criteria columns 0.0 in the basement area\n",
    "# with the exception of a single row\n",
    "\n",
    "df[feats] = df[feats].fillna(0)\n",
    "df_test = df_test.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[feats].isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final evaluation of a basement's quality is as follows:\n",
    "$\\begin{align}\n",
    "E =  \\frac{c_1 \\cdot \\frac{BasQua}{6} + c_2 \\cdot \\frac{BasCon}{6} + c_3 \\cdot \\frac{BasExp}{5} + c_4 \\cdot \\frac{BasType1}{7}}{4}\n",
    "\\end{align}$\n",
    "where $c_i$ represents is the correlation of the $i$-th criteria with ***y*** divided by the sum of correlations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let' calculate correlations    \n",
    "\n",
    "corr = np.array([df.loc[:, [f, y]].corr()[y][f] for f in feats])\n",
    "corr = [c / np.sum(corr) for c in corr]\n",
    "c1, c2, c3, c4, c5 = corr\n",
    "print(corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bas_eval(row):\n",
    "    row['bas_eval_1'] = c1 * row[bas_h] / 6 + c2 * row[bas_c] / 6 + c3 * row[bas_x] / 5 + c4 * row[bas_r1] / 7\n",
    "    row['bas_eval_1'] = row['bas_eval_1'] / 4\n",
    "    return row\n",
    "\n",
    "df = df.apply(bas_eval, axis=1)\n",
    "df['bas_eval_2'] = c5 * df[bas_r2] / 7\n",
    "\n",
    "df_test = df_test.apply(bas_eval, axis=1)\n",
    "df_test['bas_eval_2'] = c5 * df_test[bas_r2] / 7\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final numerical value that best describes the basement contribution to the sale price is engineered as follows:\n",
    "$\\begin{align} E_1 \\cdot area_1 + E_2 \\cdot area_2\n",
    "\\end{align}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# after adding the final evaluation\n",
    "def basement(row):\n",
    "    row['bas'] = row['bas_eval_1'] * row[fir_bas_area] + row['bas_eval_2'] * row[sec_bas_area]\n",
    "    return row\n",
    "\n",
    "df = df.apply(basement, axis=1)\n",
    "df_test = df_test.apply(basement, axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.loc[:, [y,'bas']].corr()[y]['bas']) \n",
    "for f in bas_feats:\n",
    "    print(df.loc[:, [y, f]].corr()['y'][f])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a large number of basement-related features, the new feature **bas** incorporates as much information as possible. With a relatively high correlation of 0.45 it might represent a good indicator of the house' price.\n",
    "These changes will be applied to both train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bas_feats.extend(['bas_eval_1', 'bas_eval_2', \"TotalBsmtSF\"])\n",
    "\n",
    "df.drop(bas_feats, inplace=True, axis=1)\n",
    "df_test.drop(bas_feats, inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### studying the effect of the floors' areas and living area\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_num_feature_price(11, 5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "insights:\n",
    "* The total unfinished area might be dropped\n",
    "* The total basement's surface seems to positively correlate with the price\n",
    "* The floors' areas correlates positively\n",
    "* lowQualityFin surface will most probably be dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the basement's surface will be dropped in favor of the \"bas\" feature engineered in the previous section.\n",
    "\n",
    "# the lowQualityFin is to be dropped\n",
    "\n",
    "df.drop(\"LowQualFinSF\", axis=1, inplace=True)\n",
    "df_test.drop(\"LowQualFinSF\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_a = \"1stFlrSF\"\n",
    "f2_a = \"2ndFlrSF\"\n",
    "\n",
    "df_most_a = df[df[f1_a] <= 3000]\n",
    "\n",
    "df_most_a.plot(kind='scatter', x=f1_a, y=y,)\n",
    "\n",
    "df_f2 = df[df[f2_a] > 0]\n",
    "df_f2.plot(kind='scatter', x=f2_a, y=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.loc[:, [f1_a, f2_a, y]].corr()['y'])\n",
    "floors = df.loc[:, [f1_a, f2_a, y]]\n",
    "print(floors[floors[f1_a] <= floors[f2_a]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['area'] = df[f1_a] + df[f2_a]\n",
    "df.plot(kind='scatter', x='area', y=y)\n",
    "df_test['area'] = df_test[f1_a] + df_test[f2_a]\n",
    "\n",
    "print(df.loc[:, ['area', y]].corr())\n",
    "# the total area is singificantly more correlated with the price than each of floor's areas individually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's consider another feature, which the percentage of the living area out of the total area\n",
    "liv_a = \"GrLivArea\"\n",
    "df['living_a_p'] = df[liv_a] / df['area']\n",
    "df.plot(kind='scatter', x='living_a_p', y=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "liv_area_more_total = df[df['living_a_p'] != 1]\n",
    "liv_area_e_total = df[df['living_a_p'] == 1]\n",
    "print(liv_area_more_total[y].describe())\n",
    "print(\"#\" * 70)\n",
    "print(liv_area_e_total[y].describe())\n",
    "# we can see that having living_a_p larger than one does significantly affect the price, thus it is most probably more beneficial\n",
    "# to drop both living area and the new feature\n",
    "\n",
    "df.drop([liv_a, 'living_a_p', f1_a, f2_a], axis=1, inplace=True)\n",
    "df_test.drop([liv_a, f1_a, f2_a], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### bathrooms and half bathrooms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_num_feature_price(16, 4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "insights:\n",
    "* all bath features should be combined into one final more expressive feature\n",
    "* the living area is quite important feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### adding all bathes together\n",
    "b1 = \"BsmtFullBath\"\n",
    "bh1 = \"BsmtHalfBath\"\n",
    "b2 = \"FullBath\"\n",
    "bh2 = \"HalfBath\"\n",
    "df['baths'] = df[b1] + df[bh1] + df[b2] + df[bh2]\n",
    "df_test['baths'] = df_test[b1] + df_test[bh1] + df_test[b2] + df_test[bh2]\n",
    "\n",
    "df.drop([b1, bh1, b2, bh2], axis=1, inplace=True)\n",
    "df_test.drop([b1, bh1, b2, bh2], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### working on rooms and kitchen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_num_feature_price(20, 4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "insights:\n",
    "* the number of rooms should be coupled with the house's area. As a large number of houses might mean smaller ones which might lower the sale price.\n",
    "* the number of kitchens is not too significant. a final feature including rooms, bathrooms and kitchen might be necessary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_rooms = \"TotRmsAbvGrd\"\n",
    "n_bed = \"BedroomAbvGr\"\n",
    "df['area_room'] = df['area'] / df[n_rooms]\n",
    "df.plot(kind='scatter', x='area_room', y=y)\n",
    "print(df.loc[:, ['area_room', y]].corr())\n",
    "# add the new feature to test set\n",
    "df_test['area_room'] = df_test[n_rooms] / df_test['area']\n",
    "\n",
    "df.drop([n_bed, n_rooms],axis=1, inplace=True)\n",
    "df_test.drop([n_bed, n_rooms],axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kit = \"KitchenAbvGr\"\n",
    "\n",
    "df[kit] = df_train_org[kit].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# is kitchen significant?: the number is definitely not, let's check the quality\n",
    "kit_q = \"KitchenQual\"\n",
    "price_by_kit = pd.pivot_table(df, index=kit_q, values=y, aggfunc=['count', np.mean, np.max, np.min])\n",
    "print(price_by_kit)\n",
    "\n",
    "# we can see that kitchen's quality impacts the price range. houses with excellent houses are estimated by at least 86k. Additionally, the most expensive house is alos equipped with an excellent kitchen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_kitchen(row):\n",
    "    kit_qua = row[kit_q]\n",
    "    if kit_qua not in price_by_kit.index:\n",
    "        kit_qua = \"TA\"\n",
    "    min = price_by_kit.loc[kit_qua, ('amin', 'y')].squeeze()\n",
    "    max = price_by_kit.loc[kit_qua, ('amax', 'y')].squeeze()\n",
    "    count = price_by_kit.loc[kit_qua, ('count', 'y')].squeeze()\n",
    "    row['kit'] = min + (max - min) / count\n",
    "    return row\n",
    "df = df.apply(set_kitchen, axis=1)\n",
    "df_test = df_test.apply(set_kitchen, axis=1)\n",
    "print(df.loc[:, ['kit', y]].corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.apply(set_kitchen, axis=1)\n",
    "df.drop([kit_q, kit], axis=1, inplace=True)\n",
    "df_test.drop([kit_q, kit], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### working on Garage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_num_feature_price(23, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# similarly to the basement, garage is described by a number of other features such as condition, quality.\n",
    "# let's consider all of them in this section\n",
    "gt = \"GarageType\"\n",
    "gf = \"GarageFinish\"\n",
    "gq = \"GarageQual\"\n",
    "gcon = \"GarageCond\"\n",
    "\n",
    "gc = \"GarageCars\"\n",
    "ga = \"GarageArea\"\n",
    "\n",
    "gd = \"PavedDrive\"\n",
    "\n",
    "\n",
    "g_f_cat = [gt, gf, gq, gcon, gd]\n",
    "for f in g_f_cat:\n",
    "    print(pd.pivot_table(df, values=y, index=f, aggfunc=agg_func))\n",
    "\n",
    "g_feat = [gc, ga]\n",
    "g_feat.extend(g_f_cat)\n",
    "print(g_feat)\n",
    "# The vast majority of garages are of typical/average  quality and condition. The other categories do not set any remarkable price ranges.\n",
    "# On the other hand, the other two cat features require more careful consideration. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's consider the relation between the garage built year and\n",
    "gy = \"GarageYrBlt\"\n",
    "hy = \"GarageYrBlt\"\n",
    "hry = \"YearRemodAdd\"\n",
    "def compare_built_years(row):\n",
    "    row['hgy'] = (row[gy] - row[hy]) if (row[gy] < row[hry]) else (row[gy] - row[hry])\n",
    "    return row\n",
    "df = df.apply(compare_built_years, axis=1)\n",
    "print(pd.pivot_table(df, index='hgy', values=y, columns=gt, aggfunc=['count']))\n",
    "print(df['hgy'].value_counts())\n",
    "df.drop('hgy', axis=1, inplace=True)\n",
    "# as most of the garage were built either the same year as the house and most of houses with different years have a specific garage type\n",
    "# the year where the garage was built will be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_by_gt = pd.pivot_table(df, values=y, index=gt, aggfunc=agg_func)\n",
    "\n",
    "for t in price_by_gt.index:\n",
    "    df[df[gt] == t].loc[:, [gt, y]].plot(kind='scatter', y=y, x=gt, title=t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[gt] = df_train_org[gt].copy()\n",
    "df[gf] = df_train_org[gf].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# considering the different charts, a reasonable order of types is:\n",
    "gt_order = [\"NA\", \"CarPort\", \"2Types\", \"Basment\", \"Detchd\", \"BuiltIn\", \"Attchd\"]\n",
    "\n",
    "gf_order = [\"Fin\", \"RFn\", \"Unf\",\"NA\"][::-1]\n",
    "\n",
    "# let's apply this to our train data\n",
    "for f, o in zip([gt, gf], [gt_order, gf_order]):\n",
    "    df[f] = df[f].astype(\"category\").cat.set_categories(o, ordered=True)\n",
    "    df_test[f] = df_test[f].astype(\"category\").cat.set_categories(o, ordered=True)\n",
    "        \n",
    "for f, o in zip([gt, gf], [gt_order, gf_order]):    \n",
    "    df[f] = df[f].apply(ord_to_num(o).get)\n",
    "    df_test[f] = df_test[f].apply(ord_to_num(o).get)\n",
    "# fill NaN values\n",
    "# print(df[(df[gt].isna()) | (df[gf].isna())].loc[:, g_feat])\n",
    "df.fillna(dict(zip([gt, gf],[0, 0])), inplace=True)\n",
    "df_test.fillna(dict(zip([gt, gf],[0, 0])), inplace=True)\n",
    "\n",
    "for f, o in zip([gt, gf], [gt_order, gf_order]):    \n",
    "    df[f] = df[f].astype(int)\n",
    "    df_test[f] = df_test[f].astype(int)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final evaluation of a garage is estimated as:\n",
    "$\\begin{align} \n",
    "\\frac{c_1 \\frac{type}{7} + c_2 \\frac{finish}{4}}{2}\n",
    "\\end{align}$\n",
    "where $c_i$ are calculated out of correlations with the price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "corr = np.array([df.loc[:, [f, y]].corr()[y][f] for f in [gt, gf]])\n",
    "corr = [c / np.sum(corr) for c in corr]\n",
    "c1 , c2 = corr\n",
    "print(corr)\n",
    "g_eval = \"g_eval\"\n",
    "def garage_eval(row):\n",
    "    row[g_eval] = c1 * row[gt] / 7 + c2 * row[gf] / 4 \n",
    "    row[g_eval] = row[g_eval] / 2\n",
    "    return row\n",
    "\n",
    "df = df.apply(garage_eval, axis=1)\n",
    "df_test = df_test.apply(garage_eval, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.loc[:, [gf, gt, g_eval, y]].corr()['y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's finalize the garage contribution with coupling the garage evaluation with its area\n",
    "print(df.loc[:, [ga, gc, y]].corr())\n",
    "# we can easily see that garage's area and garage' cars are highly correlated features, it is superflous to keep both of them.\n",
    "\n",
    "df.plot(kind='scatter', x=gc, y=y, title='cars and price')\n",
    "df.plot(kind='scatter', x=ga, y=y, title='garage area and price')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's \n",
    "area_by_car = pd.pivot_table(df, index=gc, values=ga, aggfunc=agg_func)\n",
    "print(area_by_car)\n",
    "price_by_car = pd.pivot_table(df, index=gc, values=y, aggfunc=agg_func)\n",
    "print(\"*\" * 70)\n",
    "print(price_by_car)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set all 4s in the garage car column to 3\n",
    "df[gc] = df[gc].apply(lambda x: x if x != 4 else 3)\n",
    "df_test[gc] = df_test[gc].apply(lambda x: x if x != 4 else 3)\n",
    "\n",
    "# final estimatation of garage's contribution is the product between g_eval and g_cars\n",
    "g = \"garage\"\n",
    "def set_garage(row):\n",
    "    row[g] = row[gc] * row[g_eval]\n",
    "    return row\n",
    "\n",
    "df = df.apply(set_garage, axis=1)\n",
    "df_test = df_test.apply(set_garage, axis=1)\n",
    "print(df.loc[:, [g, y, gc, ga]].corr())\n",
    "g_feat.append(gy)\n",
    "df.drop(g_feat, axis=1, inplace=True)\n",
    "df_test.drop(g_feat, axis=1, inplace=True)\n",
    "df.drop(g_eval, axis=1, inplace=True)\n",
    "df_test.drop(g_eval, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### working with porches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_num_feature_price(26, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wd = \"WoodDeckSF\"\n",
    "p = 'porch'\n",
    "op = 'OpenPorchSF'\n",
    "ep = \"EnclosedPorch\"\n",
    "s3p = \"3SsnPorch\"\n",
    "sp = \"ScreenPorch\"\n",
    "\n",
    "p_list = [op, ep, s3p, sp]\n",
    "\n",
    "df[p] = ((df[op] != 0) + (df[ep] != 0) + (df[s3p] != 0) + (df[sp] != 0)).astype(int)\n",
    "print(df[p].value_counts())\n",
    "\n",
    "# a house in the training data has at most one porch\n",
    "# let's try to estimate the value of each type\n",
    "\n",
    "for po in p_list:\n",
    "    df[df[po] != 0].plot(kind='scatter', x=po, y=y, title= po + \"+ price\")\n",
    "\n",
    "# the final feature for porchs is the sum of all prochs + the wood deck\n",
    "df[p] = (df[op] + df[ep] + df[s3p] + df[sp] + df[wd])\n",
    "df_test[p] = (df[op] + df[ep] + df[s3p] + df[sp] + df[wd])\n",
    "p_list.append(wd)\n",
    "df.drop(p_list, axis=1, inplace=True)\n",
    "df_test.drop(p_list, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### working with other features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_num_feature_price(31, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "decisions:\n",
    "* the pool quality and miscellaneous values columns are to be dropped\n",
    "* the first and simplest approach is to drop the year as well as the month\n",
    "* the second is to reconsider the column after considering all other features, read more inflation in the [2006, 2010] period and inject this\n",
    "knowledge into the model. (For improving the model even further possibly !!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = \"YearRemodAdd\"\n",
    "\n",
    "def group_by_year(row):\n",
    "    row['decade'] = 1949 + ((row[year] - 1940) // 10) * 10\n",
    "    return row\n",
    "\n",
    "df = df.apply(group_by_year, axis=1)\n",
    "df_test = df_test.apply(group_by_year, axis=1)\n",
    "\n",
    "lf = \"LotFrontage\"\n",
    "yb = \"YearBuilt\"\n",
    "yr = \"YearRemodAdd\"\n",
    "pool = \"PoolArea\"\n",
    "mis = \"MiscVal\"\n",
    "mos = \"MoSold\"\n",
    "yrs = \"YrSold\"\n",
    "\n",
    "df.drop([lf, yb, yr, pool, mis, mos, yrs], axis=1, inplace=True)\n",
    "df_test.drop([lf, yb, yr, pool, mis, mos, yrs], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### working with 1st set of categorial features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cat_feature_price(start, number):\n",
    "    global df, agg_func\n",
    "    \n",
    "    for j in range(start, min(start+number, len(CAT_COLS) - 1)):\n",
    "        if CAT_COLS[j] in df.columns:\n",
    "            try:\n",
    "                df.plot(kind='scatter', x=CAT_COLS[j], y='y', title=\"{}'s effect on price\".format(CAT_COLS[j]))\n",
    "            except:\n",
    "                print(pd.pivot_table(df, index=CAT_COLS[j], values=y, aggfunc=agg_func))\n",
    "        else:\n",
    "            print(CAT_COLS[j] + \" has already been dropped in the previous sections\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cat_feature_price(0, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first let's check the Utitilies column in the test dataset\n",
    "u = \"Utilities\"\n",
    "print(len(df_test[df_test[u] != \"AllPub\"][u]))\n",
    "# only two outliers in the test set and one in the training set\n",
    "# the utitilies column is to be dropped\n",
    "df.drop(u, axis=1, inplace=True)\n",
    "df_test.drop(u, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = \"Street\"\n",
    "df.drop(s, axis=1, inplace=True)\n",
    "df_test.drop(s, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zone = \"MSZoning\"\n",
    "df[zone] = df_train_org[zone].copy()\n",
    "price_by_z = pd.pivot_table(df, values=y, index=zone, aggfunc=agg_func)\n",
    "print(price_by_z)\n",
    "def set_zone(row):\n",
    "    z = row[zone]\n",
    "    if z not in price_by_z.index:\n",
    "        z = \"RL\"\n",
    "    min = price_by_z.loc[z, ('amin', 'y')].squeeze()\n",
    "    max = price_by_z.loc[z, ('amax', 'y')].squeeze()\n",
    "    count = price_by_z.loc[z, ('count', 'y')].squeeze()\n",
    "    row[\"zone\"] = min + (max - min) / count\n",
    "    return row\n",
    "df = df.apply(set_zone, axis=1)\n",
    "df_test = df_test.apply(set_zone, axis=1)\n",
    "df.drop(zone, axis=1, inplace=True)\n",
    "df_test.drop(zone, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cat_feature_price(5, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the Neighborhood column requires knowledge of the city itself: it will be temporarily dropped\n",
    "# the conditions columns can be mainly expressed as an extra noise feature\n",
    "c1 = \"Condition1\"\n",
    "c2 = \"Condition2\"\n",
    "n = \"Norm\"\n",
    "df['e_noise'] = ((df[c1] != n) + (df[c2] != n)).astype(int)\n",
    "df_test['e_noise'] = (df_test[c1] != n).astype(int) + (df_test[c2] != n).astype(int)\n",
    "nei = \"Neighborhood\"\n",
    "\n",
    "df.drop([c1, c2, nei], axis=1, inplace=True)\n",
    "df_test.drop([c1, c2, nei], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Working with the 2nd set of categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cat_feature_price(10, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taking into consideration that the first feature represents a a large number of categories \n",
    "# the columns \"BldgType\" and \"HouseStyle\" can be got rid of.\n",
    "df.drop([\"HouseStyle\", \"BldgType\"], axis=1, inplace=True)\n",
    "df_test.drop([\"HouseStyle\", \"BldgType\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After conducting slight research on the different available building materials, I decided to group the materials into four groups according to \n",
    "reliability and cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad = [\"AsbShng\",\"AsphShn\",\"CBlock\", \"Other\"]\n",
    "good_aff = [\"CemntBd\", \"HdBoard\", \"ImStucc\", \"Stucco\", \"PreCast\", \"Plywood\", \"WdShing\"]\n",
    "good_exp = [\"Stone\", \"BrkComm\", \"BrkFace\"]\n",
    "sidings = [\"MetalSd\", \"VinylSd\", \"Wd Sdng\"]\n",
    "material = [bad, good_aff, good_exp, sidings]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = \"RoofStyle\"\n",
    "rm = \"RoofMatl\"\n",
    "ex1 = \"Exterior1st\"\n",
    "ex2 = \"Exterior2nd\"\n",
    "# let's first set the exterior feature\n",
    "ex = \"exterior\"\n",
    "\n",
    "exq = \"ExterQual\"\n",
    "exc = \"ExterCond\"\n",
    "ex_v = [\"Ex\",\"Gd\",\"TA\",\"Fa\",\"Po\"][::-1]\n",
    "\n",
    "df[ex1] = df_train_org[ex1].copy()\n",
    "df[ex2] = df_train_org[ex2].copy()\n",
    "df[exq] = df_train_org[exq].copy()\n",
    "df[exc] = df_train_org[exc].copy()\n",
    "\n",
    "df_test[ex1] = df_test_org[ex1].copy()\n",
    "df_test[ex2] = df_test_org[ex2].copy()\n",
    "df_test[exq] = df_test_org[exq].copy()\n",
    "df_test[exc] = df_test_org[exc].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f, o in zip([exq, exc], [ex_v, ex_v]):\n",
    "    df[f] = df[f].astype(\"category\").cat.set_categories(o, ordered=True)\n",
    "    df_test[f] = df_test[f].astype(\"category\").cat.set_categories(o, ordered=True)\n",
    "\n",
    "df_test.fillna({ex1:1, ex2:1}, inplace=True)\n",
    "\n",
    "for f, o in zip([exq, exc], [ex_v, ex_v]):\n",
    "    df[f] = df[f].apply(ord_to_num(o).get).astype(int)\n",
    "    df_test[f] = df_test[f].apply(ord_to_num(o).get).astype(int)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's first consider the correlations\n",
    "corr = np.array([df.loc[:, [f, y]].corr()[y][f] for f in [exq, exc]])\n",
    "corr = corr / np.sum(corr)\n",
    "c1, c2 = corr\n",
    "# np.array([df.loc[:, [f, y]].corr()[y][f]for f in [exq, exc]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ex_eval = 'ex_eval'\n",
    "def set_exterior(row):\n",
    "    global c1, c2\n",
    "    row[ex_eval] = c1 * row[exq] + c2 * row[exc]\n",
    "    row[ex] = 0\n",
    "    mat1 = row[ex1]\n",
    "    mat2 = row[ex2]\n",
    "    for i in range(len(material)):\n",
    "        if mat1 in material[i]:\n",
    "            row[ex] += i \n",
    "        if mat2 in material[i]:\n",
    "            row[ex] += i\n",
    "    row[ex] = row[ex] * row[ex_eval]\n",
    "    return row \n",
    "\n",
    "df = df.apply(set_exterior, axis=1)\n",
    "df_test = df_test.apply(set_exterior, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.loc[:, [ex, ex1, ex2, ex_eval, exq, exc, y]].corr()['y'])\n",
    "# we can experiment with both:, ex_eval and ex, both are not too bad of features\n",
    "# let's drop features now:\n",
    "df.drop([ex1, ex2, exq, exc], axis=1, inplace=True)\n",
    "df_test.drop([ex1, ex2, exq, exc], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Working on the 3rd set of categorical features: roof + lot + land"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Roof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cat_feature_price(10, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's pivot the table on roof's type as well as material\n",
    "rfs = \"RoofStyle\"\n",
    "rfm = \"RoofMatl\"\n",
    "###\n",
    "df[rfs] = df_train_org[rfs].copy()\n",
    "df[rfm] = df_train_org[rfm].copy()\n",
    "df_test[rfs] = df_test_org[rfs].copy()\n",
    "df_test[rfm] = df_test_org[rfm].copy()\n",
    "\n",
    "\n",
    "price_by_rfs = pd.pivot_table(df, index=rfs, values=y, aggfunc=agg_func) \n",
    "\n",
    "print(len(df[df[rfm] == \"CompShg\"]) / len(df))\n",
    "# the roof's material will be dropped as 98,2 % of the roofs are built with the same material\n",
    "# let's set the rfs column and check the test dataset first\n",
    "\n",
    "print(df.loc[df[rfs].isna()]) # no Nan values in the test set\n",
    "\n",
    "def set_rfs(row):\n",
    "    rf = row[rfs]\n",
    "    min = price_by_rfs.loc[rf, ('amin', 'y')].squeeze()\n",
    "    max = price_by_rfs.loc[rf, ('amax', 'y')].squeeze()\n",
    "    count = price_by_rfs.loc[rf, ('count', 'y')].squeeze()\n",
    "    row['roof'] = min + (max - min) / count\n",
    "    return row\n",
    "\n",
    "df = df.apply(set_rfs, axis=1)\n",
    "df_test = df_test.apply(set_rfs, axis=1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop([rfs, rfm], axis=1, inplace=True)\n",
    "df_test.drop([rfs, rfm], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Lot + Land"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's consider the land\n",
    "lot_land = [\"LandContour\", \"LotShape\", \"LotConfig\", \"LandSlope\"]\n",
    "for f in lot_land:\n",
    "    if f in df.columns: \n",
    "        df.plot(kind='scatter', x=f, y=y)\n",
    "ls = \"LotShape\"\n",
    "\n",
    "def set_shape(row):\n",
    "    row['shape'] = 1 if row[ls] in [\"Reg\", \"IR1\"] else 0\n",
    "    return row\n",
    "\n",
    "l_c = \"LotConfig\"\n",
    "def set_config(row):\n",
    "    if row[l_c] == \"Inside\":\n",
    "        row[\"config\"] = 2\n",
    "    if row[l_c] == \"Corner\":\n",
    "        row['config'] = 1\n",
    "    else: \n",
    "        row['config'] = 0\n",
    "    return row\n",
    "\n",
    "l_s = \"LandSlope\"\n",
    "def set_slope(row):\n",
    "    row['slope'] = 1 if row[l_s] == \"Gtl\" else 0\n",
    "    return row\n",
    "\n",
    "l_l_f = [set_config, set_shape, set_slope]\n",
    "for f in l_l_f:\n",
    "    df = df.apply(f, axis=1)\n",
    "    df_test = df_test.apply(f, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "landc = \"LandContour\"\n",
    "df[landc] = df_train_org[landc].copy()\n",
    "df_test[landc] = df_test_org[landc].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# consider the landcontour \n",
    "print(pd.pivot_table(df, values=y, index=landc, aggfunc=agg_func))\n",
    "df[(df[landc] == 'Bnk') | (df[landc] == 'Low')].plot(kind='scatter', x=landc, y=y)\n",
    "# we can see that Low is a better option than Bnk\n",
    "\n",
    "df[(df[landc] == 'Bnk') | (df[landc] == 'Low')].plot(kind='scatter', x=landc, y=y)\n",
    "df[(df[landc] == 'HLS') | (df[landc] == 'Lvl')].plot(kind='scatter', x=landc, y=y)\n",
    "# Lvl is indeed a better option than HLS\n",
    "# then the final order is: \n",
    "land_c_o = [\"Lvl\", \"HLS\", \"Low\", \"Bnk\"][::-1]\n",
    "# let's fill the Nan values first\n",
    "print(df[landc].isna().sum())\n",
    "print(df_test[landc].isna().sum())\n",
    "# there are no Nan values: thus we can order the values and convert them to int\n",
    "\n",
    "\n",
    "df[landc] = df[landc].apply(ord_to_num(land_c_o).get).astype(int)\n",
    "\n",
    "df_test[landc] = df_test[landc].apply(ord_to_num(land_c_o).get).astype(int)\n",
    "\n",
    "# dropping the original features\n",
    "df.drop(lot_land, axis=1, inplace=True)\n",
    "df_test.drop(lot_land, axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Working with Heat and Electricity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = \"Heating\"\n",
    "hq = \"HeatingQC\"\n",
    "ca = \"CentralAir\"\n",
    "es = \"Electrical\"\n",
    "hf = [h, hq, ca, es]\n",
    "print(pd.pivot_table(df, index=es, values=y, aggfunc=agg_func))\n",
    "print(df[es].isna().sum())\n",
    "# he = [h, hq, ca, es]\n",
    "# for f in he:\n",
    "#     try:\n",
    "#         df.plot(kind='scatter', y=y, x=f)\n",
    "#     except:\n",
    "#         print(pd.pivot_table(df, index=f, values=y, aggfunc=agg_func))\n",
    "\n",
    "\n",
    "y_by_elec = pd.pivot_table(df, index='decade', columns=es, aggfunc=['count'], values=y)\n",
    "\n",
    "print(y_by_elec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the simplest feature to engineer out of eletrical is turn it into a categorical feature as : isSBrkr\n",
    "def set_elec(row):\n",
    "    row['elec'] = 0 if row[es] != 'SBrkr' else 1 \n",
    "    return row\n",
    "\n",
    "# the same can be applied to heating type\n",
    "def set_heat(row):\n",
    "    row['heat'] = 1 if row[h] != 'GasA' else 2\n",
    "    points = {\"Ex\":5, \"Gd\":4, \"TA\":3, \"Fa\":2, \"Po\":1}\n",
    "    row['heat'] = row['heat'] * points[row[hq]]\n",
    "    return row\n",
    "\n",
    "df = df.apply(set_elec, axis=1)\n",
    "df = df.apply(set_heat, axis=1)\n",
    "\n",
    "df_test = df_test.apply(set_elec, axis=1)\n",
    "df_test = df_test.apply(set_heat, axis=1)\n",
    "\n",
    "df[\"ca\"] = df[ca].apply({\"N\":0, \"Y\":1}.get)\n",
    "df_test[\"ca\"] = df_test[ca].apply({\"N\":0, \"Y\":1}.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(hf, axis=1, inplace=True)\n",
    "df_test.drop(hf, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['Functional',\"SaleType\"], axis=1, inplace=True)\n",
    "df_test.drop(['Functional',\"SaleType\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The categorical features left so far: \n",
    "* MsType\n",
    "* FirePlaceQuality\n",
    "* HeatingQuality, central Air\n",
    "* SaleType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cat_feature_price(0, 55)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_num_feature_price(0, 50)\n",
    "to_drop = ['MasVnrArea', \"Fireplaces\", \"FireplaceQu\", \"MasVnrType\", \"Foundation\", 'SaleCondition']\n",
    "df.drop(to_drop, axis=1, inplace=True)\n",
    "df_test.drop(to_drop, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print((df.columns))\n",
    "# print((df_test.columns))\n",
    "\n",
    "# additional thoughts on the features:\n",
    "# baths might not be a very good indicator\n",
    "# elec , heat, ca can be further merged to a single feature\n",
    "# ex_eval, exterior can be manipulated even further\n",
    "# let's check the correlations\n",
    "print(len(df.corr()['y']) == len(df.columns)) # this means all data is numerical and can be fed to a model\n",
    "print(df.isna().sum().sum()) # there is no Nan values in the training data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "X_train = df.drop(y, axis=1).values\n",
    "y_train = df[y].values # / 10 ** 3\n",
    "X_test = df_test.values \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Models\n",
    "Very first attempts are to better evaluate the problem's difficulty as well as the quality of the features engineered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test) \n",
    "print(X_train)\n",
    "print(\"#\" * 50)\n",
    "print(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, KFold, cross_validate\n",
    "n_splits = 4\n",
    "random_state = 5\n",
    "kf = KFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = LinearRegression()\n",
    "# reg.fit(X_train, y_train)\n",
    "\n",
    "metrics = ['neg_mean_absolute_error', 'r2']\n",
    "cv_scores = cross_validate(reg, X_train, y_train, cv=kf)\n",
    "print(cv_scores)\n",
    "reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sub(y_pred, sub_name):\n",
    "    global df_test_org\n",
    "    sub_df = pd.DataFrame({\"Id\": df_test_org['Id'],\"SalePrice\": y_pred})\n",
    "    sub_df.to_csv(sub_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = reg.predict(X_test) # 10 ** 3\n",
    "create_sub(y_pred, \"first_sub\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_coeffs(model, features=None, abv=False):\n",
    "    if features is None:\n",
    "        features = df.drop('y', axis=1).columns\n",
    "    model_coeff = model.coef_\n",
    "    feat_coeff = dict(zip(features,  model_coeff))\n",
    "    if abv:\n",
    "        feat_coeff = dict(zip(features, [abs(coeff) for coeff in model_coeff]))\n",
    "\n",
    "    feat_coeff = dict(sorted(feat_coeff.items(), key=lambda x:x[1]))\n",
    "    return feat_coeff\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_coeffs(reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's consider regularized Linear regression\n",
    "# starting with the Lesso model\n",
    "from sklearn.linear_model import Lasso, LassoLarsCV, LassoCV\n",
    "\n",
    "l1Reg = LassoLarsCV(cv=kf)\n",
    "l1Reg.fit(X_train, y_train)\n",
    "y_pred = l1Reg.predict(X_test)\n",
    "\n",
    "results_lasso = cross_val_score(l1Reg, X_train, y_train, cv=kf)\n",
    "create_sub(y_pred, 'lasso.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(l1Reg.alpha_)\n",
    "features_coeffs(l1Reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The differences in coefficients are a source of experimentation\n",
    "it seems natural to drop \"baths\", \"config\", \"shape\" and \"exterior\".  \n",
    "it might be worthwhile imporving each of  \"decade\", \"zone\", \"elec\", \"heat\" and \"ca\"  \n",
    "The confusing part is that slope is assigned a negative value as if it correlates negatively with the price\n",
    "let's reconsider these features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's consider the original column\n",
    "print(df_train_org[\"LandSlope\"].value_counts())\n",
    "print(len(df_train_org[df_train_org[\"LandSlope\"] == 'Gtl']) / len(df))\n",
    "# the slop feature might indeed be misleading\n",
    "df.drop('slope', axis=1, inplace=True)\n",
    "df_test.drop('slope', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's consider a feature related to the year\n",
    "hy = \"YearBuilt\"\n",
    "hyr = \"YearRemodAdd\"\n",
    "df[hy] = df_train_org[hy]\n",
    "df[hyr] = df_train_org[hyr]\n",
    "df_test[hy] = df_test_org[hy]\n",
    "df_test[hyr] = df_test_org[hyr]\n",
    " \n",
    "df['renewal'] = (df[yr] == df[hyr]).astype(int)\n",
    "# let's apply a\n",
    "h_per_year = df[hy].value_counts().sort_index(ascending=True) \n",
    "\n",
    "# x = h_per_year.index\n",
    "# fig, ax = plt.subplots()\n",
    "# ax.plot(x, h_per_year.values)\n",
    "# leg = ax.legend();\n",
    "\n",
    "# print(pd.pivot_table(df, index='renewal', values='y', ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def set_built_decade(row):\n",
    "    row['built_dec'] = (row[hy] - df[hy].min()) // 10\n",
    "    return row\n",
    "df = df.apply(set_built_decade, axis=1)\n",
    "\n",
    "# let's try to work on the age of the house\n",
    "median_year = df[hy].median()\n",
    "print(median_year)\n",
    "df_antique = df[(df[hy] < median_year)]\n",
    "df_antique_renewed = df[(df[hy] < median_year) & (df[hyr] != df[hy])]\n",
    "\n",
    "# print(df_antique_renewed.loc[:, [y, hyr, hy]])\n",
    "\n",
    "# print(df_antique_renewed.loc[:, [y, hyr, hy]].corr())\n",
    "\n",
    "# # print(df_antique_renewed[y].plot(kind='scatter', x=))\n",
    "# print(pd.pivot_table(df_antique, index='built_dec', values=y, aggfunc=agg_func))\n",
    "\n",
    "# let's consider the house's age\n",
    "last_year = df[hyr].max()\n",
    "\n",
    "def set_ages(row):\n",
    "    global last_year\n",
    "    row['age'] = last_year - row[hy]\n",
    "    row['n_age'] = last_year - row[hyr]\n",
    "    return row\n",
    "\n",
    "df = df.apply(set_ages, axis=1)\n",
    "df_test = df_test.apply(set_ages, axis=1)\n",
    "y_age_corr = df.loc[:, [y, 'age', \"n_age\"]].corr()\n",
    "c = np.array([y_age_corr[y]['age'], y_age_corr[y]['n_age']])\n",
    "c = c / np.sum(c)\n",
    "c1, c2 = c\n",
    "\n",
    "def set_age_feat(row):\n",
    "    global c1, c2\n",
    "    row['age_feat'] = c1 * row['age'] + c2 * row['n_age']\n",
    "    return row\n",
    "\n",
    "df = df.apply(set_age_feat, axis=1)\n",
    "df_test = df_test.apply(set_age_feat, axis=1)\n",
    "\n",
    "print(df.loc[:, [y, 'age', 'n_age', 'age_feat']].corr()['y']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's consider the heat elec and ca features\n",
    "# print(df.loc[:, [y, 'ca', 'elec', 'heat']])\n",
    "h = \"Heating\"\n",
    "hq = \"HeatingQC\"\n",
    "df[h] = df_train_org[h]\n",
    "df[hq] = df_train_org[hq]\n",
    "df_test[h] = df_test_org[h]\n",
    "df_test[hq] = df_test_org[hq]\n",
    "\n",
    "# print(pd.pivot_table(df, index=[h, hq], values=y, aggfunc=agg_func))\n",
    "# let's slightly consider the houses with GasA heating system and with quality either Good or TA \n",
    "df_heat_gas = df[(df[h] == 'GasA') & ((df[hq] == 'Gd') | (df[hq] == 'TA'))]\n",
    "df_heat_gas.plot(kind='scatter', x=hq, y=y)\n",
    "# we can see from the plot that neither of those qualities make a difference:thus\n",
    "# the final classification would be as follows: GasAEx, GasA(Gd, TA), GasA(other), other\n",
    "def set_heat_final(row):\n",
    "    value = 0 \n",
    "    if row[h] != 'GasA':\n",
    "        value = 0\n",
    "    elif row[hq] in ['Fa', 'Po']:\n",
    "        value = 1\n",
    "    elif row[hq] in ['TA', 'Gd']:\n",
    "        value = 2\n",
    "    else:\n",
    "        value = 3\n",
    "    row['heat'] = value\n",
    "    return row\n",
    "\n",
    "df = df.apply(set_heat_final, axis=1)\n",
    "df_test = df_test.apply(set_heat_final, axis=1)\n",
    "print(df['elec'].value_counts())\n",
    "print(df[\"ca\"].value_counts())\n",
    "# taking into consideration the imbalanced distribution, it might be worth it to add all three features, heating, elec and ca together\n",
    "\n",
    "def set_utilities(row):\n",
    "    row['utilities'] = row['heat'] + row['ca'] + row['elec']\n",
    "    return row\n",
    "\n",
    "df = df.apply(set_utilities, axis=1)\n",
    "df_test= df_test.apply(set_utilities, axis=1)\n",
    "\n",
    "# df_test = df_test.apply(set_utilities)\n",
    "print(df.loc[:, [y, 'utilities']].corr()) # this feature seems quite promising"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's check our features again\n",
    "print(df.columns)\n",
    "to_drop_train = ['decade', 'elec', 'heat', 'ca', 'YearBuilt', \"YearRemodAdd\", 'renewal', 'built_dec', 'age', 'n_age', 'Heating', 'HeatingQC']\n",
    "df.drop(to_drop_train, axis=1, inplace=True)\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_drop_test = ['decade', 'Heating', 'elec', 'heat', 'ca', 'YearBuilt', 'YearRemodAdd', 'age', 'n_age', 'Heating', 'HeatingQC']\n",
    "print(df_test.columns) \n",
    "df_test.drop(to_drop_test, axis=1, inplace=True)\n",
    "print(df_test.columns) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.columns)\n",
    "print(df_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def X_Y(train_df, test_df, scaler, y='y'):\n",
    "    X_train = train_df.drop(y, axis=1).values\n",
    "    X_test = test_df.values\n",
    "    y_train = df[y].values\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    return (X_train, y_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test = X_Y(df, df_test, scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lasso_model(X_train, y_train, cv=None):\n",
    "    global kf\n",
    "    if cv is None:\n",
    "        cv = kf\n",
    "    l1Reg = LassoLarsCV(cv=kf)\n",
    "    l1Reg.fit(X_train, y_train)\n",
    "    resLasso = cross_val_score(l1Reg, X_train, y_train, cv=kf)\n",
    "    return l1Reg, resLasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1Reg, resl = lasso_model(X_train, y_train)\n",
    "\n",
    "# create_sub(y_pred, 'lasso3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results_lasso)\n",
    "print(features_coeffs(l1Reg, features=df.drop('y', axis=1).columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.copy()\n",
    "dft2 = df_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the l2 regularization, I will keep area_room, zone and utilities as I am quite sure they are relevant\n",
    "df2.drop(['baths', 'area_room', 'zone', 'exterior', 'config', 'shape', 'utilities'],axis=1, inplace=True)\n",
    "dft2.drop(['baths', 'area_room', 'zone', 'exterior', 'config', 'shape', 'utilities'],axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.drop([ 'area_room','zone', 'utilities'],axis=1, inplace=True)\n",
    "# df_test.drop([ 'area_room','zone', 'utilities'],axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test = X_Y(df2, dft2, scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeCV\n",
    "def ridge_model(X_train, y_train, a=None, cv=None):\n",
    "    global kf\n",
    "    if cv is None: \n",
    "        cv = kf\n",
    "    if a is None:\n",
    "        a = 10 ** np.linspace(0, 2.5, 200)\n",
    "    # print(a)\n",
    "    l2Reg = RidgeCV(alphas=a, scoring='r2', cv=kf)\n",
    "    l2Reg.fit(X_train, y_train)\n",
    "    resRidge = cross_val_score(l2Reg ,X_train, y_train, cv=kf)\n",
    "    return l2Reg, resRidge\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2Reg, resRidge = ridge_model(X_train, y_train)\n",
    "print(features_coeffs(l2Reg, df2.drop('y', axis=1).columns))\n",
    "y_pred = l2Reg.predict(X_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(resRidge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create_sub(y_pred, 'ridge3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model_details(model, features, model_name, file_name, ):\n",
    "     with open(file_name, 'w') as f:\n",
    "        f.write(model_name + \": \\n\")\n",
    "        f.write(\"hyperparamaters: \\n\")\n",
    "      \n",
    "        f.write(str(model.get_params()))\n",
    "        f.write(\"features: \\n\")\n",
    "        f.write(str(features)) \n",
    "        # print(\"bro the model passed is not part of the sklearn library\") \n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the lasso model as well as the ridge one\n",
    "import os\n",
    "# ridge_file = os.path.join(\"models\", \"ridge: 0.18092 .txt\")\n",
    "# lasso_file = os.path.join(\"models\", \"lasso: 0.18431 .txt\")\n",
    "# save_model_details(l2Reg, df2.drop(y, axis=1).columns ,\"Ridge\", ridge_file)\n",
    "# save_model_details(l1Reg, df2.drop(y, axis=1).columns, \"Lasso\", lasso_file)\n",
    "# print(l2Reg.get_params())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Feature selection\n",
    "The number of features is quite high which might significantly affect the performance as many of them might be redundant: does not provide any predective utility, or even mislead the model. The feature selection phase is quite important for our current problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First approach: convert each column to numerical column and perform a general correlation calculations.\n",
    "One fast way to have a better understanding of certain features importances is to compute the correlations between the price and every single feature. Considering a minimum trehshold of absolute correlation value, we can drop even more (statistically) uncorrelated features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.copy()\n",
    "dft2 = df_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlations = df2.corr().loc[[\"y\"],:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlations = correlations.transpose().squeeze().sort_values(ascending=False)\n",
    "correlations_abs = correlations.transpose().squeeze().apply(abs).sort_values(ascending=False)\n",
    "print(correlations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# based on the correlations here, an optimal set of features woul exclude\n",
    "# shape, e_noise, config, zone, roof, exterior\n",
    "df2.drop([\"shape\", \"e_noise\", \"config\", 'zone', \"roof\", \"exterior\"], axis=1, inplace=True)\n",
    "dft2.drop([\"shape\", \"e_noise\", \"config\", 'zone', \"roof\", \"exterior\"], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's experiment with these features\n",
    "X_train, y_train, X_test = X_Y(df2, dft2,scaler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2Reg, l2res = ridge_model(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(l2Reg.alpha_)\n",
    "print(l2res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "l1Reg, l1res = lasso_model(X_train, y_train)\n",
    "print(l1res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for l1, l2 in zip(l1res, l2res):\n",
    "    print(\"l1: \" + str(l1) + \",  l2: \" + str(l2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_l2 = l2Reg.predict(X_test)\n",
    "create_sub(y_pred_l2, \"ridge4.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(features_coeffs(l1Reg, df2.drop('y', axis=1).columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_l1 = l1Reg.predict(X_test)\n",
    "create_sub(y_pred, 'lasso4.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lasso_file = os.path.join(\"models\", \"lasso: 18.092 .txt\")\n",
    "# save_model_details(l1Reg, df2.drop(y, axis=1).columns, \"LassoLarCV\", lasso_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2nd Approach: SELECT_K_BEST Functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's use another feature selection technique\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_regression\n",
    "\n",
    "# get the original dataframes\n",
    "df2 = df.copy().drop(\"y\", axis=1)\n",
    "yk = df['y']\n",
    "dft2 = df_test.copy()\n",
    "\n",
    "selector = SelectKBest(f_regression, k=12)\n",
    "X_k = selector.fit_transform(df2, yk) # dfk is currently a np.array with only values\n",
    "filter = selector.get_support()\n",
    "Xtk = dft2.loc[: , filter].values\n",
    "# print(X_k.shape)\n",
    "# print(yk.shape)\n",
    "# print(Xtk.shape)\n",
    "l2regk, l2res= ridge_model(X_k, yk)\n",
    "\n",
    "# print(X_k)\n",
    "# print(X_train)\n",
    "features = df2.loc[:, filter].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## save the dataframes to actual csv files\n",
    "df2.loc[:, filter].to_csv(\"train_k.csv\", index=False)\n",
    "dft2.loc[:, filter].to_csv(\"test_k.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(l2res)\n",
    "l2reg_k_sub = os.path.join(\"subs\", \"l2regk.csv\")\n",
    "create_sub(l2regk.predict(Xtk), l2reg_k_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1regk, l1res = lasso_model(X_k, yk)\n",
    "print(l1res)\n",
    "l1reg_k_sub = os.path.join(\"subs\", \"l1regk.csv\")\n",
    "create_sub(l1regk.predict(Xtk), l1reg_k_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the best linear model so far\n",
    "# lasso_file = os.path.join(\"models\", \"LassoCV: 0.17884 .txt\")\n",
    "# features = df_test.loc[:, filter].columns\n",
    "\n",
    "# save_model_details(l1regk, features, \"lasso classifier\", lasso_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine \n",
    "It is time to consider more advanced models such as Support Vector machine. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "def hypertune_models(model, para_names:list, init_values:list, expo:list=None, sample_per_iter:int=25, max_iter:int=4, t_scalar:int=0.5, t_expo:int=0.0625):\n",
    "    if (expo is None): # the default is to use  non-exponential ranges for all hyperparameters\n",
    "        expo = [False for _ in init_values]\n",
    "    \n",
    "    best_models = [] # variable to store the intermediate best models\n",
    "    values = np.array([np.array(v) for v in init_values]) # convert the init_values parameter into a numpy array of numpy arrays\n",
    "    values = np.array([10 ** v for (v, e) in zip(values, expo) if e]) # convert the values to exponential range if the corresponding boolean flag is set to True\n",
    "\n",
    "    params = dict(zip(para_names, values)) # create the dictionary to be used for the\n",
    "\n",
    "    b_model = GridSearchCV(model, param_grid=params, n_jobs=-1) # find the best combination of hyperparameters\n",
    "\n",
    "    best_models.append(b_model)\n",
    "\n",
    "    iters = 1 \n",
    "    threshold_cond = True\n",
    "    \n",
    "    def check_range(v, e):\n",
    "        if v.dtype == 'object':\n",
    "            return True\n",
    "        elif e:\n",
    "            return (10 ** v[-1] - 10 ** v[0]) >= np.exp(t_expo)\n",
    "        else:\n",
    "            return (v[-1] - v[0]) >= t_scalar\n",
    "\n",
    "    def new_range(v, best_value, e):\n",
    "        if v.dtype == 'object':\n",
    "            return v\n",
    "        if e :\n",
    "            range = (10 ** v[-1] - 10 ** v[0]) / 2\n",
    "            return 10 ** np.linspace(best_value - range / 2, best_value + range / 2, sample_per_iter + 1)\n",
    "        else:\n",
    "            range = (v[-1] - v[0]) / 2\n",
    "            return np.linspace(best_value - range / 2, best_value + range / 2, sample_per_iter + 1)\n",
    "\n",
    "\n",
    "    while (iters < max_iter and threshold_cond):\n",
    "        # keep tuning as long as at least one parameter's range is still above the treshhold\n",
    "        threshold_cond = any([check_range(v, e)  for (v, e) in zip(values, expo)]) \n",
    "\n",
    "        b_para = b_model.estimator.get_params()\n",
    "        best_values = np.array([k for (k, v) in b_para.items() if k in para_names]) \n",
    "        values = np.array([new_range(v, best_value, e) for (v, best_value, e) in zip(values, best_values, expo)])\n",
    "        params = dict(zip(para_names, values))\n",
    "        b_model = GridSearchCV(model, para_grid=params)\n",
    "        iters += 1\n",
    "        best_models.append(b_model)\n",
    "    \n",
    "    return best_models\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sklearn.metrics.get_scorer_names())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "svr = SVR()\n",
    "scoring = \"neg_root_mean_squared_error\"\n",
    "\n",
    "params = {\"kernel\": [\"rbf\"], \"C\":10 ** np.linspace(0, 2, 26), \"epsilon\": 10 ** np.linspace(-3, 0, 26)}\n",
    "svr_gs = GridSearchCV(SVR(epsilon = 0.01), params, cv = kf, scoring=scoring)\n",
    "\n",
    "\n",
    "svr = GridSearchCV(svr, params)\n",
    "print(cross_val_score(svr, X_k, yk, cv=kf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svr.fit(X_k, yk)\n",
    "svr_predict = svr.predict(Xtk)\n",
    "svr_sub = os.path.join(\"subs\", \"svr1.csv\")\n",
    "create_sub(svr_predict, svr_sub)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tree-Based Models\n",
    "It is time to explore the famous tree-based models:\n",
    "* DecisionTreeRegressor\n",
    "* RandomForestRegressor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "\n",
    "random_state = 3\n",
    "dreg = DecisionTreeRegressor(max_depth=3, random_state=random_state)\n",
    "dreg.fit(X_k, yk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tree_struct(dt, features_names=None):\n",
    "\n",
    "    n_nodes = dt.tree_.node_count\n",
    "    children_left = dt.tree_.children_left\n",
    "    children_right = dt.tree_.children_right\n",
    "    feature = dt.tree_.feature\n",
    "    threshold = dt.tree_.threshold\n",
    "\n",
    "    node_depth = np.zeros(shape=n_nodes, dtype=np.int64)\n",
    "    is_leaves = np.zeros(shape=n_nodes, dtype=bool)\n",
    "    stack = [(0, 0)]  # start with the root node id (0) and its depth (0)\n",
    "    while len(stack) > 0:\n",
    "        # `pop` ensures each node is only visited once\n",
    "        node_id, depth = stack.pop()\n",
    "        node_depth[node_id] = depth\n",
    "\n",
    "        # If the left and right child of a node is not the same we have a split\n",
    "        # node\n",
    "        is_split_node = children_left[node_id] != children_right[node_id]\n",
    "        # If a split node, append left and right children and depth to `stack`\n",
    "        # so we can loop through them\n",
    "        if is_split_node:\n",
    "            stack.append((children_left[node_id], depth + 1))\n",
    "            stack.append((children_right[node_id], depth + 1))\n",
    "        else:\n",
    "            is_leaves[node_id] = True\n",
    "\n",
    "    print(\n",
    "        \"The binary tree structure has {n} nodes and has \"\n",
    "        \"the following tree structure:\\n\".format(n=n_nodes)\n",
    "    )\n",
    "    for i in range(n_nodes):\n",
    "        if is_leaves[i]:\n",
    "            print(\n",
    "                \"{space}node={node} is a leaf node.\".format(\n",
    "                    space=node_depth[i] * \"\\t\", node=i\n",
    "                )\n",
    "            )\n",
    "        else:\n",
    "            print(\n",
    "                \"{space}node={node} is a split node: \"\n",
    "                \"go to node {left} if X[:, {feature}] <= {threshold} \"\n",
    "                \"else to node {right}.\".format(\n",
    "                    space=node_depth[i] * \"\\t\",\n",
    "                    node=i,\n",
    "                    left=children_left[i],\n",
    "                    feature=feature[i] if features_names is None else features_names[feature[i]],\n",
    "                    threshold=threshold[i],\n",
    "                    right=children_right[i],\n",
    "                )\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_struct(dreg, features_names=features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_imp = dreg.feature_importances_\n",
    "print(dict(zip(features, f_imp)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it might be worth experimenting with depth = 4\n",
    "dReg4 = DecisionTreeRegressor(max_depth=4, random_state=random_state)\n",
    "dReg4.fit(X_k, yk)\n",
    "tree_struct(dReg4, features_names=features)\n",
    "f_imp = dReg4.feature_importances_\n",
    "print(dict(zip(features, f_imp)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_params = {\"max_depth\": [3,4,5,6], \"min_samples_leaf\": range(5, 11), \"max_features\":[\"log2\", \"sqrt\"]}\n",
    "\n",
    "dtr = DecisionTreeRegressor(random_state=random_state)\n",
    "best_dReg = GridSearchCV(dtr,  param_grid=tree_params, cv=kf, scoring=scoring, refit=True, n_jobs=-1)\n",
    "best_dReg.fit(X_k, yk)\n",
    "best_dReg = best_dReg.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cross_val_score(best_dReg, X_k, yk, cv=kf, scoring=scoring))\n",
    "tree_struct(best_dReg, features)\n",
    "f_imp = dReg4.feature_importances_\n",
    "print(dict(zip(features, f_imp)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Boosting Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor as gbr\n",
    "\n",
    "# the max depth will be reduced to 4\n",
    "# the rest of the parameters will be set to the same values in bestDReg.\n",
    "\n",
    "gradb = gbr(max_depth=4, min_samples_leaf=9, random_state=random_state)\n",
    "\n",
    "lr = [0.0001, 0.001, 0.01, 0.2, 0.3]\n",
    "n_est = 100  * np.array(range(1, 6))\n",
    "gradbest = GridSearchCV(gradb, {\"learning_rate\": lr, \"n_estimators\": n_est}, cv=kf, scoring=scoring)\n",
    "\n",
    "gradbest.fit(X_k, yk)\n",
    "\n",
    "means = gradbest.cv_results_['mean_test_score']\n",
    "stds = gradbest.cv_results_['std_test_score']\n",
    "params = gradbest.cv_results_['params']\n",
    "\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "\tprint(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "lr_n_est = [i * j for i in lr for j in n_est]\n",
    "\n",
    "\n",
    "# plot\n",
    "plt.errorbar(lr_n_est, means, yerr=stds)\n",
    "plt.title(\"XGBoost learning_rate vs Log Loss\")\n",
    "plt.xlabel('learning_rate')\n",
    "plt.ylabel('loss')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_p_dr = best_dReg.predict(Xtk)\n",
    "create_sub(y_p_dr, \"dtr.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's try to set a randomforest model for us\n",
    "from sklearn.ensemble import RandomForestRegressor as rfr\n",
    "max_d = [3, 4, 5]\n",
    "max_features = [\"sqrt\", \"log2\"]\n",
    "n_estimators = np.linspace(100, 500, 26).astype(int)\n",
    "\n",
    "rf = rfr(min_samples_leaf=9, random_state=random_state, n_jobs=-1)\n",
    "rf_params = {\"max_depth\":max_d, \"n_estimators\": n_estimators, \"max_features\": max_features}\n",
    "rf_grid = GridSearchCV(rf, rf_params, cv=kf, scoring=scoring)\n",
    "rf_grid.fit(X_k, yk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cat = df2[NUM_COLS]\n",
    "df_cat_some_null = df_cat.isna().any()\n",
    "print(df_cat_some_null[df_cat_some_null == True])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider the 3 features found above:\n",
    "* LotFrontage: the area of the street leading to the property\n",
    "* MasVnrArea : Masonry veneer area in square feet\n",
    "* GarageYrBlt: Year garage was built\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Imputing LotFrontage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a reasonale assumption is correlating the LotFrontage to the zone classification as well as the the street type\n",
    "lf = \"LotFrontage\"\n",
    "msz = \"MSZoning\"\n",
    "s = \"Street\"\n",
    "lot_frontage_cols = [lf, msz, s]\n",
    "df_lot_frontage = df2.loc[:, lot_frontage_cols]\n",
    "lot_frontage_by_zone_street = pd.pivot_table(df_lot_frontage, index=[msz, s], values= lf, aggfunc=['count', 'mean'])\n",
    "\n",
    "print(lot_frontage_by_zone_street)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imput_lot_frontage(row):\n",
    "    if(np.isnan(row[lf])):\n",
    "        row[lf] = float(lot_frontage_by_zone_street.loc[(row[msz], row[s]), 'mean'])\n",
    "    return row\n",
    "\n",
    "df2 = df2.apply(lambda row: imput_lot_frontage(row), axis=1)\n",
    "print(df2[lf])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Imputing the MasVnrArea feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# consider the MasVnrArea feature\n",
    "mnva = \"MasVnrArea\"\n",
    "mnv = \"MasVnrType\"\n",
    "print(df2[mnv].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2[mnva] = df2[mnva].fillna(0)\n",
    "df2[mnv] = df2[mnv].fillna(\"None\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Imputing the GarageBuiltYear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# consider the garage building's year\n",
    "grg = \"GarageType\"\n",
    "grg_year = \"GarageYrBlt\"\n",
    "no_garage = df2[df2[grg].isna()].index\n",
    "no_garage_year = df2[df2[grg_year].isna()].index\n",
    "# print(no_garage == no_garage_year)\n",
    "df2[grg_year] = df2[grg_year].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying the Peason's correlation test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the first test: better understanding correlations between features and sales price.\n",
    "# let's consider more in-depth features selections processes\n",
    "# we will use both Pearsons correlation and Spearmans rank coefficients for numerical features\n",
    "\n",
    "num_correlations = [np.corrcoef(df2[col].values, df2[\"y\"].values)[0][1] for col in NUM_COLS]\n",
    "num_correlations = dict(zip(NUM_COLS, num_correlations))\n",
    "\n",
    "num_corr_series = pd.Series(num_correlations).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(num_corr_series)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying the Spearman's correlation test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's repeat the same process but with Spearman's correlation rank\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "num_corr_2 = [spearmanr(df2[col].values, df2[\"y\"].values)[0] for col in NUM_COLS]\n",
    "num_corr_2_p = [spearmanr(df2[col].values, df2[\"y\"].values)[1] for col in NUM_COLS]\n",
    "num_corr_2_series = pd.Series(dict(zip(NUM_COLS, num_corr_2)))\n",
    "num_corr_2_p_series = pd.Series(dict(zip(NUM_COLS, num_corr_2_p)))\n",
    "# print(num_corr_2_series)\n",
    "# print(num_corr_2_p_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_corr_df = pd.DataFrame([num_corr_2, num_corr_2_p], index=['corr', 'p']).transpose()\n",
    "# consider the columns that are statistically correlated with the sale price\n",
    "num_corr_df = num_corr_df[(abs(num_corr_df['corr']) >= .1) & (num_corr_df['p'] < 0.05)] \n",
    "\n",
    "NUM_COLS = [NUM_COLS[i] for i in range(len(NUM_COLS)) if i in list(num_corr_df.index)]\n",
    "NUM_COLS_LEFT = [NUM_COLS[i] for i in range(len(NUM_COLS)) if i not in list(num_corr_df.index)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying the Kendalltau rank coefficient on categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import kendalltau\n",
    "\n",
    "cat_corr_2 = [kendalltau(df2[col].values, df2[\"y\"].values)[0] for col in CAT_COLS]\n",
    "cat_corr_2_p = [kendalltau(df2[col].values, df2[\"y\"].values)[1] for col in CAT_COLS]\n",
    "cat_corr_2_series = pd.Series(dict(zip(CAT_COLS, cat_corr_2)))\n",
    "cat_corr_2_p_series = pd.Series(dict(zip(CAT_COLS, cat_corr_2_p)))\n",
    "# print(num_corr_2_series)\n",
    "# print(num_corr_2_p_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_corr_df = pd.DataFrame([cat_corr_2, num_corr_2_p], index=['corr', 'p']).transpose()\n",
    "# consider the columns that are statistically correlated with the sale price\n",
    "cat_corr_df = cat_corr_df[(abs(cat_corr_df['corr']) >= .1) & (cat_corr_df['p'] < 0.05)] \n",
    "\n",
    "CAT_COLS = [CAT_COLS[i] for i in range(len(CAT_COLS)) if i in list(cat_corr_df.index)]\n",
    "CAT_COLS_LEFT = [CAT_COLS[i] for i in range(len(CAT_COLS)) if i not in list(cat_corr_df.index)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Consider poorly correlated features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(NUM_COLS_LEFT)\n",
    "print(CAT_COLS_LEFT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After consuling the file description, it can be seen that certain features impose certain order on the different categories: \n",
    "* Utilities: the number of services available in the neighborhood\n",
    "* LandSlope: the slope of the land\n",
    "* ExterQual: the quality of exterior material \n",
    "* ExterCond: quality of the exterior material\n",
    "* BsmtQual: basement quality \n",
    "* BsmtCond: basement condition\n",
    "* BsmtExposure: exposure\n",
    "* BsmtFinType1: the quality of the basement's finished area\n",
    "* BsmtFinType2: the quality of the basement's additional areas\n",
    "* HeatingQC: Heating quality and condition\n",
    "* Electrical: Electrical system\n",
    "* KitchenQual: Kitchen quality\n",
    "* Functional: Home functionality\n",
    "* FireplaceQu: Fireplace quality\n",
    "* GarageFinish: Interior finish of the garage\n",
    "* GarageQual: Garage quality\n",
    "* GarageCond: Garage condition\n",
    "* PavedDrive: Paved driveway\n",
    "* PoolQC: Pool quality: dropped because of nan-value ratio\n",
    "* Fence: Fence quality: dropped because of nan-value ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ORD_COL = [\"Utilities\", \"LandSlope\", \"ExterQual\", \"ExterCond\", \"BsmtQual\",\n",
    "\"BsmtCond\",\"BsmtExposure\",\"BsmtFinType1\", \"BsmtFinType2\",\"HeatingQC\",\"Electrical\",\"KitchenQual\",\n",
    "\"Functional\",\"FireplaceQu\",\"GarageFinish\",\"GarageQual\", \"GarageCond\",\"PavedDrive\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('ds_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "006414dea9a04848ce797b510a25f3f28ac8668e3d3244e777242cca6bed477f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
