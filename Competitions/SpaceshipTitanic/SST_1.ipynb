{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oKu1ElvSz8DW"
      },
      "source": [
        "This is my first attempt to solve the Spaceship Titanic Data science competition on the data science platform Kaggle."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5iZCUBxz8Dc"
      },
      "source": [
        "# Preliminary: imports and load the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "bXmqMH3Rz8Dd"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt \n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TMs9vijyz8Df"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mnotebook controller is DISPOSED. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "# variable to determine whether the notebook is running on Collab or not\n",
        "try:\n",
        "  import google.colab\n",
        "  IN_COLAB = True\n",
        "except:\n",
        "  IN_COLAB = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SapEliRjz8Dg"
      },
      "source": [
        "Please make sure to download the data and have it in the same directory as this notebook when running it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WiinLt1ez8Dh",
        "outputId": "cad51e87-cb80-4a01-824b-763461d9c4db"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mnotebook controller is DISPOSED. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "import os\n",
        "from zipfile import ZipFile\n",
        "\n",
        "DOWNLOAD_DIR = \"sample_data\" if IN_COLAB else os.getcwd() \n",
        "file_name = \"spaceship-titanic.zip\"\n",
        "file_name_no_zip = \"spaceship-titanic\"\n",
        "loc = os.path.join(DOWNLOAD_DIR, file_name)\n",
        "# opening the zip file in READ mode\n",
        "\n",
        "if not os.path.isdir(os.path.join(DOWNLOAD_DIR, file_name_no_zip)):\n",
        "    with ZipFile(loc, 'r') as zip_ref:\n",
        "        zip_ref.extractall()\n",
        "        print('Done!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8sQAhGXFz8Dj"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mnotebook controller is DISPOSED. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "train_loc = os.path.join(os.getcwd(), \"train.csv\")\n",
        "test_loc = os.path.join(os.getcwd(), \"test.csv\")\n",
        "\n",
        "df = pd.read_csv(train_loc)\n",
        "df_test = pd.read_csv(test_loc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NJzUOqbKz8Dk"
      },
      "source": [
        "# Helper functions\n",
        "in this subsection I write a number of functions to frequently use later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PSAAfaa9z8Dk"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mnotebook controller is DISPOSED. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "# a function to delete columnsfrom both the train and test datasets\n",
        "def drop_cols(cols, df_train: pd.DataFrame, df_test: pd.DataFrame):\n",
        "    if isinstance(cols, str):\n",
        "        df1 = df_train.drop(cols, axis=1)\n",
        "        df2 = df_test.drop(cols, axis=1)\n",
        "    else:\n",
        "        df1 = df_train.drop(columns=cols)\n",
        "        df2 = df_test.drop(columns=cols)\n",
        "    return df1, df2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aSKPeHn-z8Dl"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mnotebook controller is DISPOSED. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "def new_col_names(old_new_names, df_train ,df_test):\n",
        "    try:    \n",
        "        df = df_train.rename(columns=old_new_names)\n",
        "        df_test = df_test.rename(columns=old_new_names)\n",
        "        return df, df_test\n",
        "    except:\n",
        "        df_no_col = [col for col in old_new_names.keys if col not in df.columns]\n",
        "        df_test_no_col = [col for col in old_new_names.keys if col not in df_test.columns]\n",
        "        print(\"{cols} are not in the {dataf}\".format(df_no_col, \"training dataset\"))\n",
        "        print(\"{cols} are not in the {dataf}\".format(df_test_no_col, \"test dataset\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8KfJIoPwz8Dm"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mnotebook controller is DISPOSED. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "def one_hot_encoder(df: pd.DataFrame, features_names: list=None):\n",
        "    # select all categorical features if none of them were specified\n",
        "    if features_names is None:\n",
        "        features_names = df.select_dtypes(include=['object']).columns.tolist()\n",
        "    # define the encoder\n",
        "    \n",
        "    ohe = OneHotEncoder(drop='first', handle_unknown='error', sparse_output=False, dtype=np.int64)\n",
        "    for f in features_names:        \n",
        "        try:\n",
        "            unique_values = np.unique(df[f])\n",
        "        except:\n",
        "            unique_values = list(df[f].value_counts().index)\n",
        "        \n",
        "        # if there is only one unique value in the column, then this column is to be dropped\n",
        "        if len(unique_values) <= 1 :\n",
        "            break\n",
        "        # the names for the  newly-created features\n",
        "        new_names = [f'{f}_{u}' for u in unique_values[1:]]\n",
        "        new_values = ohe.fit_transform(df[[f]])\n",
        "        print(ohe.categories_)\n",
        "        df[new_names] = new_values\n",
        "    \n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eyp_RZnl1754",
        "outputId": "857785eb-4b61-45f2-bcb9-7c93b493ad51"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mnotebook controller is DISPOSED. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "! pip install empiricaldist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zJOxRCSdz8Dn"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mnotebook controller is DISPOSED. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "from empiricaldist import Cdf\n",
        "\n",
        "def display_cdf(data: pd.DataFrame, num_feat: str, cat_feat: str, feat_values: list=None):\n",
        "    if feat_values is None:\n",
        "        # extract the unique values in the passed feature\n",
        "        feat_values = list(data[cat_feat].value_counts().index)\n",
        "    \n",
        "    # iterate through the different values of feat_values \n",
        "    for v in feat_values:\n",
        "        d = data[data[cat_feat] == v][num_feat]\n",
        "        data_cdf = Cdf.from_seq(d)\n",
        "        data_cdf.plot(label=v)\n",
        "\n",
        "    plt.legend()\n",
        "    plt.xlabel(f'{num_feat}')\n",
        "    plt.ylabel(f'cdf of {num_feat}')\n",
        "    plt.show()   \n",
        "                "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "63c6wYabz8Do"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mnotebook controller is DISPOSED. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "# Create table for missing data analysis\n",
        "def draw_missing_data_table(df):\n",
        "    # find the number of nans in each column\n",
        "    total = df.isnull().sum().sort_values(ascending=False)\n",
        "    # calculate the percentage of missing values in each column\n",
        "    percent = (df.isnull().sum()/df.isnull().count()).sort_values(ascending=False)\n",
        "    # convert the results into a dataframe\n",
        "    missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n",
        "    return missing_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J0CyyKtnz8Do"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mnotebook controller is DISPOSED. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "# let' define a function that applies a function to either \n",
        "# the whole dataframe or certain columns on the dataframe\n",
        "\n",
        "def apply_functions(df_train, df_val, funcs, col_names=None):\n",
        "    # either have one function passed that should be applied to the whole dataframe\n",
        "    # or have an equal number of columns and functions where each funtion will be applied to the corresponding column\n",
        "    all_data = callable(funcs) and col_names is None\n",
        "    col_funcs = True\n",
        "    #  if the funcs argument is indeed a function, then the code below will raise an error \n",
        "    try:\n",
        "        col_funcs = (all([callable(f) for f in funcs]) and len(funcs) == len(col_names))\n",
        "    except:\n",
        "        col_funcs = False\n",
        "    \n",
        "    assert all_data or col_funcs\n",
        "        \n",
        "    if col_names is None: # if the function is to be applied on the whole dataframe\n",
        "        df = df_train.apply(funcs, axis=1)\n",
        "        df_test = df_val.apply(funcs, axis=1)\n",
        "    else:\n",
        "        df = df_train.copy()\n",
        "        df_test = df_val.copy()\n",
        "\n",
        "        for col, f in zip(col_names, funcs):\n",
        "            df[col] = df[col].apply(f)\n",
        "            df_test[col] = df_test[col].apply(f)\n",
        "            \n",
        "    return df, df_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mnotebook controller is DISPOSED. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "def get_col_types(df: pd.DataFrame):\n",
        "    object_type = \"object\"\n",
        "    cat_type = 'category'\n",
        "    non_num_cols = list(df.select_dtypes([object_type, cat_type]).columns)\n",
        "    num_cols = list(df.select_dtypes(np.number).columns)\n",
        "    return num_cols, non_num_cols\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mnotebook controller is DISPOSED. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "def visualize_interaction(data:pd.DataFrame, feature, use_cols=None, nunique_as_discrete=20, max_cat_values=25):\n",
        "    if use_cols is None:\n",
        "        use_cols = data.columns\n",
        "\n",
        "    # first determine which columns are numerical and which are categorical\n",
        "    num_feats, cat_feats = get_col_types(data)\n",
        "    # convert the data into sets\n",
        "    num_feats = set(num_feats)\n",
        "    cat_feats = set(cat_feats)\n",
        "    # consider the discrete features as categorical\n",
        "    for n in num_feats:\n",
        "        if len(data.value_counts(n)) <= nunique_as_discrete:\n",
        "            # consider the numerical feature as categorical\n",
        "            cat_feats.add(n)\n",
        "    # remove every element in num_feats that belongs to cat_feats as well\n",
        "    num_feats = num_feats.difference(cat_feats)\n",
        "\n",
        "    # consider the case where the passed feat is indeed categorical:\n",
        "    if feature in cat_feats:\n",
        "        # iterate through all the columns in the dataset\n",
        "        for col in use_cols:\n",
        "            if col == feature: continue\n",
        "\n",
        "            if col in num_feats:\n",
        "                sns.boxplot(data=data, x=col, y=feature, orient='h')\n",
        "                plt.show()\n",
        "            # only visualize categorical feature with a reasonable number of possible values\n",
        "            elif len(data.value_counts(col)) < max_cat_values:\n",
        "                # the hue parameter should be assigned the feature with the smaller number of possible values\n",
        "                feats = [col, feature]\n",
        "                feats = sorted(feats, key=lambda x: len(data.value_counts(x)))\n",
        "                hue_feat, x_feat = feats\n",
        "\n",
        "                sns.catplot(kind='count', data=data, x=x_feat, hue=hue_feat)\n",
        "                plt.xticks(rotation=45)\n",
        "                plt.show()\n",
        "    else:\n",
        "        for col in use_cols:\n",
        "            if col == feature: continue\n",
        "            \n",
        "            if col in num_feats:\n",
        "                # display the cfds of the numerial feature with different values of the categorical one\n",
        "                display_cdf(data, num_feat=col, cat_feat=feature)\n",
        "                \n",
        "            elif len(data.value_counts(col)) < max_cat_values:                \n",
        "                sns.boxplot(data=data, x=feature, y=col, orient='h')\n",
        "                plt.show()\n",
        "                \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SU8cZ675z8Dp"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mnotebook controller is DISPOSED. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import learning_curve\n",
        "def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n",
        "                        n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):\n",
        "    plt.figure()\n",
        "    plt.title(title)\n",
        "    if ylim is not None:\n",
        "        plt.ylim(*ylim)\n",
        "    plt.xlabel(\"Training examples\")\n",
        "    plt.ylabel(\"Score\")\n",
        "    train_sizes, train_scores, test_scores = learning_curve(\n",
        "        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n",
        "    train_scores_mean = np.mean(train_scores, axis=1)\n",
        "    train_scores_std = np.std(train_scores, axis=1)\n",
        "    test_scores_mean = np.mean(test_scores, axis=1)\n",
        "    test_scores_std = np.std(test_scores, axis=1)\n",
        "    plt.grid()\n",
        "\n",
        "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
        "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
        "                     color=\"r\")\n",
        "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
        "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
        "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
        "             label=\"Training score\")\n",
        "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
        "             label=\"Validation score\")\n",
        "\n",
        "    plt.legend(loc=\"best\")\n",
        "    return plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vmew-Cchz8Dq"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mnotebook controller is DISPOSED. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "# these represent a backup of the original data for later use\n",
        "train_data = pd.read_csv(train_loc)\n",
        "test_data = pd.read_csv(test_loc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dKOXHXDE6vDD"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mnotebook controller is DISPOSED. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "def create_submission(estimator, X_train, y, X_test, estimator_name):\n",
        "    estimator.fit(X_train, y)\n",
        "    y_pred = pd.DataFrame(data=estimator.predict(X_test), columns=['Transported'])\n",
        "    y_pred['PassengerId'] = test_data['PassengerId']\n",
        "    y_pred['Transported'] = y_pred['Transported'].astype(bool)\n",
        "    # y_pred = y_pred.loc[:, ['PassengerId', 'Transported']]\n",
        "    # set the index to the passenger's id\n",
        "    y_pred.set_index(\"PassengerId\", inplace=True)\n",
        "    y_pred.to_csv(os.path.join(os.getcwd(), f\"{estimator_name} submission.csv\")) \n",
        "    return y_pred\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BvKnqpbOz8Dq"
      },
      "source": [
        "# MvP\n",
        "In this section, I will try to build a basic, minimal machine learning pipeline to estimate the baseline performance and later proceed to enrich the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_VzdzYJzz8Dq"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mnotebook controller is DISPOSED. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "df.head()\n",
        "# let's first seperate the predictors from the label\n",
        "# rename the label column to 'y'\n",
        "df = df.rename(columns={\"Transported\":\"y\"})\n",
        "df['y'] = df['y'].astype(int)\n",
        "y = df['y']\n",
        "\n",
        "df, df_test = drop_cols(\"PassengerId\", df, df_test)\n",
        "\n",
        "df, df_test = new_col_names(str.lower, df, df_test)\n",
        "\n",
        "new_names = {\"foodcourt\":\"food\", \"shoppingmall\": \"mall\", \"roomservice\": \"room\",\n",
        "            \"homeplanet\":\"home\", \"destination\": \"des\", \"vrdeck\":\"deck\", \"cryosleep\":\"sleep\"}\n",
        "\n",
        "df, df_test = new_col_names(new_names, df, df_test)\n",
        "\n",
        "df_org = df.copy()\n",
        "df_test_org = df_test.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l7UuMilgz8Dr",
        "outputId": "3abaeab2-dcd4-484d-c655-f25667141c7f"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mnotebook controller is DISPOSED. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "frhMH47nz8Dr",
        "outputId": "9a98eb5b-2714-4696-cdae-fd0f3ebcb8c8"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mnotebook controller is DISPOSED. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "# we can see we have a couple of missing values at each column. Nevertheless with very small portions.\n",
        "# It might be useful to imput them accordingly.\n",
        "# let's see how many rows have at least one nan  value \n",
        "print(df.shape)\n",
        "df_non_nan = df.dropna()\n",
        "df_non_nan.shape\n",
        "# losing more than 2000 rows is indeed not a good idea. Let's try to impute the values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mnotebook controller is DISPOSED. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "num, non_num = get_col_types(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "CPPzKdfMz8Ds",
        "outputId": "95604c98-c691-4582-c5dc-5656a1e8e8a1"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mnotebook controller is DISPOSED. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "# let's try to understand how the different column behaves in general\n",
        "for c in non_num:\n",
        "    if len(df.value_counts(c)) <= 10:\n",
        "        sns.catplot(kind='count', data=df, x=c, hue='y')\n",
        "        plt.show()      "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JUcSs3Z5z8Dt"
      },
      "source": [
        "Remarks:\n",
        "* among the categorial features, the most informative feature seems to be CyroSleep. Sleeping passengers are likely to be teleported \n",
        "* VIP is not really that informative, as for both VIP and non-VIP, the passengers are almost equally distributed \n",
        "* destination and homePlanet as they are currently might not be really informative\n",
        "* maybe some insight could be extracted from cabin and name: at the moment, they will be dropped\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-8WvZk3Bz8Dt"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mnotebook controller is DISPOSED. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "# let's drop the name and cabin features\n",
        "df, df_test = drop_cols(['name', 'cabin'], df, df_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MBfh-5Abz8Dt",
        "outputId": "08133c0a-89bb-4210-d037-79af0091e9f0"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mnotebook controller is DISPOSED. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "# let's consider the distributions of numerical columns\n",
        "print(df.describe()) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dL1-KAJvz8Du"
      },
      "source": [
        "it is easy to see that the maximum values for each of the services in the Titanic SpaceShip are extravagent to say the least.\n",
        "This should be investigated further."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "vokl9ABbz8Du",
        "outputId": "96f43ede-e22d-490a-fa78-e23575f1d691"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mnotebook controller is DISPOSED. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "num_cols, non_num_cols = get_col_types(df)\n",
        "for n in num_cols:\n",
        "    g = df[n].plot(kind='hist')\n",
        "    g.set(xlabel=f\"values of column {n}\", ylabel=\"Frequency\")\n",
        "    g.set_title(f'The distribution of column {n}')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "jEav5EMfz8Du",
        "outputId": "6c1ca52e-b56b-4537-9750-e07b277386cc"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mnotebook controller is DISPOSED. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "# let's see how these features correlate with \"y\"\n",
        "from empiricaldist import Cdf\n",
        "for n in num_cols[:-1] :\n",
        "    d1 = df[df['y'] == 0][n]\n",
        "    d2 = df[df['y'] == 1][n]\n",
        "    cdf1 = Cdf.from_seq(d1)\n",
        "    cdf1.plot(label=\"y = 0\")\n",
        "    \n",
        "    cdf2 = Cdf.from_seq(d2)\n",
        "    cdf2.plot(label=\"y = 1\")\n",
        "    plt.legend()\n",
        "    plt.xlabel(f'{n}')\n",
        "    plt.ylabel(f'cdf of {n}')\n",
        "    plt.show()   \n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UX1iH3D1z8Dv"
      },
      "source": [
        "* We can see that roomservice + vrdeck + spa share very similar distribution between the different classes (transported and not transported): they can be possibly be combined into a single feature\n",
        "* the foodcourt and shoppingmall seem to be similar as well\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "id": "kDnCW_i_z8Dv",
        "outputId": "5bbf1d0e-de72-4adc-990b-41aadb0da9cf"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mnotebook controller is DISPOSED. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "# let's try to understand the percentage of missing values at each column\n",
        "draw_missing_data_table(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RjKhGM-Tz8Dv"
      },
      "source": [
        "only few missing values at each column, using \"mean\" or \"median\" imputing does not seem like a very bad idea for numerical columns\n",
        "as for cyrosleep and vip, it might be worth veryfing a certain assumption\n",
        "1. people in cyrosleep do not spend money at all: \n",
        "2. vip: spend significantly more than the rest of the passengers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bjSRT8tqz8Dw"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mnotebook controller is DISPOSED. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "# first let's add a total_spending column where it calculates the sum of money spent on different services\n",
        "\n",
        "spending_cols = ['deck','room','food','spa', 'mall']\n",
        "def total_spent(row):\n",
        "    spending_cols = ['deck','room','food','spa', 'mall']\n",
        "    row['total'] = row[spending_cols].sum()\n",
        "    return row\n",
        "df, df_test = apply_functions(df, df_test, total_spent)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 541
        },
        "id": "r-syymrsz8Dw",
        "outputId": "64f1627d-39de-4294-98de-61b0b81355f0"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mnotebook controller is DISPOSED. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "# let's verify our hypothesis\n",
        "\n",
        "sns.boxplot(data=df, x='total', y='sleep', orient='h')\n",
        "plt.show()\n",
        "\n",
        "# let's verify the vip hypothesis\n",
        "\n",
        "data = df[(df['total'] > 0)]\n",
        "sns.boxplot(data=data, x='total', y='vip', orient='h')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qledaahyz8Dw"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mnotebook controller is DISPOSED. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "# the first hypothesis is indeed verified: \n",
        "# total == 0 <=> passenger is asleep\n",
        "\n",
        "# the second hypothesis, is relatively in the correct direction, the mean spending of a vip passenger is significantly \n",
        "# larger than the non-vip passenger. it is not possible to determine with high certainty that a high spender is a vip\n",
        "\n",
        "def impute_total_sleep(row):\n",
        "    if row['total'] == 0 or row['sleep']:\n",
        "        for c in spending_cols:\n",
        "            row[c] = 0\n",
        "        row['sleep'] = True\n",
        "        \n",
        "    return row"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GADNbSmvz8Dx",
        "outputId": "b151ebba-9ca9-4e0a-fe70-e88f61466b0e"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mnotebook controller is DISPOSED. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "spending_cols.append('total')\n",
        "df_spent = df[df['total']>0]\n",
        "df_spent_vip = pd.pivot_table(data=df_spent, index=['vip'], values=spending_cols, aggfunc=['median'])\n",
        "print(df_spent_vip)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7I4aKysez8Dx"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mnotebook controller is DISPOSED. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "import math\n",
        "\n",
        "def impute_total_vip(row):\n",
        "    if math.isnan(row['vip']):\n",
        "        # set the rows with 'vip' missing values to either True or False depending on the\n",
        "        # total spending \n",
        "        if row['total'] > df_spent_vip.loc[False, ('median', 'total')]:\n",
        "            row['vip'] = True\n",
        "        else:\n",
        "            row['vip'] = False\n",
        "     \n",
        "    for c in spending_cols:\n",
        "        if math.isnan(row[c]):\n",
        "            row[c] = df_spent_vip.loc[row['vip'], ('median', c)]\n",
        "    return row\n",
        "\n",
        "df, df_test = apply_functions(df, df_test, impute_total_sleep)\n",
        "df, df_test = apply_functions(df, df_test, impute_total_vip)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ltdudWKSz8Dx"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mnotebook controller is DISPOSED. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "# let's use the median for the moment to impute age\n",
        "# and the mode for des and home features\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "mode_imputer = SimpleImputer(strategy='most_frequent')\n",
        "median_imputer = SimpleImputer(strategy='median')\n",
        "\n",
        "# impute des and home features\n",
        "\n",
        "planets=['home', 'des']\n",
        "\n",
        "df [planets] = pd.DataFrame(data=mode_imputer.fit_transform(df[planets]), columns=planets)\n",
        "df [['age']] = pd.DataFrame(data=median_imputer.fit_transform(df[['age']]), columns=['age'])\n",
        "\n",
        "df_test[planets] = pd.DataFrame(data=mode_imputer.transform(df_test[planets]), columns=planets)\n",
        "df_test[['age']] = pd.DataFrame(data=median_imputer.transform(df_test[['age']]), columns=['age'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        },
        "id": "8hmpb6Mcz8Dy",
        "outputId": "02cd38c7-c7aa-45c4-af3f-8b5a8674bc94"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mnotebook controller is DISPOSED. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "# as expected we have no missing values anymore\n",
        "draw_missing_data_table(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qnz3LgjEz8Dy",
        "outputId": "4a79d0f1-c502-440a-e336-b3d3e79733f5"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mnotebook controller is DISPOSED. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "# final step in the baseline pipeline is to encode the categorial features \n",
        "# and convert the boolean ones numerical\n",
        "\n",
        "df = one_hot_encoder(df, planets)\n",
        "df_test = one_hot_encoder(df_test, planets)\n",
        "\n",
        "# remove the original columns\n",
        "df, df_test = drop_cols(planets, df, df_test)\n",
        "\n",
        "def set_bool_int(row):\n",
        "    row['sleep'] = int(row['sleep'])\n",
        "    row['vip'] = int(row['vip'])\n",
        "    return row\n",
        "\n",
        "df, df_test = apply_functions(df, df_test, set_bool_int)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N1faqGGD1Ijm"
      },
      "source": [
        "before proceeding with the evaluation, I am using an external python file with a number of helper functions mainly concerning hyperparameter tuning and model selection. The cell below will download it for you!!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HdXnN8FB1dVW",
        "outputId": "ba8ccd73-bb6a-407c-998a-9e09e47e3ad0"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mnotebook controller is DISPOSED. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "! wget https://raw.githubusercontent.com/ayhem18/My_Kaggle_Repo/space_titanic/Competitions/SpaceshipTitanic/classifiers.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dOHjpLnD5jV1",
        "outputId": "cb570dfd-3a21-4c3d-bc5a-b496b8e39682"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mnotebook controller is DISPOSED. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "# let's remove the 'y' column from the training dataset \n",
        "df.pop('y')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wPGYydwyz8Dy"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mnotebook controller is DISPOSED. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "from classifiers import tune_model\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "lr = LogisticRegression(max_iter=5000)\n",
        "\n",
        "grid= {\"C\":np.logspace(-3, 3, 20)}\n",
        "\n",
        "lr = tune_model(lr, grid, df, y)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "id": "t3YNVbtXz8Dz",
        "outputId": "eb5f629b-97c1-469f-df5b-710d98b9b204"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mnotebook controller is DISPOSED. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "title = \"learning curve: Logistic Regression\"\n",
        "cv = 10\n",
        "plot_learning_curve(lr, title, df, y, cv=cv,n_jobs=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 455
        },
        "id": "zYNl2--Q5LnK",
        "outputId": "6a252a25-8f12-407a-9e64-aa57b6964176"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mnotebook controller is DISPOSED. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "# even though it is clear that Logistic Regression is underfitting the problem\n",
        "# let's test it on the platfor\n",
        "create_submission(lr, df, y, df_test, \"Logistic Regression\")\n",
        "# the baseline submission scored an accuray of 0.78816\n",
        "# let's see how far accurate our model can be!!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# improving further\n",
        "In this section I will try to apply a number of feature engineering techniques, further data cleaning and analysis to boost the model's performance. \n",
        "Among the points to address:\n",
        "* attempt to extract insights from cabin and name columns\n",
        "* address the outliers in the spending columns + vip column\n",
        "* better encoding and impuation for home and des columns\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mnotebook controller is DISPOSED. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "# let's reconsider the original dataset\n",
        "df = df_org.copy()\n",
        "df_test = df_test_org.copy()\n",
        "# let's first add the total column\n",
        "# as well as impute the sleep and the spending columns\n",
        "df, df_test = apply_functions(df, df_test, total_spent)\n",
        "df, df_test = apply_functions(df, df_test, impute_total_sleep)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## does destination or origin matter ?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mnotebook controller is DISPOSED. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "# let's see how 'home' and 'des' features interact with the rest of the features\n",
        "# outside of the spending columns, consider only the \"total\" feature \n",
        "use_cols = [c for c in df.columns if c not in ['mall', 'food', 'room','deck', 'spa']]\n",
        "# let's first consider the 'home' feature\n",
        "visualize_interaction(df, \"home\", use_cols=use_cols)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* Earth has the most passengers\n",
        "* Europa's passengers do not visit PSO\n",
        "* Earth has no vip people\n",
        "* Europe is the richest planet and with 2 thirds of its passengers getting transported"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# after calling \n",
        "# visualize_interaction(df, 'des', use_cols=use_cols) \n",
        "# the 'des' features was deemed non-informative\n",
        "# thus, it seems reasonable to binarize the 'home' feature and drop the 'des' feature\n",
        "\n",
        "# let's calculate q3 for Mars\n",
        "q3_mars = np.quantile(df[df['home'] == 'Mars'], [0.75])\n",
        "\n",
        "\n",
        "# let's binarize the 'home' feature while \n",
        "def new_home(row):\n",
        "    # let's try to impute the value if it is nan:\n",
        "    try:\n",
        "        if math.isnan(row['home']):\n",
        "            # if the destination is 'POS'... then \"home\" is not Europe\n",
        "            try:\n",
        "                if row['des'].lower().startwith('pos') or row['total'] <= q3_mars:\n",
        "                    row['is_Europe'] = False\n",
        "                else: \n",
        "                    row['is_Europe'] = True\n",
        "            except ValueError:\n",
        "                # this block is accessed if row['des'] or row['total'] is nan\n",
        "                # statistically, having the passenger's home as Europe is less likely than other planets\n",
        "                row['is_Europe'] = False   \n",
        "       \n",
        "    except TypeError:\n",
        "        # a TypeError means row['home'] is not Nan\n",
        "        row['is_Europe'] = row['home'] == 'Europe'\n",
        "\n",
        "    return row\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "df, df_test = apply_functions(df, df_test, new_home)\n",
        "# drop the home and des features\n",
        "df, df_test = drop_cols(['des', 'home'], df, df_test)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cabin, Passenger_Id, Name: any insights ? \n",
        "After reading the data description more carefully, it can be extrapolated that cabin, passenger_id and name column are not completely useless. Let's see if some feature engineering could put these columns to use"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# let's first restore the passenger_id column\n",
        "df['id'] = train_data['PassengerId']\n",
        "df_test['id'] = test_data['PassengerId']\n",
        "\n",
        "def group_number(row):\n",
        "    g, n = [int(s) for s in row['id'].strip().split(\"_\")]\n",
        "    row['group'] = g\n",
        "    row['number'] = n\n",
        "    return row\n",
        "\n",
        "df, df_test = apply_functions(df, df_test, group_number)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def split_cabin(row):\n",
        "    try:\n",
        "        d, n, side = row['cabin'].split(\"/\")\n",
        "        row['deck_num'] = d\n",
        "        row['cabin_num'] = n\n",
        "        row['side'] = side\n",
        "    except:\n",
        "        pass\n",
        "    return row\n",
        "\n",
        "def split_name(row):\n",
        "    try:\n",
        "        row['name'] = row['name'].strip().split(\" \")[1].lower()\n",
        "    except:\n",
        "        pass\n",
        "    return row\n",
        "\n",
        "df, df_test = apply_functions(df, df_test, split_cabin)\n",
        "df, df_test = apply_functions(df, df_test, split_name)   \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "new_feats = ['deck_num', 'cabin_num', 'side', 'name', 'group', 'number']\n",
        "for f in new_feats:\n",
        "    u = df[f].value_counts()\n",
        "    if len(u) <= 10:   \n",
        "        print(df[f].value_counts())\n",
        "    else:\n",
        "        print(f\"feature {f} has {len(u)} unique values\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can see that there are many families (more than 2200 unique surname). The feature as it is, is not very informative. However, we can assign a number of family members to each passenger. This value can be approximated with high certainty by the value_counts() function call on the number feature\n",
        "however it can be done definitely using the name feature.   \n",
        "The group, number, cabin_num and name (after preprocessing) features will be discarded \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# let's use the entire dataset for extracting the familities \n",
        "alldata = pd.concat([df.drop('y', axis=1), df_test])\n",
        "group_sizes = alldata.value_counts('group')\n",
        "fam_sizes = alldata.value_counts('name')\n",
        "\n",
        "def set_group_size(row):\n",
        "    try:\n",
        "        row['group'] = group_sizes.loc[row['group']]\n",
        "    except:\n",
        "        pass\n",
        "    return row\n",
        "\n",
        "def set_fam_size(row):\n",
        "    try:\n",
        "        row['fam_size'] = fam_sizes.loc[row['name']]\n",
        "    except:\n",
        "        row['fam_size'] = row['group']\n",
        "    return row \n",
        "\n",
        "\n",
        "# make sure to apply the set_group_size first\n",
        "df, df_test = apply_functions(df, df_test, set_group_size)\n",
        "df, df_test = apply_functions(df, df_test, set_fam_size)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# shit\n",
        "# let's see how the numerical features varies different families\n",
        "# survival rate\n",
        "fam_survival = pd.pivot_table(df, index='name', values=['y'], aggfunc=['mean', 'count'])\n",
        "# print(fam_survival.sort_values(by=('mean', 'y'), ascending=False)) \n",
        "# let's visualize the the distribution of the survival rate among families\n",
        "fam_survival.reset_index()[('mean', 'y')].plot(kind='hist')\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### who survives in a family ?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# shit\\\n",
        "\n",
        "# let's consider which factors determine the transport of family members\n",
        "# age mainly ? \n",
        "all_data_fam_age = alldata.sort_values(by=['name', 'age'])\n",
        "fam_age = pd.pivot_table(alldata, index='name', values=['age'], aggfunc='mean')\n",
        "def set_age_per_fam(row):\n",
        "    global fam_age\n",
        "    try:\n",
        "        row['age_frac'] = row['age'] / fam_age.loc[row['name'], 'age']\n",
        "    except KeyError:\n",
        "        pass\n",
        "    return row\n",
        "print(fam_age)\n",
        "df_c, df_t_c = apply_functions(df, df_test, set_age_per_fam)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df, df_test = apply_functions(df, df_test, set_age_per_fam)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# let's better understand the distribution of this value\n",
        "d2 = df[df['y'] == 1]['age_frac']\n",
        "d1 = df[df['y'] == 0]['age_frac']\n",
        "cdf1 = Cdf.from_seq(d1)\n",
        "cdf1.plot(label=\"y = 0\")\n",
        "\n",
        "cdf2 = Cdf.from_seq(d2)\n",
        "cdf2.plot(label=\"y = 1\")\n",
        "plt.legend()\n",
        "plt.xlabel('age_frac')\n",
        "plt.ylabel('age_frac')\n",
        "plt.show()   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# do not forget to remove the cabin, and idPassenger features\n",
        "df, df_test = drop_cols(['cabin', 'id'], df, df_test)\n",
        "\n",
        "# drop the unncessary features\n",
        "# df, df_test = drop_cols(['name', 'number', 'group', 'cabin_num'], df, df_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# let's consider the distribution of the new features\n",
        "new_feats = ['deck_num', 'side', 'fam_size']\n",
        "for nf in new_feats:\n",
        "    sns.catplot(kind='count', data=df, x=nf, hue='y')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# let's consider the spending per family\n",
        "df['total_per_fam'] = df['total'] / df['fam_size'] \n",
        "df_test['total_per_fam'] = df_test['total'] / df_test['fam_size'] \n",
        "\n",
        "\n",
        "sns.histplot(data=df, x='total_per_fam')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# the new features might have certain interactions with both vip and total features\n",
        "# fam_size\n",
        "sns.relplot(data=df, x='fam_size', y='total', hue='y', size='vip')\n",
        "plt.show()\n",
        "# cabin_num\n",
        "sns.boxplot(data=df, y='deck_num', x='total', orient='h', hue='y')\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## SPENDING OUTLIERS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# let's consider the non-sleeping customers\n",
        "# for c in spending_cols[:-1]:\n",
        "#     g = df[df[c] != 0][c].plot(kind='hist')\n",
        "#     g.set(xlabel=f\"values of column {n}\", ylabel=\"Frequency\")\n",
        "#     g.set_title(f'The distribution of column {n}')\n",
        "#     plt.show()    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# the distribution barely change even after excluding the rows with value zero.\n",
        "# before proceeding with addressing the outliers let's consider the relation between the different spending columns\n",
        "\n",
        "\n",
        "# spending_cols\n",
        "# spending_df = df.loc[:, spending_cols].fillna(0).reset_index(drop=True)\n",
        "# print(spending_df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# let's consider the those who spend at least some money\n",
        "\n",
        "# for c1 in spending_cols[:-1]:\n",
        "#     for c2 in spending_cols[:-1]:\n",
        "#         if c1 != c2:\n",
        "#             sns.relplot(data=df[df['total'] != 0], x=c1, y=c2, kind='scatter')\n",
        "#             plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The main observation is that under a certain trehshold most pair of spending are highly positevely correlated. Nevertheless, as the spending increases, most pair of spending start correlating negatively.\n",
        "Let's consider the the IQR values to detect outliers and consider in more depth their behavior"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# let's define a function to extract the necessary percentiles\n",
        "def outliers (df:pd.DataFrame, spending_col):  \n",
        "    data = df[df[spending_col] > 0]\n",
        "    q1, q3 = np.quantile(np.asarray(data[spending_col]), [0.25, 0.75])\n",
        "    iqr = q3 - q1\n",
        "    min_ = max(0, q1 - 1.5 * iqr)\n",
        "    max_ = q3 + 1.5 * iqr\n",
        "    return min_, max_\n",
        "\n",
        "spending_limits = dict(zip(spending_cols, [outliers(df, c) for c in spending_cols]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "spending_limits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sp_out = 'spending_outlier'\n",
        "def set_spending_outliers(row):\n",
        "    global spending_limits\n",
        "    spending_cols = ['deck','room','food','spa', 'mall', 'total']\n",
        "    row[sp_out] = 0\n",
        "    for c in spending_cols:\n",
        "        limits = spending_limits[c]\n",
        "        if row[c] > limits[1] or row[c] < limits[0]:\n",
        "            row[sp_out] += 1\n",
        "    return row\n",
        "\n",
        "\n",
        "df, df_test = apply_functions(df, df_test, set_spending_outliers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# what is special about these outlier\n",
        "df_outliers = df[df[sp_out] > 0]\n",
        "num, cat = get_col_types(df_outliers)\n",
        "\n",
        "print(df_outliers['y'].value_counts())\n",
        "print(f\"{df_outliers['y'].value_counts()[0] / df_outliers['y'].value_counts().sum()} of the outliers are not transported\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "num, cat = get_col_types(df_outliers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for n in num:\n",
        "    d2 = df_outliers[df_outliers['y'] == 1][n]\n",
        "    d1 = df_outliers[df_outliers['y'] == 0][n]\n",
        "    cdf1 = Cdf.from_seq(d1)\n",
        "    cdf1.plot(label=\"y = 0\")\n",
        "\n",
        "    cdf2 = Cdf.from_seq(d2)\n",
        "    cdf2.plot(label=\"y = 1\")\n",
        "    plt.legend()\n",
        "    plt.xlabel(f'{n}')\n",
        "    plt.ylabel(f'cdf of {n}')\n",
        "    plt.show()   \n",
        "    # as soon as the spending exceeds a certain threshold, the exact amount spent is no longer significant.\n",
        "    # let's understand what makes outliers get transported then"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# let's check how it works for total_per_family\n",
        "\n",
        "\n",
        "# d2 = df_outliers[df_outliers['y'] == 1][\"total_per_fam\"]\n",
        "# d1 = df_outliers[df_outliers['y'] == 0]['total_per_fam']\n",
        "# cdf1 = Cdf.from_seq(d1)\n",
        "# cdf1.plot(label=\"y = 0\")\n",
        "\n",
        "# cdf2 = Cdf.from_seq(d2)\n",
        "# cdf2.plot(label=\"y = 1\")\n",
        "# plt.legend()\n",
        "# plt.xlabel(\"total_per_fam\")\n",
        "# plt.ylabel(\"total_per_fam\")\n",
        "# plt.show()   \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# let's consider the age \n",
        "\n",
        "# sns.boxplot(data=df_outliers, x='age', y='y', orient='h')\n",
        "# plt.show()\n",
        "\n",
        "# sns.boxplot(data=df_outliers, x=sp_out, y='y', orient='h')\n",
        "# plt.show()\n",
        "\n",
        "# d1 = df_outliers[df_outliers['y'] == 0]['fam_size']\n",
        "# d2 = df_outliers[df_outliers['y'] == 1][\"fam_size\"]\n",
        "# cdf1 = Cdf.from_seq(d1)\n",
        "# cdf1.plot(label=\"y = 0\")\n",
        "\n",
        "# cdf2 = Cdf.from_seq(d2)\n",
        "# cdf2.plot(label=\"y = 1\")\n",
        "# plt.legend()\n",
        "# plt.xlabel(f'{n}')\n",
        "# plt.ylabel(f'cdf of fam_size')\n",
        "# plt.show() "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for c in cat:\n",
        "    if len(df_outliers.value_counts(c)) <= 10:\n",
        "        sns.catplot(kind='count', data=df_outliers, x=c, hue='y')\n",
        "        plt.xticks(rotation=45)\n",
        "        plt.show()      "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from scipy.stats import boxcox\n",
        "# for c in spending_cols[:-1]:\n",
        "#     df[c], lamda = boxcox(1 + df[c])\n",
        "#     # use the lambda returned by the first call to transform the test dataset\n",
        "#     df_test[c] = boxcox(1 + df_test[c], lmbda = lamda)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# for n in num_cols:\n",
        "#     g = df[n].plot(kind='hist')\n",
        "#     g.set(xlabel=f\"values of column {n}\", ylabel=\"Frequency\")\n",
        "#     g.set_title(f'The distribution of column {n}')\n",
        "#     plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# as these numerical columns are now consideredvery close to normal,applying the z-score strategy seems like a good idea \n",
        "\n",
        "\n",
        "# housing['LQFSF_Stats'] = stats.zscore(housing['Low Qual Fin SF'])\n",
        "# housing[['Low Qual Fin SF','LQFSF_Stats']].describe().round(3)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "temp_venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "35c660b631c912057dd4c556fea62668d7a2804f8ae5c4333f1772415b9840f9"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
