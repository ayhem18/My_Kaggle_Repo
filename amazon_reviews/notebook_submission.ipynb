{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-17T13:55:21.049919950Z",
     "start_time": "2023-09-17T13:55:20.867618399Z"
    }
   },
   "outputs": [],
   "source": [
    "# let's start with the data and see how it goes\n",
    "import os\n",
    "import pandas as pd\n",
    "HOME = os.getcwd()\n",
    "train_csv = os.path.join(HOME, 'data', 'train.csv')\n",
    "test_csv = os.path.join(HOME, 'data', 'test.csv')\n",
    "\n",
    "df_train = pd.read_csv(train_csv)\n",
    "df_test = pd.read_csv(test_csv)\n",
    "# set the columns names to lower case \n",
    "\n",
    "df_train.columns = [c.lower() for c in df_train.columns]\n",
    "df_test.columns = [c.lower() for c in df_test.columns]\n",
    "\n",
    "# remove unnecessary columns\n",
    "df_train.drop(columns=['helpfulness', 'score'], inplace=True)\n",
    "df_test.drop(columns=['helpfulness', 'score'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-17T13:55:21.093064586Z",
     "start_time": "2023-09-17T13:55:21.092757Z"
    }
   },
   "outputs": [],
   "source": [
    "# add a small piece of code to call the pytorch_modular code\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "current = HOME\n",
    "while 'src' not in os.listdir(current):\n",
    "    current = Path(current).parent\n",
    "\n",
    "sys.path.append(str(current))\n",
    "sys.path.append(os.path.join(current, 'src'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-17T13:55:21.094474458Z",
     "start_time": "2023-09-17T13:55:21.092989816Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Golden Valley Natural Buffalo Jerky</td>\n",
       "      <td>The description and photo on this product need...</td>\n",
       "      <td>grocery gourmet food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Westing Game</td>\n",
       "      <td>This was a great book!!!! It is well thought t...</td>\n",
       "      <td>toys games</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Westing Game</td>\n",
       "      <td>I am a first year teacher, teaching 5th grade....</td>\n",
       "      <td>toys games</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Westing Game</td>\n",
       "      <td>I got the book at my bookfair at school lookin...</td>\n",
       "      <td>toys games</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I SPY A is For Jigsaw Puzzle 63pc</td>\n",
       "      <td>Hi! I'm Martine Redman and I created this puzz...</td>\n",
       "      <td>toys games</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 title  \\\n",
       "0  Golden Valley Natural Buffalo Jerky   \n",
       "1                         Westing Game   \n",
       "2                         Westing Game   \n",
       "3                         Westing Game   \n",
       "4    I SPY A is For Jigsaw Puzzle 63pc   \n",
       "\n",
       "                                                text              category  \n",
       "0  The description and photo on this product need...  grocery gourmet food  \n",
       "1  This was a great book!!!! It is well thought t...            toys games  \n",
       "2  I am a first year teacher, teaching 5th grade....            toys games  \n",
       "3  I got the book at my bookfair at school lookin...            toys games  \n",
       "4  Hi! I'm Martine Redman and I created this puzz...            toys games  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-17T13:55:21.096717811Z",
     "start_time": "2023-09-17T13:55:21.093288265Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>PetSafe Staywell Pet Door with Clear Hard Flap</td>\n",
       "      <td>We've only had it installed about 2 weeks. So ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Kaytee Timothy Cubes, 1-Pound</td>\n",
       "      <td>My bunny had a hard time eating this because t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Body Back Buddy</td>\n",
       "      <td>would never in a million years have guessed th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>SnackMasters California Style Turkey Jerky</td>\n",
       "      <td>Being the jerky fanatic I am, snackmasters han...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Premier Busy Buddy Tug-a-Jug Treat Dispensing ...</td>\n",
       "      <td>Wondered how quick my dog would catch on to th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                              title  \\\n",
       "0   0     PetSafe Staywell Pet Door with Clear Hard Flap   \n",
       "1   1                      Kaytee Timothy Cubes, 1-Pound   \n",
       "2   2                                    Body Back Buddy   \n",
       "3   3         SnackMasters California Style Turkey Jerky   \n",
       "4   4  Premier Busy Buddy Tug-a-Jug Treat Dispensing ...   \n",
       "\n",
       "                                                text  \n",
       "0  We've only had it installed about 2 weeks. So ...  \n",
       "1  My bunny had a hard time eating this because t...  \n",
       "2  would never in a million years have guessed th...  \n",
       "3  Being the jerky fanatic I am, snackmasters han...  \n",
       "4  Wondered how quick my dog would catch on to th...  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-17T13:55:21.096915401Z",
     "start_time": "2023-09-17T13:55:21.093470176Z"
    }
   },
   "outputs": [],
   "source": [
    "import nltk \n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "try:\n",
    "    STOP_WORDS = list(stopwords.words('english'))\n",
    "except LookupError:\n",
    "    nltk.download('stopwords')\n",
    "    STOP_WORDS = list(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-17T13:55:21.097212598Z",
     "start_time": "2023-09-17T13:55:21.093713953Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "See the title of this review. Fortunately, I am a packrat, and kept a bunch of hole repair kits from various blow up things that we have gone through over the years. Does not come with a hole repair kit though, just to warn you. Anyway, it is back in black and bouncing our 3 year old all over the place. Indoor only, I would say. Very highly recommended, in spite of a hole within a week of use. Hope that this is the first and last one... probably not.\n",
      "see title review . fortunately , packrat , kept bunch hole repair kits various blow things gone years . come hole repair kit though , warn . anyway , back black bouncing year old place . indoor , would say . highly recommended , spite hole within week use . hope first last one ... probably .\n"
     ]
    }
   ],
   "source": [
    "# preprocessing functions\n",
    "import re\n",
    "from typing import List\n",
    "\n",
    "def to_lower(text: str) -> str:\n",
    "    return text.lower()\n",
    "\n",
    "def no_extra_spaces(text: str) -> str:\n",
    "    return re.sub('\\s+', ' ', text)\n",
    "\n",
    "def no_extra_chars(text: str) -> str:\n",
    "    return re.sub(r'[^a-zA-Z\\s,!.;:-]+', ' ', text) \n",
    "\n",
    "text = 'aaa5531--==-||\"z2::,.a'\n",
    "\n",
    "def remove_stop_words(text: str,\n",
    "                      tokenizer: TweetTokenizer = None) -> str:\n",
    "    text = to_lower(text)    \n",
    "    tokenizer = TweetTokenizer() if tokenizer is None else tokenizer\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    # if the remove_stop_words argument is set to True, then filter stop words\n",
    "    tokens = [t.strip() for t in tokens if t not in STOP_WORDS] \n",
    "    return \" \".join(tokens)\n",
    "\n",
    "def process(text: str) -> str:\n",
    "    # first lower, remove extrac chracters\n",
    "    text1 = to_lower(no_extra_chars(text))\n",
    "    # remove redundant words\n",
    "    text2 = remove_stop_words(text1)\n",
    "    # remove extra spaces\n",
    "    return no_extra_spaces(text2)\n",
    "\n",
    "import random\n",
    "random.seed(69)\n",
    "example = df_train['text'][int(random.random() * len(df_train))]\n",
    "print(example)\n",
    "print(process(example))\n",
    "\n",
    "# # drop the 'text' column as only the title will be used for classification\n",
    "df_train.drop(columns=['text'], inplace=True)\n",
    "df_test.drop(columns=['text'], inplace=True)\n",
    "\n",
    "# 16 rows have missing values in the 'title' column, remove them\n",
    "df_train.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-17T13:55:21.097504435Z",
     "start_time": "2023-09-17T13:55:21.093860678Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title       0\n",
      "category    0\n",
      "dtype: int64\n",
      "####################################################################################################\n",
      "id       0\n",
      "title    5\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df_train.isna().sum())\n",
    "print(\"#\" * 100)\n",
    "print(df_test.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-17T13:55:28.263882527Z",
     "start_time": "2023-09-17T13:55:21.093991142Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "cat2idx = {\n",
    "    'toys games': 0,\n",
    "    'health personal care': 1,\n",
    "    'beauty': 2,\n",
    "    'baby products': 3,\n",
    "    'pet supplies': 4,\n",
    "    'grocery gourmet food': 5,\n",
    "}\n",
    "\n",
    "idx2cat = {\n",
    "    0:'toys games',\n",
    "    1:'health personal care',\n",
    "    2:'beauty',\n",
    "    3:'baby products',\n",
    "    4:'pet supplies',\n",
    "    5:'grocery gourmet food' \n",
    "}\n",
    "\n",
    "# making sure the dataframes are ready for training\n",
    "def df_process_data(row):\n",
    "    if isinstance(row['title'], float):\n",
    "        row['title'] = random.choice(list(cat2idx.keys()))\n",
    "        return row\n",
    "    row['title'] = process(row['title'])\n",
    "    return row\n",
    "\n",
    "def df_process_labels(row):\n",
    "    row['category'] = process(row['category'])\n",
    "    # map it to an integer\n",
    "    row['category'] = cat2idx[row['category']]\n",
    "    return row\n",
    "\n",
    "# process the fields\n",
    "df_train = df_train.apply(df_process_data, axis=1)\n",
    "# process the labels\n",
    "df_train = df_train.apply(df_process_labels, axis=1)\n",
    "# process the data is the test split\n",
    "df_test = df_test.apply(df_process_data, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-17T13:55:28.278018893Z",
     "start_time": "2023-09-17T13:55:28.265548448Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_data, val_data = train_test_split(df_train, test_size=0.15, stratify=df_train['category'], random_state=69)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-17T13:55:29.050319218Z",
     "start_time": "2023-09-17T13:55:28.277831762Z"
    }
   },
   "outputs": [],
   "source": [
    "# in the rest of the code I will be using the d\n",
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "NOTEBOOK_DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "CHECKPOINT = 'distilbert-base-uncased' # let's keep it simple as for the first iteration\n",
    "MODEL = AutoModel.from_pretrained(CHECKPOINT).to(NOTEBOOK_DEVICE)\n",
    "TOKENIZER = AutoTokenizer.from_pretrained(CHECKPOINT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-17T13:55:29.093210680Z",
     "start_time": "2023-09-17T13:55:29.092817012Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "def collate_function(batch: List[str]):\n",
    "    # batch will represent a list of tuples (text, category) \n",
    "    x, y = [list(row) for row in zip(*batch)]\n",
    "    # convert both labels and data to tensors\n",
    "    y_tensor = torch.FloatTensor(y).to(device=NOTEBOOK_DEVICE)\n",
    "    embeddings = MODEL(**TOKENIZER(x, padding=True, return_tensors='pt').to(NOTEBOOK_DEVICE)).last_hidden_state # make sure to return tensors\n",
    "    return embeddings.to(NOTEBOOK_DEVICE), y_tensor\n",
    "\n",
    "# let's create a dataset object really quick:\n",
    "class LabeledReviewDS(Dataset):\n",
    "    def __init__(self, data: pd.DataFrame) -> None:\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index) -> tuple[str, int]:\n",
    "        return tuple(self.data.iloc[index, :2])\n",
    "\n",
    "# let's set the random seed\n",
    "\n",
    "torch.manual_seed(69)\n",
    "\n",
    "train_ds = LabeledReviewDS(train_data)\n",
    "val_ds = LabeledReviewDS(val_data)\n",
    "\n",
    "# create the dataloaders\n",
    "train_dl = DataLoader(dataset=train_ds, batch_size=32, shuffle=True, collate_fn=collate_function, drop_last=True)\n",
    "val_dl = DataLoader(dataset=val_ds, batch_size=32, shuffle=False, collate_fn=collate_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import random\n",
    "import torch\n",
    "\n",
    "from typing import Union, Dict, Tuple\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "def get_default_device():\n",
    "    return 'cuda' if torch.cuda.is_available() else 'cpu' \n",
    "\n",
    "ACCURACY = 'accuracy'\n",
    "\n",
    "\n",
    "def accuracy(y_pred: torch.tensor, y: torch.tensor) -> float:\n",
    "    # squeeze values if needed\n",
    "    value = (y_pred == y).type(torch.float32).mean().item()\n",
    "    return value\n",
    "\n",
    "def _set_default_parameters(device: str = None,\n",
    "                            metrics: Union[str, Dict[str, callable]] = None\n",
    "                            ) -> Tuple[str, Dict[str, callable]]:\n",
    "    # set default arguments\n",
    "    device = get_default_device() if device is None else device\n",
    "\n",
    "    # a None 'metrics' will map only to accuracy\n",
    "    metrics = {ACCURACY: accuracy} if metrics is None else metrics\n",
    "\n",
    "    return device, metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_LOSS = 'train_loss'\n",
    "VAL_LOSS = 'val_loss'\n",
    "TEST_LOSS = 'test_loss'\n",
    "\n",
    "OPTIMIZER = 'optimizer'\n",
    "SCHEDULER = 'scheduler'\n",
    "OUTPUT_LAYER = 'output_layer'\n",
    "LOSS_FUNCTION = 'loss_function'\n",
    "METRICS = 'metrics'\n",
    "MIN_TRAIN_LOSS = 'min_train_loss'\n",
    "MIN_VAL_LOSS = 'min_val_loss'\n",
    "MAX_EPOCHS = 'max_epochs'\n",
    "DEVICE = 'device'\n",
    "PROGRESS = 'progress'\n",
    "REPORT_EPOCH = 'report_epoch'\n",
    "\n",
    "# the number of epochs to discard before considering the best model\n",
    "MIN_EVALUATION_EPOCH = 'min_evaluation_epoch'\n",
    "\n",
    "# if the model does not reach a lower training loss than the current lowest loss after 'n' consecutive epochs,\n",
    "# the training will stop\n",
    "NO_IMPROVE_STOP = 'no_improve_stop'\n",
    "DEBUG = 'debug'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_per_epoch(model: nn.Module,\n",
    "                    train_dataloader: DataLoader[torch.tensor],\n",
    "                    loss_function: nn.Module,\n",
    "                    optimizer: torch.optim.Optimizer,\n",
    "                    output_layer: Union[nn.Module, callable],\n",
    "                    scheduler: lr_scheduler,\n",
    "                    device: str = None,\n",
    "                    metrics: Union[str, Dict[str, callable]] = None,\n",
    "                    debug: bool = False\n",
    "                    ) -> Dict[str, float]:\n",
    "\n",
    "    # set the default arguments\n",
    "    device, metrics = _set_default_parameters(device, metrics)\n",
    "\n",
    "    # set the model to correct device and state\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "    # set the train loss and metrics\n",
    "    train_loss, train_metrics = 0, dict([(name, 0) for name, _ in metrics.items()])\n",
    "\n",
    "    # make sure to set the `drop_last` field in the dataloader to True,\n",
    "    # as it might affect the metrics\n",
    "    if hasattr(train_dataloader, 'drop_last'):\n",
    "        if not train_dataloader.drop_last:\n",
    "            raise ValueError(f\"Please make sure to set the parameter 'drop_last' in the dataloader\"\n",
    "                             f\"to {True} to avoid any misleading decrease in performance\")\n",
    "\n",
    "    # make sure the train_dataloader shuffles the data\n",
    "    if hasattr(train_dataloader, 'shuffle'):\n",
    "        train_dataloader.shuffle = True\n",
    "\n",
    "    for _, (x, y) in enumerate(train_dataloader):\n",
    "        # depending on the type of the dataset and the dataloader, the labels can be either 1 or 2 dimensional tensors\n",
    "        # the first step is to squeeze them\n",
    "        y = torch.squeeze(y, dim=-1)\n",
    "        # THE LABELS MUST BE SET TO THE LONG DATATYPE\n",
    "        x, y = x.to(device), y.to(torch.long).to(device)\n",
    "        # pass the 1-dimensional label tensor to the loss function. In case the loss function expects 2D tensors, then\n",
    "        # the exception will be caught and the extra dimension will be added\n",
    "        y_pred = model(x)\n",
    "        try:\n",
    "            batch_loss = loss_function(y_pred, y)\n",
    "        except RuntimeError:\n",
    "            # un-squeeze the y\n",
    "            new_y = torch.unsqueeze(y, dim=-1).to(torch.long).to(device)\n",
    "            warnings.warn(\n",
    "                f\"An extra dimension has been added to the labels vectors\"\n",
    "                f\"\\nold shape: {y.shape}, new shape: {new_y.shape}\")\n",
    "            batch_loss = loss_function(y_pred, new_y)\n",
    "\n",
    "        train_loss += batch_loss.item()\n",
    "        optimizer.zero_grad()\n",
    "        batch_loss.backward()\n",
    "        # optimizer's step\n",
    "        optimizer.step()\n",
    "\n",
    "        y_pred_class = output_layer(y_pred)\n",
    "\n",
    "        # calculate the metrics for\n",
    "        for metric_name, metric_func in metrics.items():\n",
    "            train_metrics[metric_name] += metric_func(y_pred_class, y)\n",
    "\n",
    "    # update the learning rate at the end of each epoch\n",
    "    if scheduler is not None:\n",
    "        scheduler.step()\n",
    "\n",
    "    # average the loss and the metrics\n",
    "    # make sure to add the loss before averaging the 'train_loss' variable\n",
    "    train_metrics[TRAIN_LOSS] = train_loss\n",
    "    for metric_name, _ in train_metrics.items():\n",
    "        train_metrics[metric_name] /= len(train_dataloader)\n",
    "\n",
    "    return train_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_per_epoch(model: nn.Module,\n",
    "                  dataloader: DataLoader[torch.tensor],\n",
    "                  loss_function: nn.Module,\n",
    "                  output_layer: Union[nn.Module, callable],\n",
    "                  device: str = None,\n",
    "                  metrics: Union[str, Dict[str, callable]] = None,\n",
    "                  debug: bool = False\n",
    "                  ) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    This function evaluates a given model on a given test split of a dataset\n",
    "    Args:\n",
    "        debug:\n",
    "        model: the given model\n",
    "        dataloader: the loader guaranteeing access to the test split\n",
    "        loss_function: the loss function the model tries to minimize on the test split\n",
    "        output_layer: The model is assumed to output logits, this layer converts them to labels (to compute metrics)\n",
    "        metrics: defaults to only accuracy\n",
    "        device: The device on which the model will run\n",
    "    Returns: A dictionary with loss value on the training data as well as the different given metrics\n",
    "    \"\"\"\n",
    "    # set the default arguments\n",
    "    device, metrics = _set_default_parameters(device, metrics)\n",
    "\n",
    "    val_loss, val_metrics = 0, dict([(name, 0) for name, _ in metrics.items()])\n",
    "\n",
    "    # set the model to the correct device and state\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    # Turn on inference context manager\n",
    "    with torch.inference_mode():\n",
    "        # Loop through DataLoader batches\n",
    "        for _, (x, y) in enumerate(dataloader):\n",
    "            # depending on the type of the dataset and the dataloader, the labels can be either 1 or 2 dimensional tensors\n",
    "            # the first step is to squeeze them\n",
    "            y = torch.squeeze(y, dim=-1)\n",
    "            # THE LABELS MUST BE SET TO THE LONG DATATYPE\n",
    "            x, y = x.to(device), y.to(torch.long).to(device)\n",
    "            y_pred = model(x)\n",
    "            \n",
    "            # calculate the loss, and backprop\n",
    "            try:\n",
    "                loss = loss_function(y_pred, y)\n",
    "            except RuntimeError:\n",
    "                # un-squeeze the y\n",
    "                new_y = torch.unsqueeze(y, dim=-1)\n",
    "                warnings.warn(\n",
    "                    f\"An extra dimension has been added to the labels vectors\"\n",
    "                    f\"\\nold shape: {y.shape}, new shape: {new_y.shape}\")\n",
    "                loss = loss_function(y_pred, new_y.float())\n",
    "\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            predictions = output_layer(y_pred)\n",
    "\n",
    "            for metric_name, metric_func in metrics.items():\n",
    "                val_metrics[metric_name] += metric_func(y, predictions)\n",
    "\n",
    "    # make sure to add the loss without averaging the 'val_loss' variable\n",
    "    val_metrics['val_loss'] = val_loss\n",
    "    for name, metric_value in val_metrics.items():\n",
    "        val_metrics[name] = metric_value / len(dataloader)\n",
    "\n",
    "    return val_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, List, Optional\n",
    "##################################################################################################################\n",
    "# UTILITY TRAINING FUNCTIONS:\n",
    "\n",
    "# let's define a function to validate the passed training configuration\n",
    "def _validate_training_configuration(train_configuration: Dict) -> Dict[str, Any]:\n",
    "    # first step: extract the necessary parameters for the training: optimizer and scheduler\n",
    "    optimizer = train_configuration.get(OPTIMIZER, None)\n",
    "    scheduler = train_configuration.get(SCHEDULER, None)\n",
    "\n",
    "    # set the default multi-class classification loss\n",
    "    train_configuration[LOSS_FUNCTION] = train_configuration.get(LOSS_FUNCTION, nn.CrossEntropyLoss())\n",
    "\n",
    "    # the default output layer: argmax: since only the default loss expects logits: the predictions need hard labels\n",
    "    def default_output(x: torch.Tensor) -> torch.Tensor:\n",
    "        return x.argmax(dim=-1)\n",
    "\n",
    "    train_configuration[OUTPUT_LAYER] = train_configuration.get(OUTPUT_LAYER, default_output)\n",
    "\n",
    "    necessary_training_params = [(OPTIMIZER, optimizer),\n",
    "                                 (SCHEDULER, scheduler)]\n",
    "\n",
    "    # make sure these parameters are indeed passed to the train_model function\n",
    "    for name, tp in enumerate(necessary_training_params):\n",
    "        if tp is None:\n",
    "            raise TypeError(f\"The argument {name} is expected to be passed as non-None to the configuration\\n\"\n",
    "                            f\"Found: {type(tp)}\")\n",
    "\n",
    "    # set the default parameters\n",
    "    train_configuration[METRICS] = train_configuration.get(METRICS, {ACCURACY: accuracy})\n",
    "    train_configuration[MIN_TRAIN_LOSS] = train_configuration.get(MIN_TRAIN_LOSS, None)\n",
    "    train_configuration[MIN_VAL_LOSS] = train_configuration.get(MIN_VAL_LOSS, None)\n",
    "    train_configuration[MAX_EPOCHS] = train_configuration.get(MAX_EPOCHS, 50)\n",
    "    train_configuration[MIN_EVALUATION_EPOCH] = train_configuration.get(MIN_EVALUATION_EPOCH,\n",
    "                                                                           train_configuration[MAX_EPOCHS] // 10)\n",
    "\n",
    "    train_configuration[DEVICE] = train_configuration.get(DEVICE, get_default_device())\n",
    "    train_configuration[PROGRESS] = train_configuration.get(PROGRESS, True)\n",
    "    train_configuration[REPORT_EPOCH] = train_configuration.get(REPORT_EPOCH, None)\n",
    "    # the default value will be set to 5% of the max number of epochs\n",
    "    train_configuration[NO_IMPROVE_STOP] = train_configuration.get(NO_IMPROVE_STOP,\n",
    "                                                                      train_configuration[MAX_EPOCHS] * 0.15)\n",
    "\n",
    "    train_configuration[DEBUG] = train_configuration.get(DEBUG, False)\n",
    "\n",
    "    return train_configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "def _track_performance(performance_dict: Dict[str, List[float]],\n",
    "                       train_loss: float,\n",
    "                       val_loss: float,\n",
    "                       train_metric: Dict[str, float],\n",
    "                       val_metrics: Dict[str, float]) -> None:\n",
    "    # add the losses first\n",
    "    performance_dict[TRAIN_LOSS].append(train_loss)\n",
    "    performance_dict[VAL_LOSS].append(val_loss)\n",
    "\n",
    "    # update train metrics\n",
    "    for metric_name, metric_value in train_metric.items():\n",
    "        performance_dict[f'train_{metric_name}'].append(metric_value)\n",
    "\n",
    "    # update val metrics\n",
    "    for metric_name, metric_value in val_metrics.items():\n",
    "        performance_dict[f'val_{metric_name}'].append(metric_value)\n",
    "\n",
    "\n",
    "def _set_summary_writer(writer: SummaryWriter,\n",
    "                        epoch_train_loss,\n",
    "                        epoch_val_loss,\n",
    "                        epoch_train_metrics,\n",
    "                        epoch_val_metrics,\n",
    "                        epoch) -> None:\n",
    "    # track loss results\n",
    "    writer.add_scalars(main_tag='Loss',\n",
    "                       tag_scalar_dict={\"train_loss\": epoch_train_loss, 'val_loss': epoch_val_loss},\n",
    "                       global_step=epoch)\n",
    "\n",
    "    for name, m in epoch_train_metrics.items():\n",
    "        writer.add_scalars(main_tag=name,\n",
    "                           tag_scalar_dict={f\"train_{name}\": m, f\"val_{name}\": epoch_val_metrics[name]},\n",
    "                           global_step=epoch)\n",
    "\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import re\n",
    "\n",
    "def default_file_name(hour_ok: bool = True,\n",
    "                      minute_ok: bool = True):\n",
    "    # Get timestamp of current date (all experiments on certain day live in same folder)\n",
    "    current_time = datetime.now()\n",
    "    current_hour = current_time.hour\n",
    "    current_minute = current_time.minute\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d\")  # returns current date in YYYY-MM-DD format\n",
    "    # now it is much more detailed: better tracking\n",
    "    timestamp += f\"-{(current_hour if hour_ok else '')}-{current_minute if minute_ok else ''}\"\n",
    "\n",
    "    # make sure to remove any '-' left at the end\n",
    "    timestamp = re.sub(r'-+$', '', timestamp)\n",
    "    return timestamp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def abs_path(path: Union[str, Path]) -> Path:\n",
    "    return Path(path) if os.path.isabs(path) else Path(os.path.join(os.getcwd(), path))\n",
    "\n",
    "def process_save_path(save_path: Union[str, Path, None],\n",
    "                      dir_ok: bool = True,\n",
    "                      file_ok: bool = True,\n",
    "                      condition: callable = None,\n",
    "                      error_message: str = 'error!!') -> Union[str, Path, None]:\n",
    "    if save_path is not None:\n",
    "        # first make the save_path absolute\n",
    "        save_path = abs_path(save_path)\n",
    "        assert not \\\n",
    "            ((not file_ok and os.path.isfile(save_path)) or\n",
    "             (not dir_ok and os.path.isdir(save_path))), \\\n",
    "            f'MAKE SURE NOT TO PASS A {\"directory\" if not dir_ok else \"file\"}'\n",
    "\n",
    "        assert condition is None or condition(save_path), error_message\n",
    "\n",
    "        # create the directory if needed\n",
    "        if not os.path.isfile(save_path):\n",
    "            os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "    return save_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __verify_extension(p):\n",
    "    return os.path.basename(p).endswith('.pt') or os.path.basename(p).endswith('.pth')\n",
    "\n",
    "\n",
    "def save_model(model: nn.Module, path: Union[str, Path] = None) -> None:\n",
    "    # the time of saving the model\n",
    "    now = datetime.now()\n",
    "    file_name = \"-\".join([str(now.month), str(now.day), str(now.hour), str(now.minute)])\n",
    "    # add the extension\n",
    "    file_name += '.pt'\n",
    "\n",
    "    # first check if the path variable is None:\n",
    "    path = path if path is not None else os.path.join(os.getcwd(), file_name)\n",
    "\n",
    "    # process the path\n",
    "    path = process_save_path(path,\n",
    "                             dir_ok=True,\n",
    "                             file_ok=True,\n",
    "                             condition=lambda p: not os.path.isfile(p) or __verify_extension(p),\n",
    "                             error_message='MAKE SURE THE FILE PASSED IS OF THE CORRECT EXTENSION')\n",
    "\n",
    "    if os.path.isdir(path):\n",
    "        path = os.path.join(path, file_name)\n",
    "\n",
    "    # finally save the model.\n",
    "    torch.save(model.state_dict(), path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def create_summary_writer(parent_dir: Union[str, Path],\n",
    "                          experiment_name: str = None,\n",
    "                          model_name: str = None,\n",
    "                          return_path: bool = False) -> Union[SummaryWriter, tuple[SummaryWriter, Path]]:\n",
    "    timestamp = default_file_name()\n",
    "    # process the parent_dir first\n",
    "    parent_dir = process_save_path(parent_dir, file_ok=False, dir_ok=True)\n",
    "\n",
    "    # set the default values\n",
    "    experiment_name = experiment_name if experiment_name is not None else f'experience_{len(os.listdir(parent_dir))}'\n",
    "    exp_dir = os.path.join(parent_dir, experiment_name)\n",
    "\n",
    "    # create the directory if needed\n",
    "    os.makedirs(exp_dir, exist_ok=True)\n",
    "    model_name = model_name if model_name is not None else f'experience_{len(os.listdir(parent_dir))}'\n",
    "\n",
    "    log_dir = os.path.join(parent_dir, experiment_name, model_name, timestamp)\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "    print(f\"[INFO] Created SummaryWriter, saving to: {log_dir}...\")\n",
    "    if return_path:\n",
    "        return SummaryWriter(log_dir=log_dir), Path(log_dir)\n",
    "\n",
    "    return SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "\n",
    "def save_info(save_path: Union[Path, str],\n",
    "              details: dict[str, object],\n",
    "              details_folder: str = 'details'):\n",
    "    save_path = process_save_path(os.path.join(save_path, details_folder), dir_ok=True, file_ok=False)\n",
    "\n",
    "    for name, obj in details.items():\n",
    "        p = os.path.join(save_path, (name + '.pkl'))\n",
    "        with open(p, 'wb') as f:\n",
    "            pickle.dump(obj, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _report_performance(train_loss: float,\n",
    "                        val_loss: float,\n",
    "                        train_metrics: Dict[str, float],\n",
    "                        val_metrics: Dict[str, float]) -> None:\n",
    "    print(\"#\" * 25)\n",
    "    print(f\"training loss: {train_loss}\")\n",
    "\n",
    "    for metric_name, metric_value in train_metrics.items():\n",
    "        print(f\"train_{metric_name}: {metric_value}\")\n",
    "\n",
    "    print(f\"validation loss : {val_loss}\")\n",
    "    for metric_name, metric_value in val_metrics.items():\n",
    "        print(f\"val_{metric_name}: {metric_value}\")\n",
    "    print(\"#\" * 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "# THE MAIN TRAINING FUNCTION:\n",
    "def train_model(model: nn.Module,\n",
    "                train_dataloader: DataLoader[torch.Tensor],\n",
    "                test_dataloader: DataLoader[torch.Tensor],\n",
    "                train_configuration: Dict,\n",
    "                log_dir: Optional[Union[Path, str]] = None,\n",
    "                save_path: Optional[Union[Path, str]] = None,\n",
    "                ):\n",
    "    # set the default parameters\n",
    "    train_configuration = _validate_training_configuration(train_configuration)\n",
    "\n",
    "    save_path = save_path if save_path is not None else log_dir\n",
    "\n",
    "    performance_dict = {TRAIN_LOSS: [],\n",
    "                        VAL_LOSS: []}\n",
    "\n",
    "    metrics = train_configuration[METRICS]\n",
    "\n",
    "    # save 2 copies: val and train for each metric\n",
    "    for name, _ in metrics.items():\n",
    "        performance_dict[f'train_{name}'] = []\n",
    "        performance_dict[f'val_{name}'] = []\n",
    "\n",
    "    # best_model, best_loss = None, None\n",
    "    min_training_loss, no_improve_counter, best_model = float('inf'), 0, None\n",
    "\n",
    "    # in addition to the model save all the details:\n",
    "    # build the details:\n",
    "    details = {OPTIMIZER: train_configuration[OPTIMIZER],\n",
    "               SCHEDULER: train_configuration[SCHEDULER],\n",
    "               MAX_EPOCHS: train_configuration[MAX_EPOCHS],\n",
    "               MIN_TRAIN_LOSS: train_configuration[MIN_TRAIN_LOSS],\n",
    "               MIN_VAL_LOSS: train_configuration[MIN_VAL_LOSS]}\n",
    "\n",
    "    # before proceeding with the training, let's set the summary writer\n",
    "    writer = None if log_dir is None else create_summary_writer(log_dir)\n",
    "\n",
    "    for epoch in tqdm(range(train_configuration[MAX_EPOCHS])):\n",
    "\n",
    "        epoch_train_metrics = train_per_epoch(model=model,\n",
    "                                              train_dataloader=train_dataloader,\n",
    "                                              loss_function=train_configuration[LOSS_FUNCTION],\n",
    "                                              optimizer=train_configuration[OPTIMIZER],\n",
    "                                              output_layer=train_configuration[OUTPUT_LAYER],\n",
    "                                              scheduler=train_configuration[SCHEDULER],\n",
    "                                              device=train_configuration[DEVICE],\n",
    "                                              debug=train_configuration[DEBUG])\n",
    "\n",
    "        epoch_val_metrics = val_per_epoch(model=model, dataloader=test_dataloader,\n",
    "                                          loss_function=train_configuration[LOSS_FUNCTION],\n",
    "                                          output_layer=train_configuration[OUTPUT_LAYER],\n",
    "                                          device=train_configuration[DEVICE],\n",
    "                                          debug=train_configuration[DEBUG])\n",
    "\n",
    "        epoch_train_loss = epoch_train_metrics[TRAIN_LOSS]\n",
    "        del (epoch_train_metrics[TRAIN_LOSS])\n",
    "\n",
    "        epoch_val_loss = epoch_val_metrics[VAL_LOSS]\n",
    "        del (epoch_val_metrics[VAL_LOSS])\n",
    "\n",
    "        no_improve_counter = no_improve_counter + 1 if min_training_loss < epoch_train_loss else 0\n",
    "\n",
    "        if min_training_loss > epoch_train_loss:\n",
    "            # save the model with the lowest training error\n",
    "            min_training_loss = epoch_train_loss\n",
    "            best_model = model\n",
    "\n",
    "        if (train_configuration[REPORT_EPOCH] is not None\n",
    "                and epoch % train_configuration[REPORT_EPOCH] == 0):\n",
    "            _report_performance(epoch_train_loss,\n",
    "                                epoch_val_loss,\n",
    "                                epoch_train_metrics,\n",
    "                                epoch_val_metrics)\n",
    "\n",
    "        # save the model's performance for this epoch\n",
    "        _track_performance(performance_dict=performance_dict,\n",
    "                           train_loss=epoch_train_loss,\n",
    "                           val_loss=epoch_val_loss,\n",
    "                           train_metric=epoch_train_metrics,\n",
    "                           val_metrics=epoch_val_metrics)\n",
    "\n",
    "        _set_summary_writer(writer,\n",
    "                            epoch_train_loss=epoch_train_loss,\n",
    "                            epoch_val_loss=epoch_val_loss,\n",
    "                            epoch_train_metrics=epoch_train_metrics,\n",
    "                            epoch_val_metrics=epoch_val_metrics,\n",
    "                            epoch=epoch\n",
    "                            )\n",
    "\n",
    "        # check if the losses reached the minimum thresholds\n",
    "        if ((train_configuration[MIN_TRAIN_LOSS] is not None and\n",
    "             train_configuration[MIN_TRAIN_LOSS] >= epoch_train_loss) or\n",
    "\n",
    "                (train_configuration[MIN_VAL_LOSS] is not None\n",
    "                 and train_configuration[MIN_VAL_LOSS] >= epoch_val_loss)):\n",
    "            # the first state that reaches lower scores than the specified thresholds\n",
    "            # is consequently the model's best state\n",
    "            break\n",
    "\n",
    "        # abort training if the training loss did not decrease \n",
    "        if no_improve_counter >= train_configuration[NO_IMPROVE_STOP]:\n",
    "            warnings.warn(f\"The training loss did not improve for {no_improve_counter} consecutive epochs.\"\n",
    "                          f\"\\naborting training!!\", category=RuntimeWarning)\n",
    "            break\n",
    "\n",
    "    save_info(save_path=log_dir, details=details)\n",
    "    save_model(best_model, path=save_path)\n",
    "    return performance_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(base_model: nn.Module,\n",
    "               path: Union[str, Path]) -> nn.Module:\n",
    "    # first process the path\n",
    "    path = process_save_path(path,\n",
    "                             dir_ok=False,\n",
    "                             file_ok=True,\n",
    "                             condition=lambda p: not os.path.isfile(p) or __verify_extension(p),\n",
    "                             error_message='MAKE SURE THE FILE PASSED IS OF THE CORRECT EXTENSION')\n",
    "\n",
    "    base_model.load_state_dict(torch.load(path))\n",
    "\n",
    "    return base_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train A model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-17T13:55:29.094325168Z",
     "start_time": "2023-09-17T13:55:29.093361853Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.nn.functional import leaky_relu\n",
    "\n",
    "class SeqClassModel(nn.Module):\n",
    "    def __init__(self, \n",
    "                in_features: int,\n",
    "                hidden_size: int, \n",
    "                num_classes: int, \n",
    "                num_layers: int = 2, \n",
    "                dropout: float=0.25, \n",
    "                *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.output_units = num_classes if num_classes > 2 else 1\n",
    "        self.rnn = nn.LSTM(input_size=in_features, \n",
    "                           hidden_size=hidden_size, \n",
    "                           dropout=dropout, \n",
    "                           num_layers=num_layers,\n",
    "                           bidirectional=True, # bidiretional RNN are more powerful\n",
    "                           batch_first=True # easier manipulation\n",
    "                           )\n",
    "        # 2: comes from the fact that the lstm is bidirectional, the rest is similar to the LSTM documention Pytorch\n",
    "        linear_input_dim = 2 * num_layers * hidden_size \n",
    "        self.batch_layer= nn.BatchNorm1d(num_features=linear_input_dim)\n",
    "        # self.relu_layer = nn.LeakyReLU()\n",
    "        self.head = nn.Linear(in_features=linear_input_dim, out_features=self.output_units)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        # first pass it through the rnn\n",
    "        _, (hidden_state, _) = self.rnn(x)\n",
    "        batch_size = hidden_state.shape[1]\n",
    "        # first permuting channels: batch_size as dimensions '0' \n",
    "        # only only the last lstm layer\n",
    "        hidden_state = hidden_state.permute((1, 0, 2)).reshape((batch_size, -1))\n",
    "        return self.head.forward(self.batch_layer(hidden_state))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-17T13:55:29.094486140Z",
     "start_time": "2023-09-17T13:55:29.093505082Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import LinearLR\n",
    "from torchmetrics.classification import MulticlassF1Score, MulticlassAccuracy\n",
    "\n",
    "base_model = SeqClassModel(in_features=768, hidden_size=128, num_classes=6)\n",
    "optimizer = AdamW(base_model.parameters(), lr=0.01)\n",
    "scheduler = LinearLR(optimizer, start_factor=1.0, end_factor=0.005, total_iters=100)\n",
    "\n",
    "accuracy_metric, f1_metric = MulticlassAccuracy(num_classes=6), MulticlassF1Score(num_classes=6)\n",
    "\n",
    "metrics = {'accuracy': accuracy_metric, 'f1_score': f1_metric}\n",
    "\n",
    "\n",
    "train_configuration = {'optimizer': optimizer,\n",
    "                        'scheduler': scheduler,\n",
    "                        'min_val_loss': 10 ** -4,\n",
    "                        'max_epochs': 100,\n",
    "                        'report_epoch': 5,\n",
    "                        'device': NOTEBOOK_DEVICE, \n",
    "                        'metrics': metrics,\n",
    "                        'no_improve_stop': 30\n",
    "                        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-17T13:55:58.229534101Z",
     "start_time": "2023-09-17T13:55:29.093614286Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Created SummaryWriter, saving to: /home/ayhem18/DEV/My_Kaggle_Repo/amazon_reviews/runs/experience_19/experience_19/2023-09-19-18-21...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 1/2 [00:54<00:54, 54.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#########################\n",
      "training loss: 0.6592031277982753\n",
      "train_accuracy: 0.7730402542372882\n",
      "validation loss : 0.5822132710129657\n",
      "val_accuracy: 0.8122387921556513\n",
      "#########################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [01:48<00:00, 54.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#########################\n",
      "training loss: 0.4895275359913679\n",
      "train_accuracy: 0.832391713747646\n",
      "validation loss : 0.4638859727122682\n",
      "val_accuracy: 0.8458634120352725\n",
      "#########################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# import src.pytorch_modular.image_classification.engine_classification as cls\n",
    "results = train_model(base_model, train_dl, val_dl, train_configuration,    \n",
    "                            log_dir=os.path.join(HOME, 'runs'),         \n",
    "                            save_path=os.path.join(HOME, 'saved_models'))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time to set the inference part of the script\n",
    "_VALID_RETURN_TYPES = ['np', 'pt', 'list']\n",
    "# relatively small test splits (that can fit th memory)\n",
    "from torchvision import transforms as tr\n",
    "\n",
    "\n",
    "class InferenceDirDataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 test_dir: Union[str, Path],\n",
    "                 transformations: tr) -> None:\n",
    "        # the usual base class constructor call\n",
    "        super().__init__()\n",
    "        test_data_path = process_save_path(test_dir, file_ok=False, dir_ok=True)\n",
    "        self.data = [os.path.join(test_data_path, file_name) for file_name in os.listdir(test_data_path)]\n",
    "        self.t = transformations\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index) -> int:\n",
    "        # don't forget to apply the transformation before returning the index-th element in the directory\n",
    "        return self.t(Image.open(self.data[index]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _set_inference_loader(inference_source_data: Union[DataLoader[torch.tensor], Path, str],\n",
    "                          transformations: tr = None) -> DataLoader:\n",
    "    # the input to this function should be validated\n",
    "    if isinstance(inference_source_data, (Path, str)):\n",
    "\n",
    "        warnings.warn(f\"The inference source data was passed as a path to a directory...\"\n",
    "                      f\"\\nBuilding the dataloader\", category=RuntimeWarning)\n",
    "\n",
    "        # make sure the transformations argument is passed\n",
    "        if transformations is None:\n",
    "            raise TypeError(\"The 'transformations' argument must be passed if the data source is a directory\"\n",
    "                            f\"\\nFound: {transformations}\")\n",
    "        ds = InferenceDirDataset(inference_source_data, transformations)\n",
    "        dataloader = DataLoader(ds,\n",
    "                                batch_size=100,\n",
    "                                shuffle=False,  # shuffle false to keep the original order of the test-split samples\n",
    "                                num_workers=os.cpu_count() // 2)\n",
    "        return dataloader\n",
    "\n",
    "    return inference_source_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from torchvision import transforms as tr\n",
    "\n",
    "def inference(model: nn.Module,\n",
    "              inference_source_data: Union[DataLoader, Path, str],\n",
    "              transformation: tr = None,\n",
    "              output_layer: Union[nn.Module, callable] = None,\n",
    "              device: str = None,\n",
    "              return_tensor: str = 'np'\n",
    "              ) -> Union[np.ndarray, torch.tensor, List[int]]:\n",
    "    # first let's make sure our loader is set\n",
    "    loader = _set_inference_loader(inference_source_data,\n",
    "                                   transformation)\n",
    "\n",
    "    device = get_default_device() if device is None else device\n",
    "    # make sure the return_tensor argument is a set to a valid value\n",
    "    if return_tensor not in _VALID_RETURN_TYPES:\n",
    "        raise ValueError(f'the `return_tensor` argument is expected to be among {_VALID_RETURN_TYPES}\\n'\n",
    "                         f'found: {return_tensor}')\n",
    "\n",
    "    def default_output(x: torch.Tensor):\n",
    "        return x.argmax(dim=-1)\n",
    "\n",
    "    # the default output layer is the softmax layer: (reduced to argmax)\n",
    "    output_layer = default_output if output_layer is None else output_layer\n",
    "    # set to the inference mode\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        result = [output_layer(model.forward(X.to(device))) for X in loader]\n",
    "\n",
    "    # now we have a list of pytorch tensors\n",
    "    if return_tensor == 'pt':\n",
    "        res = torch.stack(result)\n",
    "        res = torch.squeeze(res, dim=-1)\n",
    "\n",
    "    else:\n",
    "        # convert res to a list of lists\n",
    "        res = [torch.squeeze(r, dim=-1).cpu().tolist() for r in result]\n",
    "        # flatten the list\n",
    "        res = list(itertools.chain(*res))\n",
    "        res = np.asarray(res) if return_tensor == 'np' else res\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-09-17T13:55:58.229436037Z"
    }
   },
   "outputs": [],
   "source": [
    "# let's make the damn submission\n",
    "from src.pytorch_modular.pytorch_utilities import load_model\n",
    "# base_model = SeqClassModel(in_features=768, hidden_size=128, num_classes=6)\n",
    "# base_model = load_model(base_model=base_model, path=os.path.join(HOME, 'saved_models', '9-17-15-10.pt'))\n",
    "# let's create a dataset object really quick:\n",
    "class TestReviewDS(Dataset):\n",
    "    def __init__(self, data: pd.DataFrame) -> None:\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index) -> tuple[str, int]:\n",
    "        return self.data.iloc[index, 1]\n",
    "\n",
    "# we need a different callate_function\n",
    "def test_collate_function(batch):\n",
    "    embeddings = MODEL(**TOKENIZER(batch, padding=True, return_tensors='pt').to(NOTEBOOK_DEVICE)).last_hidden_state # make sure to return tensors\n",
    "    return embeddings.to(NOTEBOOK_DEVICE)\n",
    "    \n",
    "# let's set the random seed\n",
    "\n",
    "torch.manual_seed(69)\n",
    "\n",
    "test_ds = TestReviewDS(data=df_test)\n",
    "test_loader = DataLoader(test_ds, batch_size=32, shuffle=False, collate_fn=test_collate_function)\n",
    "# next(iter(test_loader)).shape\n",
    "predictions = inference(base_model, inference_source_data=test_loader, return_tensor='list')\n",
    "# convert the numerical labels to the string ones\n",
    "predictions = [idx2cat[p] for p in predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-17T13:55:58.276032280Z",
     "start_time": "2023-09-17T13:55:58.272755631Z"
    }
   },
   "outputs": [],
   "source": [
    "submission = pd.DataFrame(data={\"id\": df_test['id'].tolist(), \"Category\": predictions})\n",
    "sub_dir = os.path.join(HOME, 'submissions')\n",
    "submission.to_csv(os.path.join(sub_dir, f'sub_{len(os.listdir(sub_dir)) + 1}.csv'), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
