{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Practical machine learning and deep learning. Lab 3\n","\n","# Deep Learning in Natural Language Processing\n","\n","# [Competition](https://www.kaggle.com/t/4677b08c063f433ba1eb8f3543af90b4)\n","\n","## Goal\n","\n","Your goal is to implement Neural Network to classify Amazon Products reviews. \n","\n","## Submission\n","\n","Submission format is described at competition page."]},{"cell_type":"markdown","metadata":{},"source":["## Data preprocessing\n","\n","Data preprocessing is an essential step in building a Machine Learning model and depending on how well the data has been preprocessed.\n","\n","In NLP, text preprocessing is the first step in the process of building a model.\n","\n","The various text preprocessing steps are:\n","\n","* Tokenization\n","* Lower casing\n","* Stop words removal\n","* Stemming\n","* Lemmatization\n","\n","These various text preprocessing steps are widely used for dimensionality reduction.\n","\n","First, let's read the input data and then perform preprocessing steps"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-09-10T16:15:47.753751Z","iopub.status.busy":"2023-09-10T16:15:47.753403Z","iopub.status.idle":"2023-09-10T16:15:48.039971Z","shell.execute_reply":"2023-09-10T16:15:48.038883Z","shell.execute_reply.started":"2023-09-10T16:15:47.753722Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Title</th>\n","      <th>Helpfulness</th>\n","      <th>Score</th>\n","      <th>Text</th>\n","      <th>Category</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Golden Valley Natural Buffalo Jerky</td>\n","      <td>0/0</td>\n","      <td>3.0</td>\n","      <td>The description and photo on this product need...</td>\n","      <td>grocery gourmet food</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Westing Game</td>\n","      <td>0/0</td>\n","      <td>5.0</td>\n","      <td>This was a great book!!!! It is well thought t...</td>\n","      <td>toys games</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Westing Game</td>\n","      <td>0/0</td>\n","      <td>5.0</td>\n","      <td>I am a first year teacher, teaching 5th grade....</td>\n","      <td>toys games</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Westing Game</td>\n","      <td>0/0</td>\n","      <td>5.0</td>\n","      <td>I got the book at my bookfair at school lookin...</td>\n","      <td>toys games</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>I SPY A is For Jigsaw Puzzle 63pc</td>\n","      <td>2/4</td>\n","      <td>5.0</td>\n","      <td>Hi! I'm Martine Redman and I created this puzz...</td>\n","      <td>toys games</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                 Title Helpfulness  Score  \\\n","0  Golden Valley Natural Buffalo Jerky         0/0    3.0   \n","1                         Westing Game         0/0    5.0   \n","2                         Westing Game         0/0    5.0   \n","3                         Westing Game         0/0    5.0   \n","4    I SPY A is For Jigsaw Puzzle 63pc         2/4    5.0   \n","\n","                                                Text              Category  \n","0  The description and photo on this product need...  grocery gourmet food  \n","1  This was a great book!!!! It is well thought t...            toys games  \n","2  I am a first year teacher, teaching 5th grade....            toys games  \n","3  I got the book at my bookfair at school lookin...            toys games  \n","4  Hi! I'm Martine Redman and I created this puzz...            toys games  "]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["import pandas as pd\n","\n","train_dataframe = pd.read_csv('/kaggle/input/pmldl-week-3-dl-in-natural-language-processing/train.csv')\n","test_dataframe = pd.read_csv('/kaggle/input/pmldl-week-3-dl-in-natural-language-processing/test.csv')\n","\n","train_dataframe.head()"]},{"cell_type":"markdown","metadata":{},"source":["In the training data we have `4` features (`Title`, `Helpfulness`, `Score` and `Text`) with target category (`Category`). For the test features are the same, except for target column.\n","\n","First, let's write functions for preprocessing helpfulness and score feature in case we needed them."]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2023-09-10T16:15:48.042376Z","iopub.status.busy":"2023-09-10T16:15:48.041992Z","iopub.status.idle":"2023-09-10T16:15:48.049601Z","shell.execute_reply":"2023-09-10T16:15:48.048121Z","shell.execute_reply.started":"2023-09-10T16:15:48.042346Z"},"trusted":true},"outputs":[],"source":["def preprocess_score_inplace(df):\n","    \"\"\"\n","    Normalizes score to make it from 0 to 1.\n","    \n","    For now it is from 1.0 to 5.0, so natural choice\n","    is to normalize by (f - 1.0)/4.0\n","    \"\"\"\n","    df['Score'] = (df['Score'] - 1.0) / 4.0\n","    return df\n","\n","def preprocess_helpfulness_inplace(df):\n","    \"\"\"\n","    Splits feature by '/' and normalize helpfulness to make it from 0 to 1\n","    \n","    The total number of assessments can be 0, so let's substitute it\n","    with 1. The resulting helpfulness still will be zero but we\n","    remove the possibility of division by zero exception.\n","    \"\"\"\n","    _splitted = df['Helpfulness'].str.split('/', expand=True)\n","    _helpful, _total = _splitted[0], _splitted[1]\n","    _total.replace(\"0\", \"1\", inplace=True)\n","    df['Helpfulness'] = _helpful.astype(int) / _total.astype(int)\n","    return df    "]},{"cell_type":"markdown","metadata":{},"source":["The two other features are both text. For simplicity, let's remove concatenate them so that we will have one full text feature. The resulting code is also a function."]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2023-09-10T16:15:48.051984Z","iopub.status.busy":"2023-09-10T16:15:48.051605Z","iopub.status.idle":"2023-09-10T16:15:48.070372Z","shell.execute_reply":"2023-09-10T16:15:48.068944Z","shell.execute_reply.started":"2023-09-10T16:15:48.051953Z"},"trusted":true},"outputs":[],"source":["def concat_title_text_inplace(df):\n","    \"\"\"\n","    Concatenates Title and Text columns together\n","    \"\"\"\n","    df['Text'] = df['Title'] + \" \" + df['Text']\n","    df.drop('Title', axis=1, inplace=True)\n","    return df"]},{"cell_type":"markdown","metadata":{},"source":["Also, encode the target categories, so that the output is become an index"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2023-09-10T16:15:48.074412Z","iopub.status.busy":"2023-09-10T16:15:48.073975Z","iopub.status.idle":"2023-09-10T16:15:48.083943Z","shell.execute_reply":"2023-09-10T16:15:48.082650Z","shell.execute_reply.started":"2023-09-10T16:15:48.074375Z"},"trusted":true},"outputs":[],"source":["# define categories indices\n","cat2idx = {\n","    'toys games': 0,\n","    'health personal care': 1,\n","    'beauty': 2,\n","    'baby products': 3,\n","    'pet supplies': 4,\n","    'grocery gourmet food': 5,\n","}\n","# define reverse mapping\n","idx2cat = {\n","    v:k for k,v in cat2idx.items()\n","}"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2023-09-10T16:15:48.086171Z","iopub.status.busy":"2023-09-10T16:15:48.085714Z","iopub.status.idle":"2023-09-10T16:15:48.102570Z","shell.execute_reply":"2023-09-10T16:15:48.101327Z","shell.execute_reply.started":"2023-09-10T16:15:48.086136Z"},"trusted":true},"outputs":[],"source":["def encode_categories(df):\n","    df['Category'] = df['Category'].apply(lambda x: cat2idx[x])\n","    return df"]},{"cell_type":"markdown","metadata":{},"source":["Let's visualize our first stage of preprocessing."]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2023-09-10T16:15:48.104645Z","iopub.status.busy":"2023-09-10T16:15:48.104266Z","iopub.status.idle":"2023-09-10T16:15:48.130026Z","shell.execute_reply":"2023-09-10T16:15:48.128681Z","shell.execute_reply.started":"2023-09-10T16:15:48.104612Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Helpfulness</th>\n","      <th>Score</th>\n","      <th>Text</th>\n","      <th>Category</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.0</td>\n","      <td>0.5</td>\n","      <td>Golden Valley Natural Buffalo Jerky The descri...</td>\n","      <td>5</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>Westing Game This was a great book!!!! It is w...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>Westing Game I am a first year teacher, teachi...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>Westing Game I got the book at my bookfair at ...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0.5</td>\n","      <td>1.0</td>\n","      <td>I SPY A is For Jigsaw Puzzle 63pc Hi! I'm Mart...</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   Helpfulness  Score                                               Text  \\\n","0          0.0    0.5  Golden Valley Natural Buffalo Jerky The descri...   \n","1          0.0    1.0  Westing Game This was a great book!!!! It is w...   \n","2          0.0    1.0  Westing Game I am a first year teacher, teachi...   \n","3          0.0    1.0  Westing Game I got the book at my bookfair at ...   \n","4          0.5    1.0  I SPY A is For Jigsaw Puzzle 63pc Hi! I'm Mart...   \n","\n","   Category  \n","0         5  \n","1         0  \n","2         0  \n","3         0  \n","4         0  "]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["train_copy = train_dataframe.head().copy()\n","\n","encode_categories(preprocess_score_inplace(preprocess_helpfulness_inplace(concat_title_text_inplace(train_copy))))"]},{"cell_type":"markdown","metadata":{},"source":["### Text cleaning\n","\n","For text cleaning, you can use lower casting, punctuation removal, numbers removal, tokenization, stop words removal, stemming. This will get a perfectly cleaned text without any garbage information."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-10T16:15:48.132208Z","iopub.status.busy":"2023-09-10T16:15:48.131538Z","iopub.status.idle":"2023-09-10T16:15:48.139413Z","shell.execute_reply":"2023-09-10T16:15:48.138057Z","shell.execute_reply.started":"2023-09-10T16:15:48.132177Z"},"trusted":true},"outputs":[],"source":["import re\n","\n","def lower_text(text: str):\n","    return text.lower()\n","\n","def remove_numbers(text: str):\n","    \"\"\"\n","    Substitute all punctuations with space in case of\n","    \"there is5dogs\".\n","    \n","    If subs with '' -> \"there isdogs\"\n","    With ' ' -> there is dogs\n","    \"\"\"\n","    text_nonum = re.sub(r'\\d+', ' ', text)\n","    return text_nonum\n","\n","def remove_punctuation(text: str):\n","    \"\"\"\n","    Substitute all punctiations with space in case of\n","    \"hello!nice to meet you\"\n","    \n","    If subs with '' -> \"hellonice to meet you\"\n","    With ' ' -> \"hello nice to meet you\"\n","    \"\"\"\n","    text_nopunct = re.sub(r'[^a-z|\\s]+', ' ', text)\n","    return text_nopunct\n","\n","def remove_multiple_spaces(text: str):\n","    text_no_doublespace = re.sub('\\s+', ' ', text).strip()\n","    return text_no_doublespace"]},{"cell_type":"markdown","metadata":{},"source":["This will give us clean text."]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2023-09-10T16:15:48.141589Z","iopub.status.busy":"2023-09-10T16:15:48.141224Z","iopub.status.idle":"2023-09-10T16:15:48.161404Z","shell.execute_reply":"2023-09-10T16:15:48.159832Z","shell.execute_reply.started":"2023-09-10T16:15:48.141559Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["I SPY A is For Jigsaw Puzzle 63pc Hi! I'm Martine Redman and I created this puzzle for Briarpatch using a great photo from Jean Marzollo and Walter Wick's terrific book, I Spy School Days. Kids need lots of practice to master the ABC's, and this puzzle provides an enjoyable reinforcing tool. Its visual richness helps non-readers and readers alike to remember word associations, and the wealth of cleverly chosen objects surrounding each letter promote language development. The riddle included multiplies the fun of assembling this colorful puzzle. For another great Briarpatch puzzle, check out I Spy Blocks. END\n","----------\n","i spy a is for jigsaw puzzle 63pc hi! i'm martine redman and i created this puzzle for briarpatch using a great photo from jean marzollo and walter wick's terrific book, i spy school days. kids need lots of practice to master the abc's, and this puzzle provides an enjoyable reinforcing tool. its visual richness helps non-readers and readers alike to remember word associations, and the wealth of cleverly chosen objects surrounding each letter promote language development. the riddle included multiplies the fun of assembling this colorful puzzle. for another great briarpatch puzzle, check out i spy blocks. end\n","----------\n","i spy a is for jigsaw puzzle  pc hi! i'm martine redman and i created this puzzle for briarpatch using a great photo from jean marzollo and walter wick's terrific book, i spy school days. kids need lots of practice to master the abc's, and this puzzle provides an enjoyable reinforcing tool. its visual richness helps non-readers and readers alike to remember word associations, and the wealth of cleverly chosen objects surrounding each letter promote language development. the riddle included multiplies the fun of assembling this colorful puzzle. for another great briarpatch puzzle, check out i spy blocks. end\n","----------\n","i spy a is for jigsaw puzzle  pc hi  i m martine redman and i created this puzzle for briarpatch using a great photo from jean marzollo and walter wick s terrific book  i spy school days  kids need lots of practice to master the abc s  and this puzzle provides an enjoyable reinforcing tool  its visual richness helps non readers and readers alike to remember word associations  and the wealth of cleverly chosen objects surrounding each letter promote language development  the riddle included multiplies the fun of assembling this colorful puzzle  for another great briarpatch puzzle  check out i spy blocks  end\n","----------\n","i spy a is for jigsaw puzzle pc hi i m martine redman and i created this puzzle for briarpatch using a great photo from jean marzollo and walter wick s terrific book i spy school days kids need lots of practice to master the abc s and this puzzle provides an enjoyable reinforcing tool its visual richness helps non readers and readers alike to remember word associations and the wealth of cleverly chosen objects surrounding each letter promote language development the riddle included multiplies the fun of assembling this colorful puzzle for another great briarpatch puzzle check out i spy blocks end\n"]}],"source":["sample_text = train_copy['Text'][4]\n","\n","_lowered = lower_text(sample_text)\n","_without_numbers = remove_numbers(_lowered)\n","_without_punct = remove_punctuation(_without_numbers)\n","_single_spaced = remove_multiple_spaces(_without_punct)\n","\n","print(sample_text)\n","print('-'*10)\n","print(_lowered)\n","print('-'*10)\n","print(_without_numbers)\n","print('-'*10)\n","print(_without_punct)\n","print('-'*10)\n","print(_single_spaced)"]},{"cell_type":"markdown","metadata":{},"source":["Now, harder preprocessing: tokenization, stop words removal and stemming.\n","For that you can use several packages, but we encourage you to use `nltk` - Natural Language ToolKit as well as `torchtext`.\n","\n","\n","Take a look at:\n","* `nltk.tokenize.word_tokenize` or `torchtext.data.utils.get_tokenizer` for tokenization\n","* `nltk.corpus.stopwords` for stop words removal\n","* `nltk.stem.PorterStemmer` for stemming"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-10T16:15:48.165884Z","iopub.status.busy":"2023-09-10T16:15:48.165507Z","iopub.status.idle":"2023-09-10T16:15:48.788344Z","shell.execute_reply":"2023-09-10T16:15:48.787453Z","shell.execute_reply.started":"2023-09-10T16:15:48.165854Z"},"trusted":true},"outputs":[],"source":["# imports here\n","\n","def tokenize_text(text: str) -> list[str]:\n","    return ...\n","\n","def remove_stop_words(tokenized_text: list[str]) -> list[str]:\n","    return ...\n","\n","def stem_words(tokenized_text: list[str]) -> list[str]:\n","    return ..."]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2023-09-10T16:15:48.790311Z","iopub.status.busy":"2023-09-10T16:15:48.789608Z","iopub.status.idle":"2023-09-10T16:15:48.808359Z","shell.execute_reply":"2023-09-10T16:15:48.806882Z","shell.execute_reply.started":"2023-09-10T16:15:48.790278Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["i spy a is for jigsaw puzzle pc hi i m martine redman and i created this puzzle for briarpatch using a great photo from jean marzollo and walter wick s terrific book i spy school days kids need lots of practice to master the abc s and this puzzle provides an enjoyable reinforcing tool its visual richness helps non readers and readers alike to remember word associations and the wealth of cleverly chosen objects surrounding each letter promote language development the riddle included multiplies the fun of assembling this colorful puzzle for another great briarpatch puzzle check out i spy blocks end\n","----------\n","['i', 'spy', 'a', 'is', 'for', 'jigsaw', 'puzzle', 'pc', 'hi', 'i', 'm', 'martine', 'redman', 'and', 'i', 'created', 'this', 'puzzle', 'for', 'briarpatch', 'using', 'a', 'great', 'photo', 'from', 'jean', 'marzollo', 'and', 'walter', 'wick', 's', 'terrific', 'book', 'i', 'spy', 'school', 'days', 'kids', 'need', 'lots', 'of', 'practice', 'to', 'master', 'the', 'abc', 's', 'and', 'this', 'puzzle', 'provides', 'an', 'enjoyable', 'reinforcing', 'tool', 'its', 'visual', 'richness', 'helps', 'non', 'readers', 'and', 'readers', 'alike', 'to', 'remember', 'word', 'associations', 'and', 'the', 'wealth', 'of', 'cleverly', 'chosen', 'objects', 'surrounding', 'each', 'letter', 'promote', 'language', 'development', 'the', 'riddle', 'included', 'multiplies', 'the', 'fun', 'of', 'assembling', 'this', 'colorful', 'puzzle', 'for', 'another', 'great', 'briarpatch', 'puzzle', 'check', 'out', 'i', 'spy', 'blocks', 'end']\n","----------\n","['spy', 'jigsaw', 'puzzle', 'pc', 'hi', 'martine', 'redman', 'created', 'puzzle', 'briarpatch', 'using', 'great', 'photo', 'jean', 'marzollo', 'walter', 'wick', 'terrific', 'book', 'spy', 'school', 'days', 'kids', 'need', 'lots', 'practice', 'master', 'abc', 'puzzle', 'provides', 'enjoyable', 'reinforcing', 'tool', 'visual', 'richness', 'helps', 'non', 'readers', 'readers', 'alike', 'remember', 'word', 'associations', 'wealth', 'cleverly', 'chosen', 'objects', 'surrounding', 'letter', 'promote', 'language', 'development', 'riddle', 'included', 'multiplies', 'fun', 'assembling', 'colorful', 'puzzle', 'another', 'great', 'briarpatch', 'puzzle', 'check', 'spy', 'blocks', 'end']\n","----------\n","['spi', 'jigsaw', 'puzzl', 'pc', 'hi', 'martin', 'redman', 'creat', 'puzzl', 'briarpatch', 'use', 'great', 'photo', 'jean', 'marzollo', 'walter', 'wick', 'terrif', 'book', 'spi', 'school', 'day', 'kid', 'need', 'lot', 'practic', 'master', 'abc', 'puzzl', 'provid', 'enjoy', 'reinforc', 'tool', 'visual', 'rich', 'help', 'non', 'reader', 'reader', 'alik', 'rememb', 'word', 'associ', 'wealth', 'cleverli', 'chosen', 'object', 'surround', 'letter', 'promot', 'languag', 'develop', 'riddl', 'includ', 'multipli', 'fun', 'assembl', 'color', 'puzzl', 'anoth', 'great', 'briarpatch', 'puzzl', 'check', 'spi', 'block', 'end']\n"]}],"source":["_tokenized = tokenize_text(_single_spaced)\n","_without_sw = remove_stop_words(_tokenized)\n","_stemmed = stem_words(_without_sw)\n","\n","print(_single_spaced)\n","print('-'*10)\n","print(_tokenized)\n","print('-'*10)\n","print(_without_sw)\n","print('-'*10)\n","print(_stemmed)"]},{"cell_type":"markdown","metadata":{},"source":["As you can see, there is a lot of words removed as well as the unnecessary language rules (I mean stems, com'on). Now we are able to construct full cleaning preprocessing stage."]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2023-09-10T16:15:48.810386Z","iopub.status.busy":"2023-09-10T16:15:48.809971Z","iopub.status.idle":"2023-09-10T16:15:48.818839Z","shell.execute_reply":"2023-09-10T16:15:48.817502Z","shell.execute_reply.started":"2023-09-10T16:15:48.810343Z"},"trusted":true},"outputs":[],"source":["def preprocessing_stage(text):\n","    _lowered = lower_text(text)\n","    _without_numbers = remove_numbers(_lowered)\n","    _without_punct = remove_punctuation(_without_numbers)\n","    _single_spaced = remove_multiple_spaces(_without_punct)\n","    _tokenized = tokenize_text(_single_spaced)\n","    _without_sw = remove_stop_words(_tokenized)\n","    _stemmed = stem_words(_without_sw)\n","    \n","    return _stemmed\n","\n","def clean_text_inplace(df):\n","    df['Text'] = df['Text'].apply(preprocessing_stage)\n","    return df\n","\n","def preprocess(df):\n","    df.fillna(\" \", inplace=True)\n","    _preprocess_score = preprocess_score_inplace(df)\n","    _preprocess_helpfulness = preprocess_helpfulness_inplace(_preprocess_score)\n","    _concatted = concat_title_text_inplace(_preprocess_helpfulness)\n","\n","    if 'Category' in df.columns:\n","        _encoded = encode_categories(_concatted)\n","        _cleaned = clean_text_inplace(_encoded)\n","    else:\n","        _cleaned = clean_text_inplace(_concatted)\n","    return _cleaned\n","    "]},{"cell_type":"markdown","metadata":{},"source":["And now let's apply it on our train and test dataframes."]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2023-09-10T16:15:48.820167Z","iopub.status.busy":"2023-09-10T16:15:48.819847Z","iopub.status.idle":"2023-09-10T16:17:03.123896Z","shell.execute_reply":"2023-09-10T16:17:03.122724Z","shell.execute_reply.started":"2023-09-10T16:15:48.820143Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Helpfulness</th>\n","      <th>Score</th>\n","      <th>Text</th>\n","      <th>Category</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.0</td>\n","      <td>0.5</td>\n","      <td>[golden, valley, natur, buffalo, jerki, descri...</td>\n","      <td>5</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>[west, game, great, book, well, thought, easil...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>[west, game, first, year, teacher, teach, th, ...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>[west, game, got, book, bookfair, school, look...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0.5</td>\n","      <td>1.0</td>\n","      <td>[spi, jigsaw, puzzl, pc, hi, martin, redman, c...</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   Helpfulness  Score                                               Text  \\\n","0          0.0    0.5  [golden, valley, natur, buffalo, jerki, descri...   \n","1          0.0    1.0  [west, game, great, book, well, thought, easil...   \n","2          0.0    1.0  [west, game, first, year, teacher, teach, th, ...   \n","3          0.0    1.0  [west, game, got, book, bookfair, school, look...   \n","4          0.5    1.0  [spi, jigsaw, puzzl, pc, hi, martin, redman, c...   \n","\n","   Category  \n","0         5  \n","1         0  \n","2         0  \n","3         0  \n","4         0  "]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["train_preprocessed = preprocess(train_dataframe)\n","test_preprocessed = preprocess(test_dataframe)\n","\n","train_preprocessed.head()"]},{"cell_type":"markdown","metadata":{},"source":["Now, let's split our original train dataset into train and val sets."]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2023-09-10T16:17:03.127989Z","iopub.status.busy":"2023-09-10T16:17:03.127443Z","iopub.status.idle":"2023-09-10T16:17:03.152760Z","shell.execute_reply":"2023-09-10T16:17:03.151812Z","shell.execute_reply.started":"2023-09-10T16:17:03.127953Z"},"trusted":true},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","ratio = 0.2\n","train, val = train_test_split(\n","    train_preprocessed, stratify=train_preprocessed['Category'], test_size=0.2, random_state=420\n",")"]},{"cell_type":"markdown","metadata":{},"source":["And now, for the best result, lets get rid of pandas so that nothing is stopping us from working with torchtext. For that let's create an iterator that is going to yield samples for us."]},{"cell_type":"markdown","metadata":{},"source":["# Creating dataloaders\n","\n","First, you should generate our vocab from the train set.\n","\n","For that, use `torchtext.vocab.build_vocab_from_iterator`."]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2023-09-10T16:32:44.269868Z","iopub.status.busy":"2023-09-10T16:32:44.269463Z","iopub.status.idle":"2023-09-10T16:32:47.174358Z","shell.execute_reply":"2023-09-10T16:32:47.173092Z","shell.execute_reply.started":"2023-09-10T16:32:44.269831Z"},"trusted":true},"outputs":[],"source":["from torchtext.vocab import build_vocab_from_iterator\n","\n","def yield_tokens(df):\n","    for _, sample in train.iterrows():\n","        yield sample.to_list()[2]\n","\n","\n","# Define special symbols and indices\n","UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3\n","# Make sure the tokens are in order of their indices to properly insert them in vocab\n","special_symbols = ['<unk>', '<pad>', '<bos>', '<eos>']\n","\n","vocab = ...\n","vocab.set_default_index(UNK_IDX)"]},{"cell_type":"markdown","metadata":{},"source":["And then use our vocab to encode the tokenized sequence"]},{"cell_type":"code","execution_count":26,"metadata":{"execution":{"iopub.execute_input":"2023-09-10T16:33:09.988862Z","iopub.status.busy":"2023-09-10T16:33:09.988458Z","iopub.status.idle":"2023-09-10T16:33:09.997309Z","shell.execute_reply":"2023-09-10T16:33:09.996137Z","shell.execute_reply.started":"2023-09-10T16:33:09.988828Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["['west', 'game', 'first', 'year', 'teacher', 'teach', 'th', 'grade', 'special', 'read', 'class', 'high', 'comprehens', 'level', 'read', 'book', 'one', 'best', 'thing', 'taught', 'year', 'expand', 'mind', 'allow', 'put', 'charact', 'place', 'easi', 'student', 'make', 'mind', 'movi', 'even', 'use', 'whole', 'read', 'class', 'time', 'order', 'finish', 'book', 'student', 'wait', 'hear', 'end', 'excel', 'book', 'read', 'everi', 'year', 'student']\n","[2556, 43, 33, 14, 2751, 807, 860, 1724, 728, 131, 1895, 191, 6981, 583, 131, 515, 5, 59, 46, 3505, 14, 2954, 528, 450, 40, 1125, 165, 50, 1924, 22, 528, 945, 30, 4, 271, 131, 1895, 13, 68, 623, 515, 1924, 426, 600, 180, 311, 515, 131, 85, 14, 1924]\n"]}],"source":["sample = train['Text'][2]\n","print(sample)\n","encoded = vocab(sample)\n","print(encoded)"]},{"cell_type":"markdown","metadata":{},"source":["Now we can define our collate function and create dataloaders"]},{"cell_type":"code","execution_count":35,"metadata":{"execution":{"iopub.execute_input":"2023-09-10T16:35:37.307444Z","iopub.status.busy":"2023-09-10T16:35:37.307110Z","iopub.status.idle":"2023-09-10T16:35:37.324888Z","shell.execute_reply":"2023-09-10T16:35:37.323537Z","shell.execute_reply.started":"2023-09-10T16:35:37.307400Z"},"trusted":true},"outputs":[],"source":["import torch\n","from torch.utils.data import DataLoader\n","\n","torch.manual_seed(420)\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","def collate_batch(batch):\n","    label_list, text_list, score_list, helpfulness_list, offsets = [], [], [], [], [0]\n","    for _helpfulnes, _score, _text, _label in batch:\n","        ...\n","    return label_list.to(device), text_list.to(device), offsets.to(device), score_list.to(device), helpfulness_list.to(device)\n","\n","train_dataloader = DataLoader(\n","    train.to_numpy(), batch_size=128, shuffle=True, collate_fn=collate_batch\n",")\n","\n","val_dataloader = DataLoader(\n","    val.to_numpy(), batch_size=128, shuffle=False, collate_fn=collate_batch\n",")"]},{"cell_type":"markdown","metadata":{},"source":["# Defining Network\n","\n","\n","For writing a network you can use `torch.nn.Embedding` or `torch.nn.EmbeddingBag`. This will allow your netorwk to learn embedding vector for your tokens.\n","\n","As for the other modules in your network, consider these options:\n","* Simple Linear layers, activations, basic stuff that goes into the network\n","* There is a possible of not using the offsets (indices of sequences) in the formard, put use predefined sequence length (maximum length, some value, etc.). If this is an option for you, change the `collate_batch` function according to your architecture.\n","* You could use all this recurrent stuff (RNN, GRU, LSTM, even Transformer, all up to you), but remembder about the dimentions and hidden states\n","* If you have any quiestions - google it"]},{"cell_type":"code","execution_count":49,"metadata":{"execution":{"iopub.execute_input":"2023-09-10T17:00:10.267625Z","iopub.status.busy":"2023-09-10T17:00:10.267160Z","iopub.status.idle":"2023-09-10T17:00:10.276201Z","shell.execute_reply":"2023-09-10T17:00:10.274651Z","shell.execute_reply.started":"2023-09-10T17:00:10.267583Z"},"trusted":true},"outputs":[],"source":["import torch.nn as nn\n","\n","class TextClassificationModel(nn.Module):\n","    def __init__(self, num_classes):\n","        super(TextClassificationModel, self).__init__()\n","        ...\n","\n","    def forward(self, text, offsets):\n","        ..."]},{"cell_type":"code","execution_count":56,"metadata":{"execution":{"iopub.execute_input":"2023-09-10T17:01:07.059754Z","iopub.status.busy":"2023-09-10T17:01:07.059384Z","iopub.status.idle":"2023-09-10T17:01:07.070400Z","shell.execute_reply":"2023-09-10T17:01:07.069450Z","shell.execute_reply.started":"2023-09-10T17:01:07.059730Z"},"trusted":true},"outputs":[],"source":["from tqdm.autonotebook import tqdm\n","\n","def train_one_epoch(\n","    model,\n","    loader,\n","    optimizer,\n","    loss_fn,\n","    epoch_num=-1\n","):\n","    loop = tqdm(\n","        enumerate(loader, 1),\n","        total=len(loader),\n","        desc=f\"Epoch {epoch_num}: train\",\n","        leave=True,\n","    )\n","    model.train()\n","    train_loss = 0.0\n","    for i, batch in loop:\n","        labels, texts, offsets, scores, helpfulness = batch\n","        # zero the parameter gradients\n","        ...\n","\n","        # forward pass\n","        outputs = ...\n","        # loss calculation\n","        loss = ...\n","        \n","        # backward pass\n","        ...\n","\n","        # optimizer run\n","        ...\n","\n","        train_loss += ...\n","        loop.set_postfix({\"loss\": train_loss/(i * len(labels))})\n","\n","def val_one_epoch(\n","    model,\n","    loader,\n","    loss_fn,\n","    epoch_num=-1,\n","    best_so_far=0.0,\n","    ckpt_path='best.pt'\n","):\n","    \n","    loop = tqdm(\n","        enumerate(loader, 1),\n","        total=len(loader),\n","        desc=f\"Epoch {epoch_num}: val\",\n","        leave=True,\n","    )\n","    val_loss = 0.0\n","    correct = 0\n","    total = 0\n","    with torch.no_grad():\n","        model.eval()  # evaluation mode\n","        for i, batch in loop:\n","            labels, texts, offsets, scores, helpfulness = batch\n","\n","            # forward pass\n","            outputs = ...\n","            # loss calculation\n","            loss = ...\n","            \n","            _, predicted = ...\n","            total += ...\n","            correct += ...\n","\n","            val_loss += ...\n","            loop.set_postfix({\"loss\": val_loss/total, \"acc\": correct / total})\n","\n","        if correct / total > best:\n","            torch.save(...)\n","            return correct / total\n","\n","    return best_so_far"]},{"cell_type":"code","execution_count":76,"metadata":{"execution":{"iopub.execute_input":"2023-09-10T17:15:00.046131Z","iopub.status.busy":"2023-09-10T17:15:00.045790Z","iopub.status.idle":"2023-09-10T17:15:00.065114Z","shell.execute_reply":"2023-09-10T17:15:00.063305Z","shell.execute_reply.started":"2023-09-10T17:15:00.046106Z"},"trusted":true},"outputs":[],"source":["epochs = ...\n","model = TextClassificationModel(...).to(device)\n","optimizer = ...\n","loss_fn = ..."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-10T17:15:00.512516Z","iopub.status.busy":"2023-09-10T17:15:00.512175Z","iopub.status.idle":"2023-09-10T17:15:48.081406Z","shell.execute_reply":"2023-09-10T17:15:48.079846Z","shell.execute_reply.started":"2023-09-10T17:15:00.512492Z"},"trusted":true},"outputs":[],"source":["best = -float('inf')\n","for epoch in range(epochs):\n","    train_one_epoch(model, train_dataloader, optimizer, loss_fn, epoch_num=epoch)\n","    best = val_one_epoch(model, val_dataloader, loss_fn, epoch, best_so_far=best)"]},{"cell_type":"markdown","metadata":{},"source":["# Predictions"]},{"cell_type":"code","execution_count":78,"metadata":{"execution":{"iopub.execute_input":"2023-09-10T17:15:53.687229Z","iopub.status.busy":"2023-09-10T17:15:53.686867Z","iopub.status.idle":"2023-09-10T17:15:53.697916Z","shell.execute_reply":"2023-09-10T17:15:53.696015Z","shell.execute_reply.started":"2023-09-10T17:15:53.687204Z"},"trusted":true},"outputs":[],"source":["def collate_batch(batch):\n","    ...\n","\n","test_dataloader = DataLoader(\n","    test_preprocessed.to_numpy(), batch_size=128, shuffle=False, collate_fn=collate_batch\n",")"]},{"cell_type":"code","execution_count":79,"metadata":{"execution":{"iopub.execute_input":"2023-09-10T17:15:54.262980Z","iopub.status.busy":"2023-09-10T17:15:54.262614Z","iopub.status.idle":"2023-09-10T17:15:54.270985Z","shell.execute_reply":"2023-09-10T17:15:54.269671Z","shell.execute_reply.started":"2023-09-10T17:15:54.262951Z"},"trusted":true},"outputs":[],"source":["def predict(\n","    model,\n","    loader,\n","):\n","    loop = tqdm(\n","        enumerate(loader, 1),\n","        total=len(loader),\n","        desc=\"Predictions:\",\n","        leave=True,\n","    )\n","    predictions = []\n","    with torch.no_grad():\n","        model.eval()  # evaluation mode\n","        for i, batch in loop:\n","            texts, offsets, scores, helpfulness = batch\n","\n","            # forward pass and loss calculation\n","            outputs = model(texts, offsets)\n","            \n","            _, predicted = torch.max(outputs.data, 1)\n","            predictions += predicted.detach().cpu().tolist()\n","\n","    return predictions"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-10T17:15:54.846205Z","iopub.status.busy":"2023-09-10T17:15:54.845815Z","iopub.status.idle":"2023-09-10T17:15:55.137130Z","shell.execute_reply":"2023-09-10T17:15:55.136138Z","shell.execute_reply.started":"2023-09-10T17:15:54.846179Z"},"trusted":true},"outputs":[],"source":["ckpt = torch.load(\"best.pt\")\n","model.load_state_dict(ckpt)\n","\n","predictions = predict(model, test_dataloader)\n","predictions[:10]"]},{"cell_type":"code","execution_count":81,"metadata":{"execution":{"iopub.execute_input":"2023-09-10T17:15:56.288564Z","iopub.status.busy":"2023-09-10T17:15:56.287913Z","iopub.status.idle":"2023-09-10T17:15:56.311894Z","shell.execute_reply":"2023-09-10T17:15:56.310598Z","shell.execute_reply.started":"2023-09-10T17:15:56.288514Z"},"trusted":true},"outputs":[],"source":["results = pd.Series(predictions).apply(lambda x: idx2cat[x])\n","results.to_csv('submission.csv', index_label='id')"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
