{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-17T13:55:21.049919950Z",
     "start_time": "2023-09-17T13:55:20.867618399Z"
    }
   },
   "outputs": [],
   "source": [
    "# let's start with the data and see how it goes\n",
    "import os\n",
    "import pandas as pd\n",
    "HOME = os.getcwd()\n",
    "train_csv = os.path.join(HOME, 'data', 'train.csv')\n",
    "test_csv = os.path.join(HOME, 'data', 'test.csv')\n",
    "\n",
    "df_train = pd.read_csv(train_csv)\n",
    "df_test = pd.read_csv(test_csv)\n",
    "# set the columns names to lower case \n",
    "\n",
    "df_train.columns = [c.lower() for c in df_train.columns]\n",
    "df_test.columns = [c.lower() for c in df_test.columns]\n",
    "\n",
    "# remove unnecessary columns\n",
    "df_train.drop(columns=['helpfulness', 'score'], inplace=True)\n",
    "df_test.drop(columns=['helpfulness', 'score'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-17T13:55:21.093064586Z",
     "start_time": "2023-09-17T13:55:21.092757Z"
    }
   },
   "outputs": [],
   "source": [
    "import nltk \n",
    "from nltk.tokenize import TweetTokenizer\n",
    "# add a small piece of code to call the pytorch_modular code\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "current = HOME\n",
    "while 'src' not in os.listdir(current):\n",
    "    current = Path(current).parent\n",
    "\n",
    "sys.path.append(str(current))\n",
    "sys.path.append(os.path.join(current, 'src'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-17T13:55:21.094474458Z",
     "start_time": "2023-09-17T13:55:21.092989816Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Golden Valley Natural Buffalo Jerky</td>\n",
       "      <td>The description and photo on this product need...</td>\n",
       "      <td>grocery gourmet food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Westing Game</td>\n",
       "      <td>This was a great book!!!! It is well thought t...</td>\n",
       "      <td>toys games</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Westing Game</td>\n",
       "      <td>I am a first year teacher, teaching 5th grade....</td>\n",
       "      <td>toys games</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Westing Game</td>\n",
       "      <td>I got the book at my bookfair at school lookin...</td>\n",
       "      <td>toys games</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I SPY A is For Jigsaw Puzzle 63pc</td>\n",
       "      <td>Hi! I'm Martine Redman and I created this puzz...</td>\n",
       "      <td>toys games</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 title  \\\n",
       "0  Golden Valley Natural Buffalo Jerky   \n",
       "1                         Westing Game   \n",
       "2                         Westing Game   \n",
       "3                         Westing Game   \n",
       "4    I SPY A is For Jigsaw Puzzle 63pc   \n",
       "\n",
       "                                                text              category  \n",
       "0  The description and photo on this product need...  grocery gourmet food  \n",
       "1  This was a great book!!!! It is well thought t...            toys games  \n",
       "2  I am a first year teacher, teaching 5th grade....            toys games  \n",
       "3  I got the book at my bookfair at school lookin...            toys games  \n",
       "4  Hi! I'm Martine Redman and I created this puzz...            toys games  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-17T13:55:21.096717811Z",
     "start_time": "2023-09-17T13:55:21.093288265Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>PetSafe Staywell Pet Door with Clear Hard Flap</td>\n",
       "      <td>We've only had it installed about 2 weeks. So ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Kaytee Timothy Cubes, 1-Pound</td>\n",
       "      <td>My bunny had a hard time eating this because t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Body Back Buddy</td>\n",
       "      <td>would never in a million years have guessed th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>SnackMasters California Style Turkey Jerky</td>\n",
       "      <td>Being the jerky fanatic I am, snackmasters han...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Premier Busy Buddy Tug-a-Jug Treat Dispensing ...</td>\n",
       "      <td>Wondered how quick my dog would catch on to th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                              title  \\\n",
       "0   0     PetSafe Staywell Pet Door with Clear Hard Flap   \n",
       "1   1                      Kaytee Timothy Cubes, 1-Pound   \n",
       "2   2                                    Body Back Buddy   \n",
       "3   3         SnackMasters California Style Turkey Jerky   \n",
       "4   4  Premier Busy Buddy Tug-a-Jug Treat Dispensing ...   \n",
       "\n",
       "                                                text  \n",
       "0  We've only had it installed about 2 weeks. So ...  \n",
       "1  My bunny had a hard time eating this because t...  \n",
       "2  would never in a million years have guessed th...  \n",
       "3  Being the jerky fanatic I am, snackmasters han...  \n",
       "4  Wondered how quick my dog would catch on to th...  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-17T13:55:21.097212598Z",
     "start_time": "2023-09-17T13:55:21.093713953Z"
    }
   },
   "outputs": [],
   "source": [
    "# preprocessing functions\n",
    "import re\n",
    "from typing import List\n",
    "\n",
    "def to_lower(text: str) -> str:\n",
    "    return text.lower()\n",
    "\n",
    "def no_extra_spaces(text: str) -> str:\n",
    "    return re.sub('\\s+', ' ', text)\n",
    "\n",
    "def no_extra_chars(text: str) -> str:\n",
    "    return re.sub(r'[^a-zA-Z\\s,!.;:-]+', ' ', text) \n",
    "\n",
    "text = 'aaa5531--==-||\"z2::,.a'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "dictionary_file_url = 'https://raw.githubusercontent.com/dwyl/english-words/master/words_alpha.txt'\n",
    "\n",
    "r = requests.get(dictionary_file_url, allow_redirects=True)\n",
    "\n",
    "dict_path = os.path.join(HOME, 'data', 'dictionary.txt') \n",
    "\n",
    "with open(dict_path, 'wb') as f:\n",
    "    f.write(r.content)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "369652"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's convert this file into a dictionary object:\n",
    "# discard any words shorter than 3 letters\n",
    "dictionary = set()\n",
    "with open(dict_path, 'r') as f:\n",
    "    for line in f.readlines():        \n",
    "        word = line[:-1]\n",
    "        if len(word) >= 3:\n",
    "            dictionary.add(word)\n",
    "\n",
    "len(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download a more comprehensive list of stop words\n",
    "stopwords_file_url = 'https://raw.githubusercontent.com/stopwords-iso/stopwords-en/master/stopwords-en.txt'\n",
    "r = requests.get(stopwords_file_url, allow_redirects=True)\n",
    "stopwords_path = os.path.join(HOME, 'data', 'stop_words_en.txt') \n",
    "with open(stopwords_path, 'wb') as f:\n",
    "    f.write(r.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "STOP_WORDS = set()\n",
    "with open(stopwords_path, 'r') as f:\n",
    "    for line in f.readlines():        \n",
    "        word = line[:-1]\n",
    "        if len(word) >= 3:\n",
    "            STOP_WORDS.add(word)\n",
    "\n",
    "len(STOP_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/ayhem18/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "LEMMATIZER = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "See the title of this review. Fortunately, I am a packrat, and kept a bunch of hole repair kits from various blow up things that we have gone through over the years. Does not come with a hole repair kit though, just to warn you. Anyway, it is back in black and bouncing our 3 year old all over the place. Indoor only, I would say. Very highly recommended, in spite of a hole within a week of use. Hope that this is the first and last one... probably not.\n",
      "title review fortunately packrat bunch hole repair kit blow hole repair kit warn black bouncing indoor highly recommended spite hole week hope\n"
     ]
    }
   ],
   "source": [
    "def filter_text(text: str, tokenizer = None) -> None:\n",
    "    tokenizer = TweetTokenizer() if tokenizer is None else tokenizer\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    # add a\n",
    "    tokens = [LEMMATIZER.lemmatize(t.strip().lower()) for t in tokens if t not in STOP_WORDS and t in dictionary]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "def process(text: str) -> str:\n",
    "    # first lower, remove extrac chracters\n",
    "    text1 = to_lower(no_extra_chars(text))\n",
    "    # remove redundant words\n",
    "    text2 = filter_text(text1)\n",
    "    # remove extra spaces\n",
    "    return no_extra_spaces(text2)\n",
    "\n",
    "import random\n",
    "random.seed(69)\n",
    "example = df_train['text'][int(random.random() * len(df_train))]\n",
    "print(example)\n",
    "print(process(example))\n",
    "\n",
    "# # drop the 'text' column as only the title will be used for classification\n",
    "# df_train.drop(columns=['text'], inplace=True)\n",
    "# df_test.drop(columns=['text'], inplace=True)\n",
    "\n",
    "# 16 rows have missing values in the 'title' column, remove them\n",
    "df_train.fillna(value='', inplace=True)\n",
    "df_test.fillna(value='', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-17T13:55:21.097504435Z",
     "start_time": "2023-09-17T13:55:21.093860678Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title       0\n",
      "text        0\n",
      "category    0\n",
      "dtype: int64\n",
      "####################################################################################################\n",
      "id       0\n",
      "title    0\n",
      "text     0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df_train.isna().sum())\n",
    "print(\"#\" * 100)\n",
    "print(df_test.isna().sum())\n",
    "\n",
    "df_train_org, df_test_org = df_train.copy(), df_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-17T13:55:28.263882527Z",
     "start_time": "2023-09-17T13:55:21.093991142Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "cat2idx = {\n",
    "    'toys games': 0,\n",
    "    'health personal care': 1,\n",
    "    'beauty': 2,\n",
    "    'baby products': 3,\n",
    "    'pet supplies': 4,\n",
    "    'grocery gourmet food': 5,\n",
    "}\n",
    "\n",
    "idx2cat = {\n",
    "    0:'toys games',\n",
    "    1:'health personal care',\n",
    "    2:'beauty',\n",
    "    3:'baby products',\n",
    "    4:'pet supplies',\n",
    "    5:'grocery gourmet food' \n",
    "}\n",
    "\n",
    "# making sure the dataframes are ready for training\n",
    "def df_process_data(row):\n",
    "    row['title'] = process(row['title'])\n",
    "    row['text'] = process(row['text'])\n",
    "    return row\n",
    "\n",
    "def df_process_labels(row):\n",
    "    # map it to an integer\n",
    "    row['category'] = cat2idx[row['category']]\n",
    "    return row\n",
    "\n",
    "# process the fields\n",
    "df_train = df_train.apply(df_process_data, axis=1)\n",
    "# process the labels\n",
    "df_train = df_train.apply(df_process_labels, axis=1)\n",
    "# process the data is the test split\n",
    "df_test = df_test.apply(df_process_data, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the data category for both training and testing data \n",
    "df_train['data'] = df_train['title'] + \"\\t\" + df_train['text']\n",
    "df_test['data'] = df_test['title'] + \"\\t\" + df_test['text']\n",
    "\n",
    "# remove the 'text' and 'title' columns\n",
    "df_train.drop(columns=['text', 'title'], inplace=True)\n",
    "df_test.drop(columns=['text', 'title'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-17T13:55:28.278018893Z",
     "start_time": "2023-09-17T13:55:28.265548448Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_data, val_data = train_test_split(df_train, test_size=0.15, stratify=df_train['category'], random_state=69)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data['data_len'] = train_data['data'].apply(lambda x: len(x))\n",
    "# print(max(train_data['data_len']))\n",
    "# print(min(train_data['data_len']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-17T13:55:29.050319218Z",
     "start_time": "2023-09-17T13:55:28.277831762Z"
    }
   },
   "outputs": [],
   "source": [
    "# in the rest of the code I will be using the d\n",
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "CHECKPOINT = 'distilbert-base-uncased' # let's keep it simple as for the first iteration\n",
    "MODEL = AutoModel.from_pretrained(CHECKPOINT).to(DEVICE)\n",
    "TOKENIZER = AutoTokenizer.from_pretrained(CHECKPOINT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-17T13:55:29.093210680Z",
     "start_time": "2023-09-17T13:55:29.092817012Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "def collate_function(batch: List[str]):\n",
    "    # batch will represent a list of tuples (text, category) \n",
    "    x, y = [list(row) for row in zip(*batch)]\n",
    "    # convert both labels and data to tensors\n",
    "    y_tensor = torch.FloatTensor(y).to(device=DEVICE)\n",
    "    embeddings = MODEL(**TOKENIZER(x, padding=True, return_tensors='pt', truncation=True).to(DEVICE)).last_hidden_state # make sure to return tensors\n",
    "    return embeddings.to(DEVICE), y_tensor\n",
    "\n",
    "# let's create a dataset object really quick:\n",
    "class LabeledReviewDS(Dataset):\n",
    "    def __init__(self, data: pd.DataFrame) -> None:\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index) -> tuple[str, int]:\n",
    "        return self.data.iloc[index]['data'], self.data.iloc[index]['category']\n",
    "\n",
    "# let's set the random seed\n",
    "\n",
    "torch.manual_seed(69)\n",
    "\n",
    "train_ds = LabeledReviewDS(train_data)\n",
    "val_ds = LabeledReviewDS(val_data)\n",
    "\n",
    "# create the dataloaders\n",
    "train_dl = DataLoader(dataset=train_ds, batch_size=16, shuffle=True, collate_fn=collate_function, drop_last=True)\n",
    "val_dl = DataLoader(dataset=val_ds, batch_size=16, shuffle=False, collate_fn=collate_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-17T13:55:29.093530619Z",
     "start_time": "2023-09-17T13:55:29.093114069Z"
    }
   },
   "outputs": [],
   "source": [
    "# next(iter(train_dl))\n",
    "# looks our data is loaded and ready to go, time to build a model!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train A model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-17T13:55:29.094325168Z",
     "start_time": "2023-09-17T13:55:29.093361853Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.nn.functional import leaky_relu\n",
    "\n",
    "class SeqClassModel(nn.Module):\n",
    "    def __init__(self, \n",
    "                in_features: int,\n",
    "                hidden_size: int, \n",
    "                num_classes: int, \n",
    "                num_layers: int = 2, \n",
    "                dropout: float=0.4, \n",
    "                *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.output_units = num_classes if num_classes > 2 else 1\n",
    "        self.rnn = nn.LSTM(input_size=in_features, \n",
    "                           hidden_size=hidden_size, \n",
    "                           dropout=dropout, \n",
    "                           num_layers=num_layers,\n",
    "                           bidirectional=True, # bidiretional RNN are more powerful\n",
    "                           batch_first=True # easier manipulation\n",
    "                           )\n",
    "        # the coefficient 2 comes from the fact that the lstm is bidirectional, the rest is similar to the LSTM documention Pytorch\n",
    "        linear_input_dim = 2 * num_layers * hidden_size \n",
    "        self.batch_layer= nn.BatchNorm1d(num_features=linear_input_dim)\n",
    "        # self.relu_layer = nn.LeakyReLU()\n",
    "        self.head = nn.Linear(in_features=linear_input_dim, out_features=self.output_units)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        # first pass it through the rnn\n",
    "        _, (hidden_state, _) = self.rnn(x)\n",
    "        batch_size = hidden_state.shape[1]\n",
    "        # first permuting channels: batch_size as dimensions '0' \n",
    "        # only only the last lstm layer\n",
    "        hidden_state = hidden_state.permute((1, 0, 2)).reshape((batch_size, -1))\n",
    "        return self.head.forward(self.batch_layer(hidden_state))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-17T13:55:29.094486140Z",
     "start_time": "2023-09-17T13:55:29.093505082Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import LinearLR\n",
    "from torchmetrics.classification import MulticlassF1Score, MulticlassAccuracy\n",
    "\n",
    "base_model = SeqClassModel(in_features=768, hidden_size=64, num_classes=6)\n",
    "optimizer = AdamW(base_model.parameters(), lr=0.01)\n",
    "scheduler = LinearLR(optimizer, start_factor=1.0, end_factor=0.005, total_iters=100)\n",
    "\n",
    "accuracy_metric, f1_metric = MulticlassAccuracy(num_classes=6), MulticlassF1Score(num_classes=6)\n",
    "\n",
    "metrics = {'accuracy': accuracy_metric, 'f1_score': f1_metric}\n",
    "\n",
    "train_configuration = {'optimizer': optimizer,\n",
    "                        'scheduler': scheduler,\n",
    "                        'min_val_loss': 10 ** -4,\n",
    "                        'max_epochs': 50,\n",
    "                        'report_epoch': 2,\n",
    "                        'device': DEVICE, \n",
    "                        'metrics': metrics,\n",
    "                        'no_improve_stop': 10\n",
    "                        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-17T13:55:58.229534101Z",
     "start_time": "2023-09-17T13:55:29.093614286Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Created SummaryWriter, saving to: /home/ayhem18/DEV/My_Kaggle_Repo/amazon_reviews/runs/experience_19...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1/50 [03:08<2:34:01, 188.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#########################\n",
      "training loss: 0.6779630907493479\n",
      "train_accuracy: 0.737939178943634\n",
      "train_f1_score: 0.7096965909004211\n",
      "validation loss : 0.5373522022565206\n",
      "val_accuracy: 0.809920072555542\n",
      "val_f1_score: 0.7875585556030273\n",
      "#########################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 3/50 [09:31<2:29:25, 190.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#########################\n",
      "training loss: 0.4981355534090715\n",
      "train_accuracy: 0.815688967704773\n",
      "train_f1_score: 0.7916708588600159\n",
      "validation loss : 0.4798527381916841\n",
      "val_accuracy: 0.8334441781044006\n",
      "val_f1_score: 0.8111178874969482\n",
      "#########################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 5/50 [15:52<2:22:57, 190.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#########################\n",
      "training loss: 0.45814702941214336\n",
      "train_accuracy: 0.8262511491775513\n",
      "train_f1_score: 0.8062968254089355\n",
      "validation loss : 0.44089928739269574\n",
      "val_accuracy: 0.8332328200340271\n",
      "val_f1_score: 0.8132728338241577\n",
      "#########################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 7/50 [22:14<2:16:48, 190.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#########################\n",
      "training loss: 0.4295128247685292\n",
      "train_accuracy: 0.8369565606117249\n",
      "train_f1_score: 0.8170448541641235\n",
      "validation loss : 0.41169475691517193\n",
      "val_accuracy: 0.8451753854751587\n",
      "val_f1_score: 0.8227936029434204\n",
      "#########################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 9/50 [28:36<2:10:25, 190.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#########################\n",
      "training loss: 0.4206440725712215\n",
      "train_accuracy: 0.84557044506073\n",
      "train_f1_score: 0.8252637982368469\n",
      "validation loss : 0.4336529390712579\n",
      "val_accuracy: 0.8521341681480408\n",
      "val_f1_score: 0.8327414393424988\n",
      "#########################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 11/50 [34:56<2:03:49, 190.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#########################\n",
      "training loss: 0.406242218555773\n",
      "train_accuracy: 0.8491209149360657\n",
      "train_f1_score: 0.8297417759895325\n",
      "validation loss : 0.41055941169460614\n",
      "val_accuracy: 0.841548502445221\n",
      "val_f1_score: 0.8220723271369934\n",
      "#########################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 13/50 [41:20<1:57:49, 191.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#########################\n",
      "training loss: 0.3932165937581483\n",
      "train_accuracy: 0.8530812859535217\n",
      "train_f1_score: 0.8326742053031921\n",
      "validation loss : 0.4221890305578709\n",
      "val_accuracy: 0.8424361944198608\n",
      "val_f1_score: 0.8225082755088806\n",
      "#########################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 15/50 [47:41<1:51:19, 190.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#########################\n",
      "training loss: 0.38871463636791004\n",
      "train_accuracy: 0.8526533842086792\n",
      "train_f1_score: 0.8329665660858154\n",
      "validation loss : 0.3936349258025487\n",
      "val_accuracy: 0.8587116599082947\n",
      "val_f1_score: 0.8401244878768921\n",
      "#########################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 17/50 [54:00<1:44:34, 190.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#########################\n",
      "training loss: 0.39703962104899043\n",
      "train_accuracy: 0.8496612310409546\n",
      "train_f1_score: 0.8311195969581604\n",
      "validation loss : 0.40311111382643383\n",
      "val_accuracy: 0.8514856696128845\n",
      "val_f1_score: 0.8310486674308777\n",
      "#########################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 19/50 [1:00:20<1:38:14, 190.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#########################\n",
      "training loss: 0.37994036928169866\n",
      "train_accuracy: 0.8591514825820923\n",
      "train_f1_score: 0.8407832384109497\n",
      "validation loss : 0.39710221311450006\n",
      "val_accuracy: 0.8531365394592285\n",
      "val_f1_score: 0.8319758772850037\n",
      "#########################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 21/50 [1:06:40<1:31:52, 190.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#########################\n",
      "training loss: 0.37503389488949496\n",
      "train_accuracy: 0.8593876361846924\n",
      "train_f1_score: 0.8416686058044434\n",
      "validation loss : 0.4022137623329957\n",
      "val_accuracy: 0.846325695514679\n",
      "val_f1_score: 0.8282150626182556\n",
      "#########################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 23/50 [1:13:00<1:25:29, 189.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#########################\n",
      "training loss: 0.3649496432858355\n",
      "train_accuracy: 0.8669677376747131\n",
      "train_f1_score: 0.848456859588623\n",
      "validation loss : 0.41163728429873786\n",
      "val_accuracy: 0.8515522480010986\n",
      "val_f1_score: 0.8334006071090698\n",
      "#########################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 25/50 [1:19:20<1:19:07, 189.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#########################\n",
      "training loss: 0.36298816239921483\n",
      "train_accuracy: 0.8664419054985046\n",
      "train_f1_score: 0.8491610884666443\n",
      "validation loss : 0.35268746707836784\n",
      "val_accuracy: 0.8680956363677979\n",
      "val_f1_score: 0.8525749444961548\n",
      "#########################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 27/50 [1:25:38<1:12:40, 189.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#########################\n",
      "training loss: 0.3585445951328558\n",
      "train_accuracy: 0.8667318224906921\n",
      "train_f1_score: 0.8487405180931091\n",
      "validation loss : 0.3641689727604389\n",
      "val_accuracy: 0.8589880466461182\n",
      "val_f1_score: 0.8433343172073364\n",
      "#########################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 29/50 [1:31:58<1:06:26, 189.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#########################\n",
      "training loss: 0.34634573701900595\n",
      "train_accuracy: 0.8702887892723083\n",
      "train_f1_score: 0.8522975444793701\n",
      "validation loss : 0.36722110349933307\n",
      "val_accuracy: 0.8775210976600647\n",
      "val_f1_score: 0.8614065647125244\n",
      "#########################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 31/50 [1:38:18<1:00:06, 189.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#########################\n",
      "training loss: 0.3443183675259352\n",
      "train_accuracy: 0.8706082701683044\n",
      "train_f1_score: 0.8526430726051331\n",
      "validation loss : 0.3616951473802328\n",
      "val_accuracy: 0.8688241243362427\n",
      "val_f1_score: 0.849892258644104\n",
      "#########################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 33/50 [1:44:37<53:44, 189.70s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#########################\n",
      "training loss: 0.3373211648385314\n",
      "train_accuracy: 0.8740084171295166\n",
      "train_f1_score: 0.8561446070671082\n",
      "validation loss : 0.37292206734915573\n",
      "val_accuracy: 0.8688001036643982\n",
      "val_f1_score: 0.8525489568710327\n",
      "#########################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 35/50 [1:50:56<47:21, 189.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#########################\n",
      "training loss: 0.3311517212022753\n",
      "train_accuracy: 0.877812922000885\n",
      "train_f1_score: 0.8611069321632385\n",
      "validation loss : 0.38545399195949237\n",
      "val_accuracy: 0.8460733294487\n",
      "val_f1_score: 0.830070972442627\n",
      "#########################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 37/50 [1:57:14<41:00, 189.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#########################\n",
      "training loss: 0.3308432402794852\n",
      "train_accuracy: 0.8728024959564209\n",
      "train_f1_score: 0.8560780882835388\n",
      "validation loss : 0.3584797342568636\n",
      "val_accuracy: 0.8686927556991577\n",
      "val_f1_score: 0.851532518863678\n",
      "#########################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 39/50 [2:03:32<34:41, 189.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#########################\n",
      "training loss: 0.32038639595052776\n",
      "train_accuracy: 0.8785847425460815\n",
      "train_f1_score: 0.8621760010719299\n",
      "validation loss : 0.41938770933945974\n",
      "val_accuracy: 0.8458176255226135\n",
      "val_f1_score: 0.8295831680297852\n",
      "#########################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 41/50 [2:09:52<28:27, 189.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#########################\n",
      "training loss: 0.3182208304992493\n",
      "train_accuracy: 0.8813996315002441\n",
      "train_f1_score: 0.864874005317688\n",
      "validation loss : 0.41232798010110855\n",
      "val_accuracy: 0.8512557744979858\n",
      "val_f1_score: 0.8320695757865906\n",
      "#########################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 43/50 [2:16:11<22:06, 189.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#########################\n",
      "training loss: 0.3105670741866617\n",
      "train_accuracy: 0.8881371021270752\n",
      "train_f1_score: 0.8706713318824768\n",
      "validation loss : 0.33383782117565475\n",
      "val_accuracy: 0.8796889781951904\n",
      "val_f1_score: 0.8663960695266724\n",
      "#########################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 45/50 [2:22:30<15:47, 189.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#########################\n",
      "training loss: 0.30087869111317045\n",
      "train_accuracy: 0.8876261115074158\n",
      "train_f1_score: 0.8718485236167908\n",
      "validation loss : 0.3514584127267202\n",
      "val_accuracy: 0.8736597299575806\n",
      "val_f1_score: 0.8583224415779114\n",
      "#########################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 47/50 [2:28:49<09:28, 189.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#########################\n",
      "training loss: 0.29267055119530244\n",
      "train_accuracy: 0.8920213580131531\n",
      "train_f1_score: 0.8755929470062256\n",
      "validation loss : 0.36771172733604907\n",
      "val_accuracy: 0.8690950274467468\n",
      "val_f1_score: 0.8554518818855286\n",
      "#########################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 49/50 [2:35:07<03:09, 189.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#########################\n",
      "training loss: 0.2787494460427586\n",
      "train_accuracy: 0.8974595069885254\n",
      "train_f1_score: 0.8824065923690796\n",
      "validation loss : 0.37258491743604344\n",
      "val_accuracy: 0.8707227110862732\n",
      "val_f1_score: 0.8527376055717468\n",
      "#########################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [2:38:17<00:00, 189.94s/it]\n"
     ]
    }
   ],
   "source": [
    "import src.pytorch_modular.image_classification.engine_classification as cls\n",
    "results = cls.train_model(base_model, train_dl, val_dl, train_configuration,  \n",
    "                            log_dir=os.path.join(HOME, 'runs'),         \n",
    "                            save_path=os.path.join(HOME, 'saved_models'))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance Diagnosis\n",
    "Judging from the tensorboard output, the model reaches a training loss of 0.01 as well as an accuracy of 99.6 \\% indicating that the model's capacity fits the problem at hand. The save learning curves clearly indicates that the model overfits and overfits significantly. The divergence of the validation loss and the training loss starts from the the 20-th epoch or so. Let's start the error analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from src.pytorch_modular.pytorch_utilities import load_model\n",
    "# import src.pytorch_modular.image_classification.engine_classification as cls\n",
    "# base_model = SeqClassModel(in_features=768, hidden_size=128, num_classes=6)\n",
    "# base_model = load_model(base_model=base_model, path=os.path.join(HOME, 'saved_models', '9-18-17-53.pt'))\n",
    "\n",
    "# # let's create a dataset object really quick:\n",
    "# class TestReviewDS(Dataset):\n",
    "#     def __init__(self, data: pd.DataFrame) -> None:\n",
    "#         super().__init__()\n",
    "#         self.data = data\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.data)\n",
    "\n",
    "#     def __getitem__(self, index) -> tuple[str, int]:\n",
    "#         return self.data.iloc[index]\n",
    "\n",
    "# # we need a different callate_function\n",
    "# def test_collate_function(batch):\n",
    "#     embeddings = MODEL(**TOKENIZER(batch, padding=True, return_tensors='pt').to(DEVICE)).last_hidden_state # make sure to return tensors\n",
    "#     return embeddings.to(DEVICE)\n",
    "\n",
    "\n",
    "# validation_data = TestReviewDS(data=val_data['title'])\n",
    "# validation_loader = DataLoader(validation_data, batch_size=32, shuffle=False, collate_fn=test_collate_function)\n",
    "\n",
    "# test_ds = TestReviewDS(data=df_test)\n",
    "# test_loader = DataLoader(test_ds, batch_size=32, shuffle=False, collate_fn=test_collate_function)\n",
    "\n",
    "# # next(iter(test_loader)).shape\n",
    "# val_predictions = cls.inference(base_model, inference_source_data=validation_loader, return_tensor='list')\n",
    "# # convert the numerical labels to the string ones\n",
    "# val_predictions = [idx2cat[p] for p in val_predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(val_predictions[:5])\n",
    "# print(val_data['category'][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_labels = val_data['category'].values\n",
    "# from sklearn.metrics import confusion_matrix, f1_score, accuracy_score\n",
    "# print(accuracy_score(val_labels, val_predictions))\n",
    "# print(f1_score(val_labels, val_predictions, average='macro'))\n",
    "# confusion = confusion_matrix(val_labels, val_predictions)\n",
    "# print(confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's calculate the error rate per class\n",
    "# error_rate_per_class = 1 - confusion[list(range(len(confusion))), [i for i in range(len(confusion))]] / np.sum(confusion, axis=0)\n",
    "# print(error_rate_per_class)\n",
    "\n",
    "# # so we can see that classes 1, 2, 3 are the most problematic classes so far\n",
    "# classes_aug = [idx2cat[i] for i in [1, 2, 3]]\n",
    "# print(classes_aug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's see how we can translate a couple of sentences\n",
    "# let's increase the data by 20 percent for each of these categories\n",
    "\n",
    "# def extract_samples(category: str, ratio: float = 0.2) -> List[str]:\n",
    "#     # first filter the dataframe\n",
    "#     all = df_train_org[df_train_org['category'] == category]['title'].tolist()\n",
    "#     return random.sample(all, k=int(len(all) * ratio))\n",
    "\n",
    "# samples = [extract_samples(cat) for cat in classes_aug]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# samples[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-09-17T13:55:58.229436037Z"
    }
   },
   "outputs": [],
   "source": [
    "# let's make the damn submission\n",
    "from src.pytorch_modular.pytorch_utilities import load_model\n",
    "# base_model = SeqClassModel(in_features=768, hidden_size=128, num_classes=6)\n",
    "# base_model = load_model(base_model=base_model, path=os.path.join(HOME, 'saved_models', '9-17-15-10.pt'))\n",
    "# let's create a dataset object really quick:\n",
    "class TestReviewDS(Dataset):\n",
    "    def __init__(self, data: pd.DataFrame) -> None:\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index) -> tuple[str, int]:\n",
    "        return self.data.iloc[index, 1]\n",
    "\n",
    "# we need a different callate_function\n",
    "def test_collate_function(batch):\n",
    "    embeddings = MODEL(**TOKENIZER(batch, padding=True, return_tensors='pt', truncation=True).to(DEVICE)).last_hidden_state # make sure to return tensors\n",
    "    return embeddings.to(DEVICE)\n",
    "    \n",
    "# let's set the random seed\n",
    "\n",
    "torch.manual_seed(69)\n",
    "\n",
    "test_ds = TestReviewDS(data=df_test)\n",
    "test_loader = DataLoader(test_ds, batch_size=32, shuffle=False, collate_fn=test_collate_function)\n",
    "# next(iter(test_loader)).shape\n",
    "predictions = cls.inference(base_model, inference_source_data=test_loader, return_tensor='list')\n",
    "# convert the numerical labels to the string ones\n",
    "predictions = [idx2cat[p] for p in predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-17T13:55:58.276032280Z",
     "start_time": "2023-09-17T13:55:58.272755631Z"
    }
   },
   "outputs": [],
   "source": [
    "submission = pd.DataFrame(data={\"id\": df_test['id'].tolist(), \"Category\": predictions})\n",
    "sub_dir = os.path.join(HOME, 'submissions')\n",
    "submission.to_csv(os.path.join(sub_dir, f'sub_{len(os.listdir(sub_dir)) + 1}.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
