{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-17T13:55:21.049919950Z",
     "start_time": "2023-09-17T13:55:20.867618399Z"
    }
   },
   "outputs": [],
   "source": [
    "# let's start with the data and see how it goes\n",
    "import os\n",
    "import pandas as pd\n",
    "HOME = os.getcwd()\n",
    "train_csv = os.path.join(HOME, 'data', 'train.csv')\n",
    "test_csv = os.path.join(HOME, 'data', 'test.csv')\n",
    "\n",
    "df_train = pd.read_csv(train_csv)\n",
    "df_test = pd.read_csv(test_csv)\n",
    "# set the columns names to lower case \n",
    "\n",
    "df_train.columns = [c.lower() for c in df_train.columns]\n",
    "df_test.columns = [c.lower() for c in df_test.columns]\n",
    "\n",
    "# remove unnecessary columns\n",
    "df_train.drop(columns=['helpfulness', 'score'], inplace=True)\n",
    "df_test.drop(columns=['helpfulness', 'score'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-17T13:55:21.093064586Z",
     "start_time": "2023-09-17T13:55:21.092757Z"
    }
   },
   "outputs": [],
   "source": [
    "# add a small piece of code to call the pytorch_modular code\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "current = HOME\n",
    "while 'src' not in os.listdir(current):\n",
    "    current = Path(current).parent\n",
    "\n",
    "sys.path.append(str(current))\n",
    "sys.path.append(os.path.join(current, 'src'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-17T13:55:21.094474458Z",
     "start_time": "2023-09-17T13:55:21.092989816Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Golden Valley Natural Buffalo Jerky</td>\n",
       "      <td>The description and photo on this product need...</td>\n",
       "      <td>grocery gourmet food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Westing Game</td>\n",
       "      <td>This was a great book!!!! It is well thought t...</td>\n",
       "      <td>toys games</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Westing Game</td>\n",
       "      <td>I am a first year teacher, teaching 5th grade....</td>\n",
       "      <td>toys games</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Westing Game</td>\n",
       "      <td>I got the book at my bookfair at school lookin...</td>\n",
       "      <td>toys games</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I SPY A is For Jigsaw Puzzle 63pc</td>\n",
       "      <td>Hi! I'm Martine Redman and I created this puzz...</td>\n",
       "      <td>toys games</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 title  \\\n",
       "0  Golden Valley Natural Buffalo Jerky   \n",
       "1                         Westing Game   \n",
       "2                         Westing Game   \n",
       "3                         Westing Game   \n",
       "4    I SPY A is For Jigsaw Puzzle 63pc   \n",
       "\n",
       "                                                text              category  \n",
       "0  The description and photo on this product need...  grocery gourmet food  \n",
       "1  This was a great book!!!! It is well thought t...            toys games  \n",
       "2  I am a first year teacher, teaching 5th grade....            toys games  \n",
       "3  I got the book at my bookfair at school lookin...            toys games  \n",
       "4  Hi! I'm Martine Redman and I created this puzz...            toys games  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-17T13:55:21.096717811Z",
     "start_time": "2023-09-17T13:55:21.093288265Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>PetSafe Staywell Pet Door with Clear Hard Flap</td>\n",
       "      <td>We've only had it installed about 2 weeks. So ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Kaytee Timothy Cubes, 1-Pound</td>\n",
       "      <td>My bunny had a hard time eating this because t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Body Back Buddy</td>\n",
       "      <td>would never in a million years have guessed th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>SnackMasters California Style Turkey Jerky</td>\n",
       "      <td>Being the jerky fanatic I am, snackmasters han...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Premier Busy Buddy Tug-a-Jug Treat Dispensing ...</td>\n",
       "      <td>Wondered how quick my dog would catch on to th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                              title  \\\n",
       "0   0     PetSafe Staywell Pet Door with Clear Hard Flap   \n",
       "1   1                      Kaytee Timothy Cubes, 1-Pound   \n",
       "2   2                                    Body Back Buddy   \n",
       "3   3         SnackMasters California Style Turkey Jerky   \n",
       "4   4  Premier Busy Buddy Tug-a-Jug Treat Dispensing ...   \n",
       "\n",
       "                                                text  \n",
       "0  We've only had it installed about 2 weeks. So ...  \n",
       "1  My bunny had a hard time eating this because t...  \n",
       "2  would never in a million years have guessed th...  \n",
       "3  Being the jerky fanatic I am, snackmasters han...  \n",
       "4  Wondered how quick my dog would catch on to th...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-17T13:55:21.096915401Z",
     "start_time": "2023-09-17T13:55:21.093470176Z"
    }
   },
   "outputs": [],
   "source": [
    "import nltk \n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "try:\n",
    "    STOP_WORDS = list(stopwords.words('english'))\n",
    "except LookupError:\n",
    "    nltk.download('stopwords')\n",
    "    STOP_WORDS = list(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-17T13:55:21.097212598Z",
     "start_time": "2023-09-17T13:55:21.093713953Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "See the title of this review. Fortunately, I am a packrat, and kept a bunch of hole repair kits from various blow up things that we have gone through over the years. Does not come with a hole repair kit though, just to warn you. Anyway, it is back in black and bouncing our 3 year old all over the place. Indoor only, I would say. Very highly recommended, in spite of a hole within a week of use. Hope that this is the first and last one... probably not.\n",
      "see title review . fortunately , packrat , kept bunch hole repair kits various blow things gone years . come hole repair kit though , warn . anyway , back black bouncing year old place . indoor , would say . highly recommended , spite hole within week use . hope first last one ... probably .\n"
     ]
    }
   ],
   "source": [
    "# preprocessing functions\n",
    "import re\n",
    "from typing import List, Tuple\n",
    "\n",
    "def to_lower(text: str) -> str:\n",
    "    return text.lower()\n",
    "\n",
    "def no_extra_spaces(text: str) -> str:\n",
    "    return re.sub('\\s+', ' ', text)\n",
    "\n",
    "def no_extra_chars(text: str) -> str:\n",
    "    return re.sub(r'[^a-zA-Z\\s,!.;:-]+', ' ', text) \n",
    "\n",
    "text = 'aaa5531--==-||\"z2::,.a'\n",
    "\n",
    "def remove_stop_words(text: str,\n",
    "                      tokenizer: TweetTokenizer = None) -> str:\n",
    "    text = to_lower(text)    \n",
    "    tokenizer = TweetTokenizer() if tokenizer is None else tokenizer\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    # if the remove_stop_words argument is set to True, then filter stop words\n",
    "    tokens = [t.strip() for t in tokens if t not in STOP_WORDS] \n",
    "    return \" \".join(tokens)\n",
    "\n",
    "def process(text: str) -> str:\n",
    "    # first lower, remove extrac chracters\n",
    "    text1 = to_lower(no_extra_chars(text))\n",
    "    # remove redundant words\n",
    "    text2 = remove_stop_words(text1)\n",
    "    # remove extra spaces\n",
    "    return no_extra_spaces(text2)\n",
    "\n",
    "import random\n",
    "random.seed(69)\n",
    "example = df_train['text'][int(random.random() * len(df_train))]\n",
    "print(example)\n",
    "print(process(example))\n",
    "\n",
    "# # drop the 'text' column as only the title will be used for classification\n",
    "df_train.drop(columns=['text'], inplace=True)\n",
    "df_test.drop(columns=['text'], inplace=True)\n",
    "\n",
    "# 16 rows have missing values in the 'title' column, remove them\n",
    "df_train.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-17T13:55:21.097504435Z",
     "start_time": "2023-09-17T13:55:21.093860678Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title       0\n",
      "category    0\n",
      "dtype: int64\n",
      "####################################################################################################\n",
      "id       0\n",
      "title    5\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df_train.isna().sum())\n",
    "print(\"#\" * 100)\n",
    "print(df_test.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-17T13:55:28.263882527Z",
     "start_time": "2023-09-17T13:55:21.093991142Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "cat2idx = {\n",
    "    'toys games': 0,\n",
    "    'health personal care': 1,\n",
    "    'beauty': 2,\n",
    "    'baby products': 3,\n",
    "    'pet supplies': 4,\n",
    "    'grocery gourmet food': 5,\n",
    "}\n",
    "\n",
    "idx2cat = {\n",
    "    0:'toys games',\n",
    "    1:'health personal care',\n",
    "    2:'beauty',\n",
    "    3:'baby products',\n",
    "    4:'pet supplies',\n",
    "    5:'grocery gourmet food' \n",
    "}\n",
    "\n",
    "# making sure the dataframes are ready for training\n",
    "def df_process_data(row):\n",
    "    if isinstance(row['title'], float):\n",
    "        row['title'] = random.choice(list(cat2idx.keys()))\n",
    "        return row\n",
    "    row['title'] = process(row['title'])\n",
    "    return row\n",
    "\n",
    "def df_process_labels(row):\n",
    "    row['category'] = process(row['category'])\n",
    "    # map it to an integer\n",
    "    row['category'] = cat2idx[row['category']]\n",
    "    return row\n",
    "\n",
    "# process the fields\n",
    "df_train = df_train.apply(df_process_data, axis=1)\n",
    "# process the labels\n",
    "df_train = df_train.apply(df_process_labels, axis=1)\n",
    "# process the data is the test split\n",
    "df_test = df_test.apply(df_process_data, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-17T13:55:28.278018893Z",
     "start_time": "2023-09-17T13:55:28.265548448Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_data, val_data = train_test_split(df_train, test_size=0.15, stratify=df_train['category'], random_state=69)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-17T13:55:29.050319218Z",
     "start_time": "2023-09-17T13:55:28.277831762Z"
    }
   },
   "outputs": [],
   "source": [
    "# in the rest of the code I will be using the d\n",
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "CHECKPOINT = 'distilbert-base-uncased' # let's keep it simple as for the first iteration\n",
    "MODEL = AutoModel.from_pretrained(CHECKPOINT).to(DEVICE)\n",
    "TOKENIZER = AutoTokenizer.from_pretrained(CHECKPOINT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-17T13:55:29.093210680Z",
     "start_time": "2023-09-17T13:55:29.092817012Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "def collate_function(batch: List[str]):\n",
    "    # batch will represent a list of tuples (text, category) \n",
    "    x, y = [list(row) for row in zip(*batch)]\n",
    "    # convert both labels and data to tensors\n",
    "    y_tensor = torch.FloatTensor(y).to(device=DEVICE)\n",
    "    embeddings = MODEL(**TOKENIZER(x, padding=True, return_tensors='pt').to(DEVICE)).last_hidden_state # make sure to return tensors\n",
    "    return embeddings.to(DEVICE), y_tensor\n",
    "\n",
    "# let's create a dataset object really quick:\n",
    "class LabeledReviewDS(Dataset):\n",
    "    def __init__(self, data: pd.DataFrame) -> None:\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index) -> Tuple[str, int]:\n",
    "        return tuple(self.data.iloc[index, :2])\n",
    "\n",
    "# let's set the random seed\n",
    "\n",
    "torch.manual_seed(69)\n",
    "\n",
    "train_ds = LabeledReviewDS(train_data)\n",
    "val_ds = LabeledReviewDS(val_data)\n",
    "\n",
    "# create the dataloaders\n",
    "train_dl = DataLoader(dataset=train_ds, batch_size=32, shuffle=True, collate_fn=collate_function, drop_last=True)\n",
    "val_dl = DataLoader(dataset=val_ds, batch_size=32, shuffle=False, collate_fn=collate_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-17T13:55:29.093530619Z",
     "start_time": "2023-09-17T13:55:29.093114069Z"
    }
   },
   "outputs": [],
   "source": [
    "# next(iter(train_dl))\n",
    "# looks our data is loaded and ready to go, time to build a model!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train A model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-17T13:55:29.094325168Z",
     "start_time": "2023-09-17T13:55:29.093361853Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.nn.functional import leaky_relu\n",
    "\n",
    "class SeqClassModel(nn.Module):\n",
    "    def __init__(self, \n",
    "                in_features: int,\n",
    "                hidden_size: int, \n",
    "                num_classes: int, \n",
    "                num_layers: int = 2, \n",
    "                dropout: float=0.25, \n",
    "                *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.output_units = num_classes if num_classes > 2 else 1\n",
    "        self.rnn = nn.LSTM(input_size=in_features, \n",
    "                           hidden_size=hidden_size, \n",
    "                           dropout=dropout, \n",
    "                           num_layers=num_layers,\n",
    "                           bidirectional=True, # bidiretional RNN are more powerful\n",
    "                           batch_first=True # easier manipulation\n",
    "                           )\n",
    "\n",
    "        self.batch_layer= nn.BatchNorm1d(num_features=2 * hidden_size)\n",
    "        self.relu_layer = nn.LeakyReLU()\n",
    "        self.head = nn.Linear(in_features=2 * hidden_size, out_features=self.output_units)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        # first pass it through the rnn\n",
    "        _, (hidden_state, _) = self.rnn(x)\n",
    "        batch_size = hidden_state.shape[1]\n",
    "        hidden_state = hidden_state.permute((1, 0, 2))[:, -2:, :].reshape((batch_size, -1))\n",
    "        # # first permuting channels: batch_size as dimensions '0' \n",
    "        # hidden_state = hidden_state.permute((1, 0, 2))\n",
    "        # # only only the last lstm layer\n",
    "        # hidden_state = hidden_state[:, -2:, :]\n",
    "        # hidden_state = hidden_state.reshape((batch_size, -1))\n",
    "        return self.head.forward(self.relu_layer(self.batch_layer(hidden_state)))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-17T13:55:29.094486140Z",
     "start_time": "2023-09-17T13:55:29.093505082Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import LinearLR\n",
    "from torchmetrics.classification import MulticlassF1Score, MulticlassAccuracy\n",
    "\n",
    "base_model = SeqClassModel(in_features=768, hidden_size=128, num_classes=6)\n",
    "optimizer = AdamW(base_model.parameters(), lr=0.05)\n",
    "scheduler = LinearLR(optimizer, start_factor=1.0, end_factor=0.005, total_iters=100)\n",
    "\n",
    "accuracy_metric, f1_metric = MulticlassAccuracy(num_classes=6), MulticlassF1Score(num_classes=6)\n",
    "\n",
    "metrics = {'accuracy': accuracy_metric, 'f1_score': f1_metric}\n",
    "\n",
    "train_configuration = {'optimizer': optimizer,\n",
    "                        'scheduler': scheduler,\n",
    "                        'min_val_loss': 10 ** -4,\n",
    "                        'max_epochs': 100,\n",
    "                        'report_epoch': 10,\n",
    "                        'device': DEVICE, \n",
    "                        'metrics': metrics\n",
    "                        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-17T13:55:58.229534101Z",
     "start_time": "2023-09-17T13:55:29.093614286Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Created SummaryWriter, saving to: /home/user/ayhem/My_Kaggle_Repo/amazon_reviews/runs/experience_10...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:10<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/user/ayhem/My_Kaggle_Repo/amazon_reviews/attempt1.ipynb Cell 19\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B10.90.123.140/home/user/ayhem/My_Kaggle_Repo/amazon_reviews/attempt1.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39msrc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpytorch_modular\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mimage_classification\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mengine_classification\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mcls\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B10.90.123.140/home/user/ayhem/My_Kaggle_Repo/amazon_reviews/attempt1.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_model(base_model, train_dl, val_dl, train_configuration,  \n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B10.90.123.140/home/user/ayhem/My_Kaggle_Repo/amazon_reviews/attempt1.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m                             log_dir\u001b[39m=\u001b[39;49mos\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mjoin(HOME, \u001b[39m'\u001b[39;49m\u001b[39mruns\u001b[39;49m\u001b[39m'\u001b[39;49m),         \n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B10.90.123.140/home/user/ayhem/My_Kaggle_Repo/amazon_reviews/attempt1.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m                             save_path\u001b[39m=\u001b[39;49mos\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mjoin(HOME, \u001b[39m'\u001b[39;49m\u001b[39msaved_models\u001b[39;49m\u001b[39m'\u001b[39;49m))   \n",
      "File \u001b[0;32m~/ayhem/My_Kaggle_Repo/src/pytorch_modular/image_classification/engine_classification.py:200\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_dataloader, test_dataloader, train_configuration, log_dir, save_path)\u001b[0m\n\u001b[1;32m    196\u001b[0m writer \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39mif\u001b[39;00m log_dir \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m create_summary_writer(log_dir)\n\u001b[1;32m    198\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m tqdm(\u001b[39mrange\u001b[39m(train_configuration[ut\u001b[39m.\u001b[39mMAX_EPOCHS])):\n\u001b[0;32m--> 200\u001b[0m     epoch_train_metrics \u001b[39m=\u001b[39m train_per_epoch(model\u001b[39m=\u001b[39;49mmodel,\n\u001b[1;32m    201\u001b[0m                                           train_dataloader\u001b[39m=\u001b[39;49mtrain_dataloader,\n\u001b[1;32m    202\u001b[0m                                           loss_function\u001b[39m=\u001b[39;49mtrain_configuration[ut\u001b[39m.\u001b[39;49mLOSS_FUNCTION],\n\u001b[1;32m    203\u001b[0m                                           optimizer\u001b[39m=\u001b[39;49mtrain_configuration[ut\u001b[39m.\u001b[39;49mOPTIMIZER],\n\u001b[1;32m    204\u001b[0m                                           output_layer\u001b[39m=\u001b[39;49mtrain_configuration[ut\u001b[39m.\u001b[39;49mOUTPUT_LAYER],\n\u001b[1;32m    205\u001b[0m                                           scheduler\u001b[39m=\u001b[39;49mtrain_configuration[ut\u001b[39m.\u001b[39;49mSCHEDULER],\n\u001b[1;32m    206\u001b[0m                                           device\u001b[39m=\u001b[39;49mtrain_configuration[ut\u001b[39m.\u001b[39;49mDEVICE],\n\u001b[1;32m    207\u001b[0m                                           debug\u001b[39m=\u001b[39;49mtrain_configuration[ut\u001b[39m.\u001b[39;49mDEBUG], \n\u001b[1;32m    208\u001b[0m                                           metrics\u001b[39m=\u001b[39;49mtrain_configuration[ut\u001b[39m.\u001b[39;49mMETRICS])\n\u001b[1;32m    210\u001b[0m     epoch_val_metrics \u001b[39m=\u001b[39m val_per_epoch(model\u001b[39m=\u001b[39mmodel, dataloader\u001b[39m=\u001b[39mtest_dataloader,\n\u001b[1;32m    211\u001b[0m                                       loss_function\u001b[39m=\u001b[39mtrain_configuration[ut\u001b[39m.\u001b[39mLOSS_FUNCTION],\n\u001b[1;32m    212\u001b[0m                                       output_layer\u001b[39m=\u001b[39mtrain_configuration[ut\u001b[39m.\u001b[39mOUTPUT_LAYER],\n\u001b[1;32m    213\u001b[0m                                       device\u001b[39m=\u001b[39mtrain_configuration[ut\u001b[39m.\u001b[39mDEVICE],\n\u001b[1;32m    214\u001b[0m                                       debug\u001b[39m=\u001b[39mtrain_configuration[ut\u001b[39m.\u001b[39mDEBUG], \n\u001b[1;32m    215\u001b[0m                                       metrics\u001b[39m=\u001b[39mtrain_configuration[ut\u001b[39m.\u001b[39mMETRICS])\n\u001b[1;32m    217\u001b[0m     epoch_train_loss \u001b[39m=\u001b[39m epoch_train_metrics[ut\u001b[39m.\u001b[39mTRAIN_LOSS]\n",
      "File \u001b[0;32m~/ayhem/My_Kaggle_Repo/src/pytorch_modular/image_classification/epoch_engine.py:107\u001b[0m, in \u001b[0;36mtrain_per_epoch\u001b[0;34m(model, train_dataloader, loss_function, optimizer, output_layer, scheduler, device, metrics, debug)\u001b[0m\n\u001b[1;32m    102\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    103\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mAn extra dimension has been added to the labels vectors\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    104\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mold shape: \u001b[39m\u001b[39m{\u001b[39;00my\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m, new shape: \u001b[39m\u001b[39m{\u001b[39;00mnew_y\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    105\u001b[0m     batch_loss \u001b[39m=\u001b[39m loss_function(y_pred, new_y)\n\u001b[0;32m--> 107\u001b[0m train_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m batch_loss\u001b[39m.\u001b[39;49mitem()\n\u001b[1;32m    108\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m    109\u001b[0m batch_loss\u001b[39m.\u001b[39mbackward()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import src.pytorch_modular.image_classification.engine_classification as cls\n",
    "\n",
    "results = cls.train_model(base_model, train_dl, val_dl, train_configuration,  \n",
    "                            log_dir=os.path.join(HOME, 'runs'),         \n",
    "                            save_path=os.path.join(HOME, 'saved_models'))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-09-17T13:55:58.229436037Z"
    }
   },
   "outputs": [],
   "source": [
    "# let's make the damn submission\n",
    "from src.pytorch_modular.pytorch_utilities import load_model\n",
    "# base_model = SeqClassModel(in_features=768, hidden_size=128, num_classes=6)\n",
    "# base_model = load_model(base_model=base_model, path=os.path.join(HOME, 'saved_models', '9-17-15-10.pt'))\n",
    "# let's create a dataset object really quick:\n",
    "class TestReviewDS(Dataset):\n",
    "    def __init__(self, data: pd.DataFrame) -> None:\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index) -> Tuple[str, int]:\n",
    "        return self.data.iloc[index, 1]\n",
    "\n",
    "# we need a different callate_function\n",
    "def test_collate_function(batch):\n",
    "    embeddings = MODEL(**TOKENIZER(batch, padding=True, return_tensors='pt').to(DEVICE)).last_hidden_state # make sure to return tensors\n",
    "    return embeddings.to(DEVICE)\n",
    "    \n",
    "# let's set the random seed\n",
    "\n",
    "torch.manual_seed(69)\n",
    "\n",
    "test_ds = TestReviewDS(data=df_test)\n",
    "test_loader = DataLoader(test_ds, batch_size=32, shuffle=False, collate_fn=test_collate_function)\n",
    "# next(iter(test_loader)).shape\n",
    "predictions = cls.inference(base_model, inference_source_data=test_loader, return_tensor='list')\n",
    "# convert the numerical labels to the string ones\n",
    "predictions = [idx2cat[p] for p in predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-17T13:55:58.276032280Z",
     "start_time": "2023-09-17T13:55:58.272755631Z"
    }
   },
   "outputs": [],
   "source": [
    "submission = pd.DataFrame(data={\"id\": df_test['id'].tolist(), \"Category\": predictions})\n",
    "sub_dir = os.path.join(HOME, 'submissions')\n",
    "submission.to_csv(os.path.join(sub_dir, f'sub_{len(os.listdir(sub_dir)) + 1}.csv'), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
