{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-17T13:55:21.049919950Z",
     "start_time": "2023-09-17T13:55:20.867618399Z"
    }
   },
   "outputs": [],
   "source": [
    "# let's start with the data and see how it goes\n",
    "import os\n",
    "import pandas as pd\n",
    "HOME = os.getcwd()\n",
    "train_csv = os.path.join(HOME, 'data', 'train.csv')\n",
    "test_csv = os.path.join(HOME, 'data', 'test.csv')\n",
    "\n",
    "df_train = pd.read_csv(train_csv)\n",
    "df_test = pd.read_csv(test_csv)\n",
    "# set the columns names to lower case \n",
    "\n",
    "df_train.columns = [c.lower() for c in df_train.columns]\n",
    "df_test.columns = [c.lower() for c in df_test.columns]\n",
    "\n",
    "# remove unnecessary columns\n",
    "df_train.drop(columns=['helpfulness', 'score'], inplace=True)\n",
    "df_test.drop(columns=['helpfulness', 'score'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-17T13:55:21.093064586Z",
     "start_time": "2023-09-17T13:55:21.092757Z"
    }
   },
   "outputs": [],
   "source": [
    "# add a small piece of code to call the pytorch_modular code\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "current = HOME\n",
    "while 'src' not in os.listdir(current):\n",
    "    current = Path(current).parent\n",
    "\n",
    "sys.path.append(str(current))\n",
    "sys.path.append(os.path.join(current, 'src'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-17T13:55:21.094474458Z",
     "start_time": "2023-09-17T13:55:21.092989816Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Golden Valley Natural Buffalo Jerky</td>\n",
       "      <td>The description and photo on this product need...</td>\n",
       "      <td>grocery gourmet food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Westing Game</td>\n",
       "      <td>This was a great book!!!! It is well thought t...</td>\n",
       "      <td>toys games</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Westing Game</td>\n",
       "      <td>I am a first year teacher, teaching 5th grade....</td>\n",
       "      <td>toys games</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Westing Game</td>\n",
       "      <td>I got the book at my bookfair at school lookin...</td>\n",
       "      <td>toys games</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I SPY A is For Jigsaw Puzzle 63pc</td>\n",
       "      <td>Hi! I'm Martine Redman and I created this puzz...</td>\n",
       "      <td>toys games</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 title  \\\n",
       "0  Golden Valley Natural Buffalo Jerky   \n",
       "1                         Westing Game   \n",
       "2                         Westing Game   \n",
       "3                         Westing Game   \n",
       "4    I SPY A is For Jigsaw Puzzle 63pc   \n",
       "\n",
       "                                                text              category  \n",
       "0  The description and photo on this product need...  grocery gourmet food  \n",
       "1  This was a great book!!!! It is well thought t...            toys games  \n",
       "2  I am a first year teacher, teaching 5th grade....            toys games  \n",
       "3  I got the book at my bookfair at school lookin...            toys games  \n",
       "4  Hi! I'm Martine Redman and I created this puzz...            toys games  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-17T13:55:21.096717811Z",
     "start_time": "2023-09-17T13:55:21.093288265Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>PetSafe Staywell Pet Door with Clear Hard Flap</td>\n",
       "      <td>We've only had it installed about 2 weeks. So ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Kaytee Timothy Cubes, 1-Pound</td>\n",
       "      <td>My bunny had a hard time eating this because t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Body Back Buddy</td>\n",
       "      <td>would never in a million years have guessed th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>SnackMasters California Style Turkey Jerky</td>\n",
       "      <td>Being the jerky fanatic I am, snackmasters han...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Premier Busy Buddy Tug-a-Jug Treat Dispensing ...</td>\n",
       "      <td>Wondered how quick my dog would catch on to th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                              title  \\\n",
       "0   0     PetSafe Staywell Pet Door with Clear Hard Flap   \n",
       "1   1                      Kaytee Timothy Cubes, 1-Pound   \n",
       "2   2                                    Body Back Buddy   \n",
       "3   3         SnackMasters California Style Turkey Jerky   \n",
       "4   4  Premier Busy Buddy Tug-a-Jug Treat Dispensing ...   \n",
       "\n",
       "                                                text  \n",
       "0  We've only had it installed about 2 weeks. So ...  \n",
       "1  My bunny had a hard time eating this because t...  \n",
       "2  would never in a million years have guessed th...  \n",
       "3  Being the jerky fanatic I am, snackmasters han...  \n",
       "4  Wondered how quick my dog would catch on to th...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-17T13:55:21.096915401Z",
     "start_time": "2023-09-17T13:55:21.093470176Z"
    }
   },
   "outputs": [],
   "source": [
    "import nltk \n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "try:\n",
    "    STOP_WORDS = list(stopwords.words('english'))\n",
    "except LookupError:\n",
    "    nltk.download('stopwords')\n",
    "    STOP_WORDS = list(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-17T13:55:21.097212598Z",
     "start_time": "2023-09-17T13:55:21.093713953Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "See the title of this review. Fortunately, I am a packrat, and kept a bunch of hole repair kits from various blow up things that we have gone through over the years. Does not come with a hole repair kit though, just to warn you. Anyway, it is back in black and bouncing our 3 year old all over the place. Indoor only, I would say. Very highly recommended, in spite of a hole within a week of use. Hope that this is the first and last one... probably not.\n",
      "see title review . fortunately , packrat , kept bunch hole repair kits various blow things gone years . come hole repair kit though , warn . anyway , back black bouncing year old place . indoor , would say . highly recommended , spite hole within week use . hope first last one ... probably .\n"
     ]
    }
   ],
   "source": [
    "# preprocessing functions\n",
    "import re\n",
    "from typing import List\n",
    "\n",
    "def to_lower(text: str) -> str:\n",
    "    return text.lower()\n",
    "\n",
    "def no_extra_spaces(text: str) -> str:\n",
    "    return re.sub('\\s+', ' ', text)\n",
    "\n",
    "def no_extra_chars(text: str) -> str:\n",
    "    return re.sub(r'[^a-zA-Z\\s,!.;:-]+', ' ', text) \n",
    "\n",
    "text = 'aaa5531--==-||\"z2::,.a'\n",
    "\n",
    "def remove_stop_words(text: str,\n",
    "                      tokenizer: TweetTokenizer = None) -> str:\n",
    "    text = to_lower(text)    \n",
    "    tokenizer = TweetTokenizer() if tokenizer is None else tokenizer\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    # if the remove_stop_words argument is set to True, then filter stop words\n",
    "    tokens = [t.strip() for t in tokens if t not in STOP_WORDS] \n",
    "    return \" \".join(tokens)\n",
    "\n",
    "def process(text: str) -> str:\n",
    "    # first lower, remove extrac chracters\n",
    "    text1 = to_lower(no_extra_chars(text))\n",
    "    # remove redundant words\n",
    "    text2 = remove_stop_words(text1)\n",
    "    # remove extra spaces\n",
    "    return no_extra_spaces(text2)\n",
    "\n",
    "import random\n",
    "random.seed(69)\n",
    "example = df_train['text'][int(random.random() * len(df_train))]\n",
    "print(example)\n",
    "print(process(example))\n",
    "\n",
    "# # drop the 'text' column as only the title will be used for classification\n",
    "df_train.drop(columns=['text'], inplace=True)\n",
    "df_test.drop(columns=['text'], inplace=True)\n",
    "\n",
    "# 16 rows have missing values in the 'title' column, remove them\n",
    "df_train.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-17T13:55:21.097504435Z",
     "start_time": "2023-09-17T13:55:21.093860678Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title       0\n",
      "category    0\n",
      "dtype: int64\n",
      "####################################################################################################\n",
      "id       0\n",
      "title    5\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df_train.isna().sum())\n",
    "print(\"#\" * 100)\n",
    "print(df_test.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-17T13:55:28.263882527Z",
     "start_time": "2023-09-17T13:55:21.093991142Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "cat2idx = {\n",
    "    'toys games': 0,\n",
    "    'health personal care': 1,\n",
    "    'beauty': 2,\n",
    "    'baby products': 3,\n",
    "    'pet supplies': 4,\n",
    "    'grocery gourmet food': 5,\n",
    "}\n",
    "\n",
    "idx2cat = {\n",
    "    0:'toys games',\n",
    "    1:'health personal care',\n",
    "    2:'beauty',\n",
    "    3:'baby products',\n",
    "    4:'pet supplies',\n",
    "    5:'grocery gourmet food' \n",
    "}\n",
    "\n",
    "# making sure the dataframes are ready for training\n",
    "def df_process_data(row):\n",
    "    if isinstance(row['title'], float):\n",
    "        row['title'] = random.choice(list(cat2idx.keys()))\n",
    "        return row\n",
    "    row['title'] = process(row['title'])\n",
    "    return row\n",
    "\n",
    "def df_process_labels(row):\n",
    "    row['category'] = process(row['category'])\n",
    "    # map it to an integer\n",
    "    row['category'] = cat2idx[row['category']]\n",
    "    return row\n",
    "\n",
    "# process the fields\n",
    "df_train = df_train.apply(df_process_data, axis=1)\n",
    "# process the labels\n",
    "df_train = df_train.apply(df_process_labels, axis=1)\n",
    "# process the data is the test split\n",
    "df_test = df_test.apply(df_process_data, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-17T13:55:28.278018893Z",
     "start_time": "2023-09-17T13:55:28.265548448Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_data, val_data = train_test_split(df_train, test_size=0.15, stratify=df_train['category'], random_state=69)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-17T13:55:29.050319218Z",
     "start_time": "2023-09-17T13:55:28.277831762Z"
    }
   },
   "outputs": [],
   "source": [
    "# in the rest of the code I will be using the d\n",
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "CHECKPOINT = 'distilbert-base-uncased' # let's keep it simple as for the first iteration\n",
    "MODEL = AutoModel.from_pretrained(CHECKPOINT).to(DEVICE)\n",
    "TOKENIZER = AutoTokenizer.from_pretrained(CHECKPOINT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-17T13:55:29.093210680Z",
     "start_time": "2023-09-17T13:55:29.092817012Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "def collate_function(batch: List[str]):\n",
    "    # batch will represent a list of tuples (text, category) \n",
    "    x, y = [list(row) for row in zip(*batch)]\n",
    "    # convert both labels and data to tensors\n",
    "    y_tensor = torch.FloatTensor(y).to(device=DEVICE)\n",
    "    embeddings = MODEL(**TOKENIZER(x, padding=True, return_tensors='pt').to(DEVICE)).last_hidden_state # make sure to return tensors\n",
    "    return embeddings.to(DEVICE), y_tensor\n",
    "\n",
    "# let's create a dataset object really quick:\n",
    "class LabeledReviewDS(Dataset):\n",
    "    def __init__(self, data: pd.DataFrame) -> None:\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index) -> tuple[str, int]:\n",
    "        return tuple(self.data.iloc[index, :2])\n",
    "\n",
    "# let's set the random seed\n",
    "\n",
    "torch.manual_seed(69)\n",
    "\n",
    "train_ds = LabeledReviewDS(train_data)\n",
    "val_ds = LabeledReviewDS(val_data)\n",
    "\n",
    "# create the dataloaders\n",
    "train_dl = DataLoader(dataset=train_ds, batch_size=32, shuffle=True, collate_fn=collate_function, drop_last=True)\n",
    "val_dl = DataLoader(dataset=val_ds, batch_size=32, shuffle=False, collate_fn=collate_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-17T13:55:29.093530619Z",
     "start_time": "2023-09-17T13:55:29.093114069Z"
    }
   },
   "outputs": [],
   "source": [
    "# next(iter(train_dl))\n",
    "# looks our data is loaded and ready to go, time to build a model!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train A model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-17T13:55:29.094325168Z",
     "start_time": "2023-09-17T13:55:29.093361853Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.nn.functional import leaky_relu\n",
    "\n",
    "class SeqClassModel(nn.Module):\n",
    "    def __init__(self, \n",
    "                in_features: int,\n",
    "                hidden_size: int, \n",
    "                num_classes: int, \n",
    "                num_layers: int = 2, \n",
    "                dropout: float=0.25, \n",
    "                *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.output_units = num_classes if num_classes > 2 else 1\n",
    "        self.rnn = nn.LSTM(input_size=in_features, \n",
    "                           hidden_size=hidden_size, \n",
    "                           dropout=dropout, \n",
    "                           num_layers=num_layers,\n",
    "                           bidirectional=True, # bidiretional RNN are more powerful\n",
    "                           batch_first=True # easier manipulation\n",
    "                           )\n",
    "        # 2: comes from the fact that the lstm is bidirectional, the rest is similar to the LSTM documention Pytorch\n",
    "        linear_input_dim = 2 * num_layers * hidden_size \n",
    "        self.batch_layer= nn.BatchNorm1d(num_features=linear_input_dim)\n",
    "        # self.relu_layer = nn.LeakyReLU()\n",
    "        self.head = nn.Linear(in_features=linear_input_dim, out_features=self.output_units)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        # first pass it through the rnn\n",
    "        _, (hidden_state, _) = self.rnn(x)\n",
    "        batch_size = hidden_state.shape[1]\n",
    "        # first permuting channels: batch_size as dimensions '0' \n",
    "        # only only the last lstm layer\n",
    "        hidden_state = hidden_state.permute((1, 0, 2)).reshape((batch_size, -1))\n",
    "        return self.head.forward(self.batch_layer(hidden_state))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-17T13:55:29.094486140Z",
     "start_time": "2023-09-17T13:55:29.093505082Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import LinearLR\n",
    "from torchmetrics.classification import MulticlassF1Score, MulticlassAccuracy\n",
    "\n",
    "base_model = SeqClassModel(in_features=768, hidden_size=128, num_classes=6)\n",
    "optimizer = AdamW(base_model.parameters(), lr=0.01)\n",
    "scheduler = LinearLR(optimizer, start_factor=1.0, end_factor=0.005, total_iters=100)\n",
    "\n",
    "accuracy_metric, f1_metric = MulticlassAccuracy(num_classes=6), MulticlassF1Score(num_classes=6)\n",
    "\n",
    "metrics = {'accuracy': accuracy_metric, 'f1_score': f1_metric}\n",
    "\n",
    "train_configuration = {'optimizer': optimizer,\n",
    "                        'scheduler': scheduler,\n",
    "                        'min_val_loss': 10 ** -4,\n",
    "                        'max_epochs': 100,\n",
    "                        'report_epoch': 5,\n",
    "                        'device': DEVICE, \n",
    "                        'metrics': metrics,\n",
    "                        'no_improve_stop': 30\n",
    "                        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-17T13:55:58.229534101Z",
     "start_time": "2023-09-17T13:55:29.093614286Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Created SummaryWriter, saving to: /home/ayhem18/DEV/My_Kaggle_Repo/amazon_reviews/runs/experience_8...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/100 [00:52<1:25:53, 52.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#########################\n",
      "training loss: 0.6558893404325747\n",
      "train_accuracy: 0.7721792459487915\n",
      "train_f1_score: 0.7497310638427734\n",
      "validation loss : 0.49693129116550405\n",
      "val_accuracy: 0.8303143382072449\n",
      "val_f1_score: 0.8115997910499573\n",
      "#########################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 6/100 [05:15<1:22:36, 52.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#########################\n",
      "training loss: 0.33543459741353093\n",
      "train_accuracy: 0.8898731470108032\n",
      "train_f1_score: 0.8712399005889893\n",
      "validation loss : 0.3558555673411552\n",
      "val_accuracy: 0.8865804076194763\n",
      "val_f1_score: 0.8709324598312378\n",
      "#########################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 11/100 [09:40<1:18:31, 52.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#########################\n",
      "training loss: 0.28026878331102106\n",
      "train_accuracy: 0.9068214297294617\n",
      "train_f1_score: 0.8924185037612915\n",
      "validation loss : 0.3465685739201751\n",
      "val_accuracy: 0.8956265449523926\n",
      "val_f1_score: 0.8800507187843323\n",
      "#########################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 16/100 [14:05<1:14:09, 52.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#########################\n",
      "training loss: 0.24023865084751403\n",
      "train_accuracy: 0.9250503182411194\n",
      "train_f1_score: 0.9102699756622314\n",
      "validation loss : 0.37825810049284014\n",
      "val_accuracy: 0.8886847496032715\n",
      "val_f1_score: 0.8720353245735168\n",
      "#########################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 21/100 [18:28<1:09:24, 52.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#########################\n",
      "training loss: 0.2081629899479519\n",
      "train_accuracy: 0.9302944540977478\n",
      "train_f1_score: 0.9165394902229309\n",
      "validation loss : 0.3073223424837627\n",
      "val_accuracy: 0.9077926278114319\n",
      "val_f1_score: 0.8932238221168518\n",
      "#########################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 26/100 [22:51<1:04:48, 52.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#########################\n",
      "training loss: 0.19074603160246925\n",
      "train_accuracy: 0.937451958656311\n",
      "train_f1_score: 0.9255355596542358\n",
      "validation loss : 0.3109566688715936\n",
      "val_accuracy: 0.9085302948951721\n",
      "val_f1_score: 0.8967682719230652\n",
      "#########################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 31/100 [27:14<1:00:33, 52.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#########################\n",
      "training loss: 0.17182372456149894\n",
      "train_accuracy: 0.9445618391036987\n",
      "train_f1_score: 0.9313549995422363\n",
      "validation loss : 0.3265013951471353\n",
      "val_accuracy: 0.9078767895698547\n",
      "val_f1_score: 0.8945206999778748\n",
      "#########################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 36/100 [31:38<56:07, 52.62s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#########################\n",
      "training loss: 0.14859041175235013\n",
      "train_accuracy: 0.9520153999328613\n",
      "train_f1_score: 0.9418550729751587\n",
      "validation loss : 0.29632890879354895\n",
      "val_accuracy: 0.9132164716720581\n",
      "val_f1_score: 0.9005258679389954\n",
      "#########################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 41/100 [36:01<51:47, 52.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#########################\n",
      "training loss: 0.13002125095583064\n",
      "train_accuracy: 0.9565749168395996\n",
      "train_f1_score: 0.9474289417266846\n",
      "validation loss : 0.3078711375524785\n",
      "val_accuracy: 0.909719705581665\n",
      "val_f1_score: 0.8984768986701965\n",
      "#########################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 46/100 [40:24<47:19, 52.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#########################\n",
      "training loss: 0.11265406316442664\n",
      "train_accuracy: 0.9635230898857117\n",
      "train_f1_score: 0.9545397758483887\n",
      "validation loss : 0.30427132342803354\n",
      "val_accuracy: 0.9247327446937561\n",
      "val_f1_score: 0.9082689881324768\n",
      "#########################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 51/100 [44:46<42:50, 52.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#########################\n",
      "training loss: 0.1007115059760671\n",
      "train_accuracy: 0.9663552641868591\n",
      "train_f1_score: 0.9578993320465088\n",
      "validation loss : 0.30731326078703786\n",
      "val_accuracy: 0.9181804656982422\n",
      "val_f1_score: 0.9109286665916443\n",
      "#########################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 56/100 [49:09<38:32, 52.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#########################\n",
      "training loss: 0.08895624943045556\n",
      "train_accuracy: 0.9690596461296082\n",
      "train_f1_score: 0.9616992473602295\n",
      "validation loss : 0.3080096492306032\n",
      "val_accuracy: 0.9212034940719604\n",
      "val_f1_score: 0.9120166301727295\n",
      "#########################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 61/100 [53:32<34:08, 52.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#########################\n",
      "training loss: 0.07316058159036529\n",
      "train_accuracy: 0.9757175445556641\n",
      "train_f1_score: 0.9690972566604614\n",
      "validation loss : 0.3381021298874328\n",
      "val_accuracy: 0.9166561365127563\n",
      "val_f1_score: 0.9055274724960327\n",
      "#########################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 66/100 [57:55<29:47, 52.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#########################\n",
      "training loss: 0.06063577710416715\n",
      "train_accuracy: 0.9787825345993042\n",
      "train_f1_score: 0.9733573794364929\n",
      "validation loss : 0.3365217211007319\n",
      "val_accuracy: 0.9233630895614624\n",
      "val_f1_score: 0.9147210121154785\n",
      "#########################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 71/100 [1:02:18<25:25, 52.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#########################\n",
      "training loss: 0.04952614438807085\n",
      "train_accuracy: 0.9844860434532166\n",
      "train_f1_score: 0.9787583947181702\n",
      "validation loss : 0.32695737641621775\n",
      "val_accuracy: 0.9375408887863159\n",
      "val_f1_score: 0.926816463470459\n",
      "#########################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 76/100 [1:06:41<21:03, 52.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#########################\n",
      "training loss: 0.03962599304531549\n",
      "train_accuracy: 0.9865507483482361\n",
      "train_f1_score: 0.9826551675796509\n",
      "validation loss : 0.3078606569832389\n",
      "val_accuracy: 0.9306030869483948\n",
      "val_f1_score: 0.9226606488227844\n",
      "#########################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 81/100 [1:11:04<16:37, 52.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#########################\n",
      "training loss: 0.02994663364164818\n",
      "train_accuracy: 0.9907798767089844\n",
      "train_f1_score: 0.9877738952636719\n",
      "validation loss : 0.3364748696070858\n",
      "val_accuracy: 0.9356984496116638\n",
      "val_f1_score: 0.9250094294548035\n",
      "#########################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 86/100 [1:15:26<12:13, 52.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#########################\n",
      "training loss: 0.02197880615812006\n",
      "train_accuracy: 0.9936609268188477\n",
      "train_f1_score: 0.9913581609725952\n",
      "validation loss : 0.32877729997469823\n",
      "val_accuracy: 0.9330189824104309\n",
      "val_f1_score: 0.9260494709014893\n",
      "#########################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 91/100 [1:19:49<07:52, 52.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#########################\n",
      "training loss: 0.01732540629366371\n",
      "train_accuracy: 0.9939391016960144\n",
      "train_f1_score: 0.9925612807273865\n",
      "validation loss : 0.3360339563251399\n",
      "val_accuracy: 0.9358867406845093\n",
      "val_f1_score: 0.9240445494651794\n",
      "#########################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 96/100 [1:24:11<03:29, 52.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#########################\n",
      "training loss: 0.013246412011904914\n",
      "train_accuracy: 0.9958216547966003\n",
      "train_f1_score: 0.9946160316467285\n",
      "validation loss : 0.3258891504607318\n",
      "val_accuracy: 0.9358094334602356\n",
      "val_f1_score: 0.9251861572265625\n",
      "#########################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [1:27:41<00:00, 52.62s/it]\n"
     ]
    }
   ],
   "source": [
    "import src.pytorch_modular.image_classification.engine_classification as cls\n",
    "results = cls.train_model(base_model, train_dl, val_dl, train_configuration,  \n",
    "                            log_dir=os.path.join(HOME, 'runs'),         \n",
    "                            save_path=os.path.join(HOME, 'saved_models'))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-09-17T13:55:58.229436037Z"
    }
   },
   "outputs": [],
   "source": [
    "# let's make the damn submission\n",
    "from src.pytorch_modular.pytorch_utilities import load_model\n",
    "# base_model = SeqClassModel(in_features=768, hidden_size=128, num_classes=6)\n",
    "# base_model = load_model(base_model=base_model, path=os.path.join(HOME, 'saved_models', '9-17-15-10.pt'))\n",
    "# let's create a dataset object really quick:\n",
    "class TestReviewDS(Dataset):\n",
    "    def __init__(self, data: pd.DataFrame) -> None:\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index) -> tuple[str, int]:\n",
    "        return self.data.iloc[index, 1]\n",
    "\n",
    "# we need a different callate_function\n",
    "def test_collate_function(batch):\n",
    "    embeddings = MODEL(**TOKENIZER(batch, padding=True, return_tensors='pt').to(DEVICE)).last_hidden_state # make sure to return tensors\n",
    "    return embeddings.to(DEVICE)\n",
    "    \n",
    "# let's set the random seed\n",
    "\n",
    "torch.manual_seed(69)\n",
    "\n",
    "test_ds = TestReviewDS(data=df_test)\n",
    "test_loader = DataLoader(test_ds, batch_size=32, shuffle=False, collate_fn=test_collate_function)\n",
    "# next(iter(test_loader)).shape\n",
    "predictions = cls.inference(base_model, inference_source_data=test_loader, return_tensor='list')\n",
    "# convert the numerical labels to the string ones\n",
    "predictions = [idx2cat[p] for p in predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-17T13:55:58.276032280Z",
     "start_time": "2023-09-17T13:55:58.272755631Z"
    }
   },
   "outputs": [],
   "source": [
    "submission = pd.DataFrame(data={\"id\": df_test['id'].tolist(), \"Category\": predictions})\n",
    "sub_dir = os.path.join(HOME, 'submissions')\n",
    "submission.to_csv(os.path.join(sub_dir, f'sub_{len(os.listdir(sub_dir)) + 1}.csv'), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
